{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataloaders import load_data, make_loaders, append_SOS\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/scratch/users/udemir15/ELEC491/bassline_transcription')\n",
    "from utilities import *\n",
    "\n",
    "from bassline_transcriber.transcription import NN_output_to_MIDI_file, replace_sustain\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "SEED = 27\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/scratch/users/udemir15/ELEC491/bassline_transcription/data/metadata/train_misc_names.txt', 'r') as infile:\n",
    "    train_misc_names = infile.read().split('\\n')\n",
    "with open('/scratch/users/udemir15/ELEC491/bassline_transcription/data/metadata/val_misc_names.txt', 'r') as infile:\n",
    "    val_misc_names = infile.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class ResidualBlock(layers.Layer):\n",
    "    def __init__(self, channel, downsample=True):\n",
    "        super().__init__()\n",
    "        strides = 2 if downsample else 1\n",
    "        self.layers = Sequential([layers.Conv1D(channel, 4, activation=None, padding=\"same\", use_bias=False),\n",
    "                        layers.BatchNormalization(),\n",
    "                        layers.Activation(\"relu\"),\n",
    "                        layers.Conv1D(channel, 4, activation=None, padding=\"same\", strides=strides, use_bias=False),\n",
    "                        layers.BatchNormalization()])\n",
    "        \n",
    "        self.shortcut = Sequential([\n",
    "            layers.Conv1D(channel, 1, activation=None, padding=\"same\", strides=strides, use_bias = False),\n",
    "            layers.BatchNormalization()])\n",
    "            \n",
    "    def call(self, x):\n",
    "        x = self.layers(x)\n",
    "        h = self.shortcut(x)\n",
    "        x = layers.add([h,x])\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        return x\n",
    "    \n",
    "class ResidualStack(layers.Layer):\n",
    "    def __init__(self, channel, n_blocks=3, downsample=True):\n",
    "        super().__init__()\n",
    "        self.layers = Sequential([ResidualBlock(channel, downsample=False)]*(n_blocks-1) +[ResidualBlock(channel, downsample)])\n",
    "        \n",
    "    def call(self, data):\n",
    "        return self.layers(data)\n",
    "    \n",
    "class Encoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self, embedding_size, embedding_dim, latent_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = layers.Embedding(embedding_size, embedding_dim)\n",
    "    \n",
    "        self.layer = Sequential([ResidualStack(32),\n",
    "                                 ResidualStack(64),\n",
    "                                 ResidualStack(128),\n",
    "                                layers.Flatten(),\n",
    "                                layers.Dense(16, activation=\"relu\")])\n",
    "               \n",
    "        self.mean_layer = layers.Dense(latent_dim, name=\"z_mean\")\n",
    "        self.var_layer = layers.Dense(latent_dim, name=\"z_log_var\")\n",
    "        \n",
    "        self.sampling = Sampling()\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x = self.layer(x)\n",
    "        \n",
    "        z_mean = self.mean_layer(x)\n",
    "        z_log_var = self.var_layer(x)\n",
    "        \n",
    "        z = self.sampling([z_mean, z_log_var])\n",
    "        \n",
    "        return z_mean, z_log_var, z \n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self, input_, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.layers = Sequential([layers.Dense(input_*8, activation=\"relu\"),\n",
    "                layers.Reshape((8, input_)),\n",
    "                layers.Conv1DTranspose(128, 4, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "                ResidualStack(128, downsample=False),\n",
    "                layers.Conv1DTranspose(64, 4, activation=\"relu\", strides=1, padding=\"same\"),\n",
    "                ResidualStack(64, downsample=False),\n",
    "                layers.Conv1DTranspose(32, 4, activation=\"relu\", strides=1, padding=\"same\"),\n",
    "                ResidualStack(32, downsample=False),\n",
    "                layers.Conv1DTranspose(output_dim, 4, activation=\"softmax\", padding=\"same\")])\n",
    "        \n",
    "    def call(self, x):        \n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.accuracy_tracker = keras.metrics.Mean(name=\"acc\")\n",
    "        \n",
    "        self.acc_fn = SparseCategoricalAccuracy()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.accuracy_tracker,\n",
    "        ]\n",
    "    \n",
    "    def call(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return z_mean, z_log_var, z, reconstruction\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z, reconstruction = self(data)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "               keras.losses.SparseCategoricalCrossentropy()(tf.cast(data, dtype=tf.float32), reconstruction)\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        acc=self.acc_fn(data, reconstruction)\n",
    "        self.accuracy_tracker.update_state(acc)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"acc\": self.accuracy_tracker.result()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 12\n",
      "Sequence Length: 16\n",
      "Number of data points: 14480\n"
     ]
    }
   ],
   "source": [
    "M = 8\n",
    "\n",
    "data_params = {'dataset_path': '/scratch/users/udemir15/ELEC491/bassline_transcription/data/datasets/[28, 51]',\n",
    "               'dataset_name': 'TechHouse_bassline_representations_4020',\n",
    "               'scale_type': 'min',\n",
    "               'M': M}\n",
    "\n",
    "X, titles = load_data(data_params, False)\n",
    "\n",
    "X = replace_sustain(X, 25)\n",
    "X[X<9] += 12\n",
    "X[X>=21] -= 12\n",
    "X -= 9\n",
    "\n",
    "X = X[[title not in val_misc_names+train_misc_names for title in titles]]\n",
    "K = int(X.max()+1) # Number of classes, assumes consecutive [0,max] inclusive\n",
    "X = X.reshape(-1, 16, 1)\n",
    "sequence_length = X.shape[1]\n",
    "\n",
    "print('Number of classes: {}\\nSequence Length: {}'.format(K, sequence_length))\n",
    "print('Number of data points: {}'.format(X.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_35 (Encoder)         multiple                  537360    \n",
      "_________________________________________________________________\n",
      "decoder_33 (Decoder)         multiple                  2565612   \n",
      "=================================================================\n",
      "Total params: 3,102,982\n",
      "Trainable params: 3,097,596\n",
      "Non-trainable params: 5,386\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "905/905 [==============================] - 19s 16ms/step - loss: 1.8366 - reconstruction_loss: 1.8061 - kl_loss: 4.8535e-05 - acc: 0.4777\n",
      "Epoch 2/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7944 - reconstruction_loss: 1.7923 - kl_loss: 2.2914e-05 - acc: 0.4812\n",
      "Epoch 3/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7895 - reconstruction_loss: 1.7908 - kl_loss: 2.1873e-05 - acc: 0.4814\n",
      "Epoch 4/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7877 - reconstruction_loss: 1.7903 - kl_loss: 2.5682e-05 - acc: 0.4818\n",
      "Epoch 5/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7911 - reconstruction_loss: 1.7897 - kl_loss: 1.8817e-05 - acc: 0.4816\n",
      "Epoch 6/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7913 - reconstruction_loss: 1.7898 - kl_loss: 3.7649e-05 - acc: 0.4816\n",
      "Epoch 7/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7905 - reconstruction_loss: 1.7892 - kl_loss: 1.5692e-05 - acc: 0.4817\n",
      "Epoch 8/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7922 - reconstruction_loss: 1.7892 - kl_loss: 1.1313e-05 - acc: 0.4818\n",
      "Epoch 9/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7965 - reconstruction_loss: 1.7892 - kl_loss: 1.5608e-05 - acc: 0.4817\n",
      "Epoch 10/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7891 - reconstruction_loss: 1.7890 - kl_loss: 1.9156e-05 - acc: 0.4819\n",
      "Epoch 11/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7883 - reconstruction_loss: 1.7890 - kl_loss: 2.2368e-05 - acc: 0.4819\n",
      "Epoch 12/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7883 - reconstruction_loss: 1.7889 - kl_loss: 1.3553e-05 - acc: 0.4818\n",
      "Epoch 13/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7822 - reconstruction_loss: 1.7891 - kl_loss: 5.8827e-05 - acc: 0.4819\n",
      "Epoch 14/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7951 - reconstruction_loss: 1.7885 - kl_loss: 1.0983e-05 - acc: 0.4818\n",
      "Epoch 15/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7902 - reconstruction_loss: 1.7885 - kl_loss: 1.4848e-05 - acc: 0.4818\n",
      "Epoch 16/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7931 - reconstruction_loss: 1.7885 - kl_loss: 1.7172e-05 - acc: 0.4818\n",
      "Epoch 17/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7935 - reconstruction_loss: 1.7884 - kl_loss: 1.8434e-05 - acc: 0.4818\n",
      "Epoch 18/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7900 - reconstruction_loss: 1.7885 - kl_loss: 1.8584e-05 - acc: 0.4819\n",
      "Epoch 19/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7877 - reconstruction_loss: 1.7885 - kl_loss: 1.8322e-05 - acc: 0.4819\n",
      "Epoch 20/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7909 - reconstruction_loss: 1.7882 - kl_loss: 1.5315e-05 - acc: 0.4819\n",
      "Epoch 21/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7861 - reconstruction_loss: 1.7884 - kl_loss: 1.8248e-05 - acc: 0.4819\n",
      "Epoch 22/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7868 - reconstruction_loss: 1.7882 - kl_loss: 1.8915e-05 - acc: 0.4819\n",
      "Epoch 23/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7846 - reconstruction_loss: 1.7882 - kl_loss: 1.9244e-05 - acc: 0.4819\n",
      "Epoch 24/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7886 - reconstruction_loss: 1.7882 - kl_loss: 1.8303e-05 - acc: 0.4819\n",
      "Epoch 25/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7843 - reconstruction_loss: 1.7881 - kl_loss: 1.8028e-05 - acc: 0.4819\n",
      "Epoch 26/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7872 - reconstruction_loss: 1.7882 - kl_loss: 1.8959e-05 - acc: 0.4819\n",
      "Epoch 27/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7941 - reconstruction_loss: 1.7881 - kl_loss: 1.7907e-05 - acc: 0.4819\n",
      "Epoch 28/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7878 - reconstruction_loss: 1.7880 - kl_loss: 1.7633e-05 - acc: 0.4819\n",
      "Epoch 29/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7887 - reconstruction_loss: 1.7883 - kl_loss: 1.7578e-05 - acc: 0.4819\n",
      "Epoch 30/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7871 - reconstruction_loss: 1.7881 - kl_loss: 1.7963e-05 - acc: 0.4819\n",
      "Epoch 31/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7907 - reconstruction_loss: 1.7881 - kl_loss: 1.7920e-05 - acc: 0.4819\n",
      "Epoch 32/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7892 - reconstruction_loss: 1.7881 - kl_loss: 1.7854e-05 - acc: 0.4819\n",
      "Epoch 33/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7862 - reconstruction_loss: 1.7880 - kl_loss: 1.7608e-05 - acc: 0.4819\n",
      "Epoch 34/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7850 - reconstruction_loss: 1.7881 - kl_loss: 1.7533e-05 - acc: 0.4819\n",
      "Epoch 35/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7884 - reconstruction_loss: 1.7879 - kl_loss: 1.7385e-05 - acc: 0.4819\n",
      "Epoch 36/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7903 - reconstruction_loss: 1.7880 - kl_loss: 1.7310e-05 - acc: 0.4819\n",
      "Epoch 37/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7891 - reconstruction_loss: 1.7882 - kl_loss: 1.7167e-05 - acc: 0.4819\n",
      "Epoch 38/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7907 - reconstruction_loss: 1.7879 - kl_loss: 1.6952e-05 - acc: 0.4819\n",
      "Epoch 39/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7920 - reconstruction_loss: 1.7878 - kl_loss: 1.6706e-05 - acc: 0.4819\n",
      "Epoch 40/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7928 - reconstruction_loss: 1.7878 - kl_loss: 1.6617e-05 - acc: 0.4819\n",
      "Epoch 41/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7862 - reconstruction_loss: 1.7879 - kl_loss: 1.6497e-05 - acc: 0.4819\n",
      "Epoch 42/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7922 - reconstruction_loss: 1.7879 - kl_loss: 1.6540e-05 - acc: 0.4819\n",
      "Epoch 43/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7838 - reconstruction_loss: 1.7880 - kl_loss: 1.6454e-05 - acc: 0.4819\n",
      "Epoch 44/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7906 - reconstruction_loss: 1.7880 - kl_loss: 1.6375e-05 - acc: 0.4819\n",
      "Epoch 45/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7900 - reconstruction_loss: 1.7879 - kl_loss: 1.6308e-05 - acc: 0.4819\n",
      "Epoch 46/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7889 - reconstruction_loss: 1.7880 - kl_loss: 1.6161e-05 - acc: 0.4819\n",
      "Epoch 47/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7861 - reconstruction_loss: 1.7878 - kl_loss: 1.6004e-05 - acc: 0.4819\n",
      "Epoch 48/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7837 - reconstruction_loss: 1.7878 - kl_loss: 1.5742e-05 - acc: 0.4819\n",
      "Epoch 49/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7933 - reconstruction_loss: 1.7879 - kl_loss: 1.5636e-05 - acc: 0.4819\n",
      "Epoch 50/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7877 - reconstruction_loss: 1.7878 - kl_loss: 1.5588e-05 - acc: 0.4819\n",
      "Epoch 51/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7845 - reconstruction_loss: 1.7878 - kl_loss: 1.5364e-05 - acc: 0.4819\n",
      "Epoch 52/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7864 - reconstruction_loss: 1.7879 - kl_loss: 1.5056e-05 - acc: 0.4819\n",
      "Epoch 53/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7932 - reconstruction_loss: 1.7878 - kl_loss: 1.4706e-05 - acc: 0.4819\n",
      "Epoch 54/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7940 - reconstruction_loss: 1.7877 - kl_loss: 1.4628e-05 - acc: 0.4819\n",
      "Epoch 55/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7855 - reconstruction_loss: 1.7878 - kl_loss: 1.4489e-05 - acc: 0.4819\n",
      "Epoch 56/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7903 - reconstruction_loss: 1.7879 - kl_loss: 1.4410e-05 - acc: 0.4819\n",
      "Epoch 57/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7839 - reconstruction_loss: 1.7877 - kl_loss: 1.4289e-05 - acc: 0.4819\n",
      "Epoch 58/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7868 - reconstruction_loss: 1.7877 - kl_loss: 1.4202e-05 - acc: 0.4819\n",
      "Epoch 59/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7844 - reconstruction_loss: 1.7878 - kl_loss: 1.4093e-05 - acc: 0.4819\n",
      "Epoch 60/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7860 - reconstruction_loss: 1.7877 - kl_loss: 1.4001e-05 - acc: 0.4819\n",
      "Epoch 61/300\n",
      "905/905 [==============================] - 16s 18ms/step - loss: 1.7895 - reconstruction_loss: 1.7878 - kl_loss: 1.3971e-05 - acc: 0.4819\n",
      "Epoch 62/300\n",
      "905/905 [==============================] - 15s 17ms/step - loss: 1.7884 - reconstruction_loss: 1.7878 - kl_loss: 1.3674e-05 - acc: 0.4819\n",
      "Epoch 63/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7895 - reconstruction_loss: 1.7878 - kl_loss: 1.3597e-05 - acc: 0.4819\n",
      "Epoch 64/300\n",
      "905/905 [==============================] - 18s 20ms/step - loss: 1.7797 - reconstruction_loss: 1.7877 - kl_loss: 1.3383e-05 - acc: 0.4819\n",
      "Epoch 65/300\n",
      "905/905 [==============================] - 16s 18ms/step - loss: 1.7775 - reconstruction_loss: 1.7878 - kl_loss: 1.3145e-05 - acc: 0.4819\n",
      "Epoch 66/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7853 - reconstruction_loss: 1.7878 - kl_loss: 1.2983e-05 - acc: 0.4819\n",
      "Epoch 67/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7832 - reconstruction_loss: 1.7877 - kl_loss: 1.3010e-05 - acc: 0.4819\n",
      "Epoch 68/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7959 - reconstruction_loss: 1.7876 - kl_loss: 1.2888e-05 - acc: 0.4819\n",
      "Epoch 69/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7921 - reconstruction_loss: 1.7877 - kl_loss: 1.2575e-05 - acc: 0.4819\n",
      "Epoch 70/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7853 - reconstruction_loss: 1.7877 - kl_loss: 1.2349e-05 - acc: 0.4819\n",
      "Epoch 71/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7829 - reconstruction_loss: 1.7877 - kl_loss: 1.2239e-05 - acc: 0.4819\n",
      "Epoch 72/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7879 - reconstruction_loss: 1.7876 - kl_loss: 1.1935e-05 - acc: 0.4819\n",
      "Epoch 73/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7907 - reconstruction_loss: 1.7877 - kl_loss: 1.1889e-05 - acc: 0.4819\n",
      "Epoch 74/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7831 - reconstruction_loss: 1.7876 - kl_loss: 1.1709e-05 - acc: 0.4819\n",
      "Epoch 75/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7829 - reconstruction_loss: 1.7877 - kl_loss: 1.1537e-05 - acc: 0.4819\n",
      "Epoch 76/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7861 - reconstruction_loss: 1.7877 - kl_loss: 1.1209e-05 - acc: 0.4819\n",
      "Epoch 77/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7886 - reconstruction_loss: 1.7877 - kl_loss: 1.1134e-05 - acc: 0.4819\n",
      "Epoch 78/300\n",
      "905/905 [==============================] - 15s 17ms/step - loss: 1.7845 - reconstruction_loss: 1.7877 - kl_loss: 1.1107e-05 - acc: 0.4819\n",
      "Epoch 79/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7919 - reconstruction_loss: 1.7877 - kl_loss: 1.0729e-05 - acc: 0.4819\n",
      "Epoch 80/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7884 - reconstruction_loss: 1.7875 - kl_loss: 1.0813e-05 - acc: 0.4819\n",
      "Epoch 81/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7912 - reconstruction_loss: 1.7876 - kl_loss: 1.0437e-05 - acc: 0.4819\n",
      "Epoch 82/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7925 - reconstruction_loss: 1.7876 - kl_loss: 1.0123e-05 - acc: 0.4819\n",
      "Epoch 83/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7902 - reconstruction_loss: 1.7877 - kl_loss: 9.8089e-06 - acc: 0.4819\n",
      "Epoch 84/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7843 - reconstruction_loss: 1.7877 - kl_loss: 9.7377e-06 - acc: 0.4819\n",
      "Epoch 85/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7906 - reconstruction_loss: 1.7876 - kl_loss: 9.5002e-06 - acc: 0.4819\n",
      "Epoch 86/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7926 - reconstruction_loss: 1.7876 - kl_loss: 9.2069e-06 - acc: 0.4819\n",
      "Epoch 87/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7886 - reconstruction_loss: 1.7876 - kl_loss: 9.0670e-06 - acc: 0.4819\n",
      "Epoch 88/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7875 - reconstruction_loss: 1.7876 - kl_loss: 8.8024e-06 - acc: 0.4819\n",
      "Epoch 89/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7875 - reconstruction_loss: 1.7876 - kl_loss: 8.7815e-06 - acc: 0.4819\n",
      "Epoch 90/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7860 - reconstruction_loss: 1.7876 - kl_loss: 8.2857e-06 - acc: 0.4819\n",
      "Epoch 91/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7868 - reconstruction_loss: 1.7876 - kl_loss: 8.1903e-06 - acc: 0.4819\n",
      "Epoch 92/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7931 - reconstruction_loss: 1.7876 - kl_loss: 8.0540e-06 - acc: 0.4819\n",
      "Epoch 93/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7863 - reconstruction_loss: 1.7876 - kl_loss: 8.0513e-06 - acc: 0.4819\n",
      "Epoch 94/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7915 - reconstruction_loss: 1.7875 - kl_loss: 7.7359e-06 - acc: 0.4819\n",
      "Epoch 95/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7911 - reconstruction_loss: 1.7875 - kl_loss: 7.4860e-06 - acc: 0.4819\n",
      "Epoch 96/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7862 - reconstruction_loss: 1.7876 - kl_loss: 7.4011e-06 - acc: 0.4819\n",
      "Epoch 97/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7921 - reconstruction_loss: 1.7876 - kl_loss: 7.3073e-06 - acc: 0.4819\n",
      "Epoch 98/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7833 - reconstruction_loss: 1.7876 - kl_loss: 7.0777e-06 - acc: 0.4819\n",
      "Epoch 99/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7884 - reconstruction_loss: 1.7876 - kl_loss: 6.9081e-06 - acc: 0.4819\n",
      "Epoch 100/300\n",
      "905/905 [==============================] - 15s 17ms/step - loss: 1.7947 - reconstruction_loss: 1.7875 - kl_loss: 6.7808e-06 - acc: 0.4819\n",
      "Epoch 101/300\n",
      "905/905 [==============================] - 19s 21ms/step - loss: 1.7853 - reconstruction_loss: 1.7876 - kl_loss: 6.4042e-06 - acc: 0.4819\n",
      "Epoch 102/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7865 - reconstruction_loss: 1.7875 - kl_loss: 6.0019e-06 - acc: 0.4819\n",
      "Epoch 103/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7946 - reconstruction_loss: 1.7876 - kl_loss: 5.8029e-06 - acc: 0.4819\n",
      "Epoch 104/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7857 - reconstruction_loss: 1.7875 - kl_loss: 5.3803e-06 - acc: 0.4819\n",
      "Epoch 105/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7847 - reconstruction_loss: 1.7877 - kl_loss: 5.2931e-06 - acc: 0.4819\n",
      "Epoch 106/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7930 - reconstruction_loss: 1.7875 - kl_loss: 4.9737e-06 - acc: 0.4819\n",
      "Epoch 107/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7837 - reconstruction_loss: 1.7876 - kl_loss: 4.9720e-06 - acc: 0.4819\n",
      "Epoch 108/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7916 - reconstruction_loss: 1.7875 - kl_loss: 4.9619e-06 - acc: 0.4819\n",
      "Epoch 109/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7892 - reconstruction_loss: 1.7876 - kl_loss: 4.8429e-06 - acc: 0.4819\n",
      "Epoch 110/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7899 - reconstruction_loss: 1.7876 - kl_loss: 4.8124e-06 - acc: 0.4819\n",
      "Epoch 111/300\n",
      "905/905 [==============================] - 17s 18ms/step - loss: 1.7870 - reconstruction_loss: 1.7875 - kl_loss: 4.5489e-06 - acc: 0.4819\n",
      "Epoch 112/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7866 - reconstruction_loss: 1.7875 - kl_loss: 4.3732e-06 - acc: 0.4819\n",
      "Epoch 113/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7832 - reconstruction_loss: 1.7875 - kl_loss: 4.1209e-06 - acc: 0.4819\n",
      "Epoch 114/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7845 - reconstruction_loss: 1.7875 - kl_loss: 3.8929e-06 - acc: 0.4819\n",
      "Epoch 115/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7842 - reconstruction_loss: 1.7875 - kl_loss: 3.1954e-06 - acc: 0.4819\n",
      "Epoch 116/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7798 - reconstruction_loss: 1.7875 - kl_loss: 3.1783e-06 - acc: 0.4819\n",
      "Epoch 117/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7918 - reconstruction_loss: 1.7876 - kl_loss: 2.7068e-06 - acc: 0.4819\n",
      "Epoch 118/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7891 - reconstruction_loss: 1.7875 - kl_loss: 2.2269e-06 - acc: 0.4819\n",
      "Epoch 119/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7864 - reconstruction_loss: 1.7875 - kl_loss: 1.9588e-06 - acc: 0.4819\n",
      "Epoch 120/300\n",
      "905/905 [==============================] - 15s 17ms/step - loss: 1.7809 - reconstruction_loss: 1.7875 - kl_loss: 1.7221e-06 - acc: 0.4819\n",
      "Epoch 121/300\n",
      "905/905 [==============================] - 15s 16ms/step - loss: 1.7809 - reconstruction_loss: 1.7875 - kl_loss: 1.5888e-06 - acc: 0.4819\n",
      "Epoch 122/300\n",
      "905/905 [==============================] - 18s 20ms/step - loss: 1.7894 - reconstruction_loss: 1.7874 - kl_loss: 1.3653e-06 - acc: 0.4819\n",
      "Epoch 123/300\n",
      "905/905 [==============================] - 16s 18ms/step - loss: 1.7836 - reconstruction_loss: 1.7875 - kl_loss: 1.0376e-06 - acc: 0.4819\n",
      "Epoch 124/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7822 - reconstruction_loss: 1.7874 - kl_loss: 6.7498e-07 - acc: 0.4819\n",
      "Epoch 125/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7854 - reconstruction_loss: 1.7875 - kl_loss: 6.6174e-07 - acc: 0.4819\n",
      "Epoch 126/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7901 - reconstruction_loss: 1.7876 - kl_loss: 2.7770e-07 - acc: 0.4819\n",
      "Epoch 127/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7831 - reconstruction_loss: 1.7874 - kl_loss: 9.9879e-08 - acc: 0.4819\n",
      "Epoch 128/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7877 - reconstruction_loss: 1.7875 - kl_loss: -1.8046e-08 - acc: 0.4819\n",
      "Epoch 129/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7855 - reconstruction_loss: 1.7876 - kl_loss: -3.9033e-07 - acc: 0.4819\n",
      "Epoch 130/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7902 - reconstruction_loss: 1.7874 - kl_loss: -7.5708e-07 - acc: 0.4819\n",
      "Epoch 131/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7844 - reconstruction_loss: 1.7876 - kl_loss: -9.4794e-07 - acc: 0.4819\n",
      "Epoch 132/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7869 - reconstruction_loss: 1.7875 - kl_loss: -9.8861e-07 - acc: 0.4819\n",
      "Epoch 133/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7900 - reconstruction_loss: 1.7875 - kl_loss: -1.0964e-06 - acc: 0.4819\n",
      "Epoch 134/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7813 - reconstruction_loss: 1.7875 - kl_loss: -1.4917e-06 - acc: 0.4819\n",
      "Epoch 135/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7896 - reconstruction_loss: 1.7874 - kl_loss: -1.7256e-06 - acc: 0.4819\n",
      "Epoch 136/300\n",
      "905/905 [==============================] - 14s 16ms/step - loss: 1.7867 - reconstruction_loss: 1.7875 - kl_loss: -1.9105e-06 - acc: 0.4819\n",
      "Epoch 137/300\n",
      "905/905 [==============================] - 18s 20ms/step - loss: 1.7860 - reconstruction_loss: 1.7875 - kl_loss: -2.0419e-06 - acc: 0.4819\n",
      "Epoch 138/300\n",
      "905/905 [==============================] - 27s 30ms/step - loss: 1.7883 - reconstruction_loss: 1.7875 - kl_loss: -2.1573e-06 - acc: 0.4819\n",
      "Epoch 139/300\n",
      "905/905 [==============================] - 27s 30ms/step - loss: 1.7868 - reconstruction_loss: 1.7875 - kl_loss: -2.2919e-06 - acc: 0.4819\n",
      "Epoch 140/300\n",
      "650/905 [====================>.........] - ETA: 8s - loss: 1.7815 - reconstruction_loss: 1.7858 - kl_loss: -2.4150e-06 - acc: 0.4819"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-22a97b0d1d0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = Encoder(K, 128, 4096)\n",
    "decoder = Decoder(64, K)\n",
    "\n",
    "model = VAE(encoder, decoder)\n",
    "model.compile(optimizer=keras.optimizers.Adam())\n",
    "model(X[:1])\n",
    "model.summary()\n",
    "model.fit(X, epochs=300, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
