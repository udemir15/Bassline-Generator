{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.backend import argmax, cast\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Input, Embedding, RepeatVector, Reshape, Conv1D, MaxPooling1D, UpSampling1D, Bidirectional, Lambda, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 8\n",
    "\n",
    "minor_df = pd.read_csv(f\"Datasets/datasets/traxsource_0-5000_bassline_representations_min_M{M}.csv\", header=None)\n",
    "major_df = pd.read_csv(f\"Datasets/datasets/traxsource_0-5000_bassline_representations_maj_M{M}.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3859, 65), (409, 65))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_df.shape, major_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat((minor_df.iloc[:,1:].astype(int), major_df.iloc[:,1:].astype(int)))\n",
    "minor_data = minor_df.values[:,1:].astype(int)\n",
    "major_data = major_df.values[:,1:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 37, 37, ..., 37, 37, 24],\n",
       "       [ 0, 37, 37, ..., 37, 37, 12],\n",
       "       [ 0, 12, 37, ...,  7, 37, 10],\n",
       "       ...,\n",
       "       [ 0, 12,  0, ...,  0, 37, 24],\n",
       "       [ 0, 37, 14, ..., 19,  0, 10],\n",
       "       [ 0, 37, 37, ..., 37, 37, 26]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = np.unique(all_data)\n",
    "vocab = np.arange(len(notes))\n",
    "\n",
    "n2v_mapping = dict(zip(notes, vocab))\n",
    "v2n_mapping = dict(zip(vocab, notes))\n",
    "\n",
    "vocab_size = len(n2v_mapping)\n",
    "\n",
    "def replace_with_dict(ar, dic):\n",
    "    # Extract out keys and values\n",
    "    k = np.array(list(dic.keys()))\n",
    "    v = np.array(list(dic.values()))\n",
    "\n",
    "    # Get argsort indices\n",
    "    sidx = k.argsort()\n",
    "\n",
    "    # Drop the magic bomb with searchsorted to get the corresponding\n",
    "    # places for a in keys (using sorter since a is not necessarily sorted).\n",
    "    # Then trace it back to original order with indexing into sidx\n",
    "    # Finally index into values for desired output.\n",
    "    return v[sidx[np.searchsorted(k,ar,sorter=sidx)]]\n",
    "\n",
    "minor_data = replace_with_dict(minor_data, n2v_mapping).astype(int)\n",
    "major_data = replace_with_dict(major_data, n2v_mapping).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(minor_data, minor_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]\n",
    "timesteps = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def replace_continue(arr, cont_token):\n",
    "#    for r in arr:\n",
    "#        for idx, el in enumerate(r[1:]):\n",
    "#            if el == cont_token:\n",
    "#                r[idx + 1] = r[idx]\n",
    "#\n",
    "#cont_token = 35\n",
    "#\n",
    "#\n",
    "#replace_continue(X_train, cont_token)\n",
    "#replace_continue(y_train, cont_token)\n",
    "#replace_continue(X_test, cont_token)\n",
    "#replace_continue(y_test, cont_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, encode_length):\n",
    "    return np.eye(encode_length)[arr.astype(np.uint)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = one_hot_encode(y_train, vocab_size)\n",
    "y_test = one_hot_encode(y_test, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Model 1: Embedding-LSTM Encoder, LSTM-Dense Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: udemir15 (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">bumbling-monkey-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/nbg/Keras_runs\" target=\"_blank\">https://wandb.ai/nbg/Keras_runs</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/nbg/Keras_runs/runs/7qstfi85\" target=\"_blank\">https://wandb.ai/nbg/Keras_runs/runs/7qstfi85</a><br/>\n",
       "                Run data is saved locally in <code>/scratch/users/udemir15/Bassline-Generator/generator/wandb/run-20210523_165709-7qstfi85</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(7qstfi85)</h1><iframe src=\"https://wandb.ai/nbg/Keras_runs/runs/7qstfi85\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2ba25c9d2b10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='Keras_runs', entity='nbg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NBG_lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timesteps = 64  # Length of your sequences\n",
    "embed_size = 32\n",
    "latent_dim = 256\n",
    "dropout = 0\n",
    "\n",
    "inputs = Input(shape=(timesteps,))\n",
    "embedded = Embedding(vocab_size, embed_size)(inputs)\n",
    "#encoded = LSTM(latent_dim, return_sequences=True, dropout=dropout)(embedded)\n",
    "encoded = LSTM(latent_dim, dropout=dropout)(embedded)\n",
    "\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "#decoded = LSTM(latent_dim, return_sequences=True, dropout=dropout)(decoded)\n",
    "decoded = LSTM(latent_dim, return_sequences=True, dropout=dropout)(decoded)\n",
    "decoded = Dense(vocab_size, activation='softmax')(decoded)\n",
    "#decoded = argmax(decoded, axis=-1)\n",
    "#decoded = cast(decoded, float)\n",
    "#decoded = Reshape((decoded.shape[1], -1))(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 64, 32)            1216      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               295936    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64, 38)            9766      \n",
      "=================================================================\n",
      "Total params: 832,230\n",
      "Trainable params: 832,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs, encoded)\n",
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(latent_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layers = sequence_autoencoder.layers[-3:]\n",
    "decoded_input = decoder_layers[0](encoded_input)\n",
    "for decoder_layer in decoder_layers[1:]:\n",
    "    decoded_input = decoder_layer(decoded_input)\n",
    "# Create the decoder model\n",
    "decoder = Model(encoded_input, decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 64, 32)            1216      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               295936    \n",
      "=================================================================\n",
      "Total params: 297,152\n",
      "Trainable params: 297,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 256)]             0         \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64, 38)            9766      \n",
      "=================================================================\n",
      "Total params: 535,078\n",
      "Trainable params: 535,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-3\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "wandb.config.learning_rate = learning_rate\n",
    "wandb.config.epochs = epochs\n",
    "wandb.config.batch_size = batch_size\n",
    "wandb.config.model = name\n",
    "\n",
    "mc = ModelCheckpoint(f'Models/{name}.hdf5', monitor='val_loss')\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "sequence_autoencoder.compile(optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.005, 'epochs': 100, 'batch_size': 32, 'model': 'NBG_lstm'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "97/97 [==============================] - 8s 24ms/step - loss: 2.2310 - val_loss: 1.9224\n",
      "Epoch 2/100\n",
      "97/97 [==============================] - 1s 15ms/step - loss: 1.9374 - val_loss: 1.8408\n",
      "Epoch 3/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.8269 - val_loss: 1.6880\n",
      "Epoch 4/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.6794 - val_loss: 1.6212\n",
      "Epoch 5/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.6113 - val_loss: 1.5569\n",
      "Epoch 6/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.5436 - val_loss: 1.5130\n",
      "Epoch 7/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.5024 - val_loss: 1.4817\n",
      "Epoch 8/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.4607 - val_loss: 1.4572\n",
      "Epoch 9/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.4485 - val_loss: 1.4305\n",
      "Epoch 10/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.4468 - val_loss: 1.4359\n",
      "Epoch 11/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.4331 - val_loss: 1.4165\n",
      "Epoch 12/100\n",
      "97/97 [==============================] - 1s 15ms/step - loss: 1.4058 - val_loss: 1.4083\n",
      "Epoch 13/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3857 - val_loss: 1.3928\n",
      "Epoch 14/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.3758 - val_loss: 1.3876\n",
      "Epoch 15/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.3594 - val_loss: 1.3774\n",
      "Epoch 16/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3663 - val_loss: 1.4302\n",
      "Epoch 17/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3949 - val_loss: 1.3823\n",
      "Epoch 18/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3571 - val_loss: 1.3696\n",
      "Epoch 19/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.3458 - val_loss: 1.4022\n",
      "Epoch 20/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3684 - val_loss: 1.3476\n",
      "Epoch 21/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.3094 - val_loss: 1.3240\n",
      "Epoch 22/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.2872 - val_loss: 1.2952\n",
      "Epoch 23/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.2880 - val_loss: 1.2862\n",
      "Epoch 24/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.2698 - val_loss: 1.2835\n",
      "Epoch 25/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.2299 - val_loss: 1.2530\n",
      "Epoch 26/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.2367 - val_loss: 1.2433\n",
      "Epoch 27/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.2049 - val_loss: 1.2521\n",
      "Epoch 28/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.2040 - val_loss: 1.2890\n",
      "Epoch 29/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.2395 - val_loss: 1.2382\n",
      "Epoch 30/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.1867 - val_loss: 1.2285\n",
      "Epoch 31/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.2147 - val_loss: 1.2362\n",
      "Epoch 32/100\n",
      "97/97 [==============================] - 1s 15ms/step - loss: 1.1996 - val_loss: 1.2170\n",
      "Epoch 33/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.1746 - val_loss: 1.2321\n",
      "Epoch 34/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.1577 - val_loss: 1.2402\n",
      "Epoch 35/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.1634 - val_loss: 1.2078\n",
      "Epoch 36/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.1303 - val_loss: 1.2078\n",
      "Epoch 37/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.1197 - val_loss: 1.2174\n",
      "Epoch 38/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.1286 - val_loss: 1.2238\n",
      "Epoch 39/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.1050 - val_loss: 1.2083\n",
      "Epoch 40/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0925 - val_loss: 1.2174\n",
      "Epoch 41/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.1188 - val_loss: 1.2272\n",
      "Epoch 42/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.1278 - val_loss: 1.2443\n",
      "Epoch 43/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.1246 - val_loss: 1.2151\n",
      "Epoch 44/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.0776 - val_loss: 1.2270\n",
      "Epoch 45/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0795 - val_loss: 1.3403\n",
      "Epoch 46/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.2497 - val_loss: 1.2580\n",
      "Epoch 47/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.1193 - val_loss: 1.2365\n",
      "Epoch 48/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0975 - val_loss: 1.2323\n",
      "Epoch 49/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0642 - val_loss: 1.2274\n",
      "Epoch 50/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0582 - val_loss: 1.2332\n",
      "Epoch 51/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0390 - val_loss: 1.2417\n",
      "Epoch 52/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.0089 - val_loss: 1.2498\n",
      "Epoch 53/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 0.9982 - val_loss: 1.2378\n",
      "Epoch 54/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 0.9673 - val_loss: 1.2493\n",
      "Epoch 55/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 0.9549 - val_loss: 1.2693\n",
      "Epoch 56/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 0.9391 - val_loss: 1.2906\n",
      "Epoch 57/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0058 - val_loss: 1.2777\n",
      "Epoch 58/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.9628 - val_loss: 1.2724\n",
      "Epoch 59/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.9346 - val_loss: 1.2967\n",
      "Epoch 60/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0242 - val_loss: 1.2745\n",
      "Epoch 61/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.9583 - val_loss: 1.2936\n",
      "Epoch 62/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.9477 - val_loss: 1.2958\n",
      "Epoch 63/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 0.9425 - val_loss: 1.2999\n",
      "Epoch 64/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.9723 - val_loss: 1.2917\n",
      "Epoch 65/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.0209 - val_loss: 1.2981\n",
      "Epoch 66/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.1213 - val_loss: 1.3160\n",
      "Epoch 67/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0536 - val_loss: 1.2804\n",
      "Epoch 68/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 0.9769 - val_loss: 1.2791\n",
      "Epoch 69/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.9364 - val_loss: 1.2907\n",
      "Epoch 70/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.9034 - val_loss: 1.3023\n",
      "Epoch 71/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8901 - val_loss: 1.3144\n",
      "Epoch 72/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 0.8862 - val_loss: 1.3269\n",
      "Epoch 73/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.9054 - val_loss: 1.3279\n",
      "Epoch 74/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8769 - val_loss: 1.3367\n",
      "Epoch 75/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8593 - val_loss: 1.3537\n",
      "Epoch 76/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.0038 - val_loss: 1.3372\n",
      "Epoch 77/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.9418 - val_loss: 1.3393\n",
      "Epoch 78/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8820 - val_loss: 1.3541\n",
      "Epoch 79/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8488 - val_loss: 1.3622\n",
      "Epoch 80/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 0.8323 - val_loss: 1.3689\n",
      "Epoch 81/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 0.8186 - val_loss: 1.4012\n",
      "Epoch 82/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8139 - val_loss: 1.3977\n",
      "Epoch 83/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8060 - val_loss: 1.4223\n",
      "Epoch 84/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8023 - val_loss: 1.4048\n",
      "Epoch 85/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8432 - val_loss: 1.4362\n",
      "Epoch 86/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 0.8965 - val_loss: 1.3970\n",
      "Epoch 87/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8269 - val_loss: 1.4045\n",
      "Epoch 88/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.7985 - val_loss: 1.4230\n",
      "Epoch 89/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 0.7913 - val_loss: 1.4300\n",
      "Epoch 90/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 0.8237 - val_loss: 1.4032\n",
      "Epoch 91/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.9283 - val_loss: 1.3973\n",
      "Epoch 92/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8601 - val_loss: 1.3962\n",
      "Epoch 93/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8569 - val_loss: 1.3949\n",
      "Epoch 94/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8733 - val_loss: 1.4148\n",
      "Epoch 95/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8189 - val_loss: 1.4159\n",
      "Epoch 96/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 0.7794 - val_loss: 1.4359\n",
      "Epoch 97/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.7627 - val_loss: 1.4534\n",
      "Epoch 98/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.9155 - val_loss: 1.4093\n",
      "Epoch 99/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 0.8935 - val_loss: 1.4091\n",
      "Epoch 100/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 0.8066 - val_loss: 1.4231\n"
     ]
    }
   ],
   "source": [
    "hist = sequence_autoencoder.fit(X_train, y_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[mc, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.plot(hist.history['loss'], label='training')\n",
    "plt.plot(hist.history['val_loss'], label='testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'figures/{name}', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_autoencoder = load_model(f'Models/{name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.686771137026239"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sequence_autoencoder.predict(X_train).argmax(axis=-1)\n",
    "accuracy_score(y_train.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5255424222797928"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sequence_autoencoder.predict(X_test).argmax(axis=-1)\n",
    "accuracy_score(y_test.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Embedding-BiLSTM-BiLSTM Encoder, LSTM-Dense Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7qstfi85) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 206711<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 9.57MB of 9.57MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/scratch/users/udemir15/Bassline-Generator/generator/wandb/run-20210523_165709-7qstfi85/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/scratch/users/udemir15/Bassline-Generator/generator/wandb/run-20210523_165709-7qstfi85/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.80655</td></tr><tr><td>val_loss</td><td>1.42312</td></tr><tr><td>_runtime</td><td>319</td></tr><tr><td>_timestamp</td><td>1621778549</td></tr><tr><td>_step</td><td>99</td></tr><tr><td>best_val_loss</td><td>1.20776</td></tr><tr><td>best_epoch</td><td>35</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>_runtime</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>_timestamp</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>_step</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">bumbling-monkey-3</strong>: <a href=\"https://wandb.ai/nbg/Keras_runs/runs/7qstfi85\" target=\"_blank\">https://wandb.ai/nbg/Keras_runs/runs/7qstfi85</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:7qstfi85). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">zesty-universe-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/nbg/Keras_runs\" target=\"_blank\">https://wandb.ai/nbg/Keras_runs</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/nbg/Keras_runs/runs/3tnzvpqu\" target=\"_blank\">https://wandb.ai/nbg/Keras_runs/runs/3tnzvpqu</a><br/>\n",
       "                Run data is saved locally in <code>/scratch/users/udemir15/Bassline-Generator/generator/wandb/run-20210523_170234-3tnzvpqu</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(3tnzvpqu)</h1><iframe src=\"https://wandb.ai/nbg/Keras_runs/runs/3tnzvpqu\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2ba2d0182c50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='Keras_runs', entity='nbg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NBG_bilstm_stacked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timesteps = 64  # Length of your sequences\n",
    "embed_size = 16\n",
    "latent_dim = 256\n",
    "dropout = 0\n",
    "\n",
    "inputs = Input(shape=(timesteps,))\n",
    "embedded = Embedding(vocab_size, embed_size)(inputs)\n",
    "encoded = Bidirectional(LSTM(latent_dim, return_sequences=True, dropout=dropout))(embedded)\n",
    "encoded = Bidirectional(LSTM(latent_dim, dropout=dropout))(embedded)\n",
    "\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "#decoded = LSTM(latent_dim, return_sequences=True, dropout=dropout)(decoded)\n",
    "decoded = LSTM(latent_dim, return_sequences=True, dropout=dropout)(decoded)\n",
    "decoded = Dense(vocab_size, activation='softmax')(decoded)\n",
    "#decoded = argmax(decoded, axis=-1)\n",
    "#decoded = cast(decoded, float)\n",
    "#decoded = Reshape((decoded.shape[1], -1))(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 64, 16)            608       \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               559104    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 64, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64, 256)           787456    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64, 38)            9766      \n",
      "=================================================================\n",
      "Total params: 1,356,934\n",
      "Trainable params: 1,356,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs, encoded)\n",
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(2*latent_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layers = sequence_autoencoder.layers[-3:]\n",
    "decoded_input = decoder_layers[0](encoded_input)\n",
    "for decoder_layer in decoder_layers[1:]:\n",
    "    decoded_input = decoder_layer(decoded_input)\n",
    "# Create the decoder model\n",
    "decoder = Model(encoded_input, decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 64, 16)            608       \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               559104    \n",
      "=================================================================\n",
      "Total params: 559,712\n",
      "Trainable params: 559,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 64, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64, 256)           787456    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64, 38)            9766      \n",
      "=================================================================\n",
      "Total params: 797,222\n",
      "Trainable params: 797,222\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-3\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "wandb.config.learning_rate = learning_rate\n",
    "wandb.config.epochs = epochs\n",
    "wandb.config.batch_size = batch_size\n",
    "wandb.config.model = name\n",
    "\n",
    "mc = ModelCheckpoint(f'Models/{name}.hdf5', monitor='val_loss')\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "sequence_autoencoder.compile(optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "97/97 [==============================] - 6s 31ms/step - loss: 2.2115 - val_loss: 1.9052\n",
      "Epoch 2/100\n",
      "97/97 [==============================] - 2s 23ms/step - loss: 1.8821 - val_loss: 1.7987\n",
      "Epoch 3/100\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 1.7955 - val_loss: 1.6757\n",
      "Epoch 4/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.8636 - val_loss: 1.9517\n",
      "Epoch 5/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9748 - val_loss: 1.9349\n",
      "Epoch 6/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9674 - val_loss: 1.9329\n",
      "Epoch 7/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9582 - val_loss: 1.9346\n",
      "Epoch 8/100\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.9687 - val_loss: 1.9306\n",
      "Epoch 9/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9627 - val_loss: 1.9367\n",
      "Epoch 10/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9612 - val_loss: 1.9272\n",
      "Epoch 11/100\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.9652 - val_loss: 1.9298\n",
      "Epoch 12/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9531 - val_loss: 1.9329\n",
      "Epoch 13/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9616 - val_loss: 1.9320\n",
      "Epoch 14/100\n",
      "97/97 [==============================] - 1s 15ms/step - loss: 1.9699 - val_loss: 1.9250\n",
      "Epoch 15/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.9621 - val_loss: 1.9299\n",
      "Epoch 16/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9515 - val_loss: 1.9327\n",
      "Epoch 17/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9552 - val_loss: 1.9273\n",
      "Epoch 18/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9569 - val_loss: 1.9316\n",
      "Epoch 19/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.9473 - val_loss: 1.9270\n",
      "Epoch 20/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9532 - val_loss: 1.9291\n",
      "Epoch 21/100\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.9570 - val_loss: 1.9373\n",
      "Epoch 22/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9563 - val_loss: 1.9296\n",
      "Epoch 23/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9560 - val_loss: 1.9267\n",
      "Epoch 24/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9466 - val_loss: 1.9285\n",
      "Epoch 25/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9527 - val_loss: 1.9282\n",
      "Epoch 26/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9540 - val_loss: 1.9329\n",
      "Epoch 27/100\n",
      "97/97 [==============================] - 1s 15ms/step - loss: 1.9417 - val_loss: 1.9419\n",
      "Epoch 28/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9663 - val_loss: 1.9319\n",
      "Epoch 29/100\n",
      "97/97 [==============================] - 1s 15ms/step - loss: 1.9538 - val_loss: 1.9321\n",
      "Epoch 30/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.9482 - val_loss: 1.9288\n",
      "Epoch 31/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.9522 - val_loss: 1.9322\n",
      "Epoch 32/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9490 - val_loss: 1.9313\n",
      "Epoch 33/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.9528 - val_loss: 1.9259\n",
      "Epoch 34/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9655 - val_loss: 1.9298\n",
      "Epoch 35/100\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.9556 - val_loss: 1.9303\n",
      "Epoch 36/100\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.9553 - val_loss: 1.9349\n",
      "Epoch 37/100\n",
      "97/97 [==============================] - 2s 21ms/step - loss: 1.9430 - val_loss: 1.9320\n",
      "Epoch 38/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9527 - val_loss: 1.9282\n",
      "Epoch 39/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9555 - val_loss: 1.9271\n",
      "Epoch 40/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9636 - val_loss: 1.9276\n",
      "Epoch 41/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.9506 - val_loss: 1.9325\n",
      "Epoch 42/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9627 - val_loss: 1.9340\n",
      "Epoch 43/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9503 - val_loss: 1.9250\n",
      "Epoch 44/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9498 - val_loss: 1.9333\n",
      "Epoch 45/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9610 - val_loss: 1.9309\n",
      "Epoch 46/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.9489 - val_loss: 1.9286\n",
      "Epoch 47/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9526 - val_loss: 1.9294\n",
      "Epoch 48/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9592 - val_loss: 1.9310\n",
      "Epoch 49/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9546 - val_loss: 1.9245\n",
      "Epoch 50/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9557 - val_loss: 1.9280\n",
      "Epoch 51/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9560 - val_loss: 1.9275\n",
      "Epoch 52/100\n",
      "97/97 [==============================] - 2s 22ms/step - loss: 1.9577 - val_loss: 1.9331\n",
      "Epoch 53/100\n",
      "97/97 [==============================] - 2s 20ms/step - loss: 1.9539 - val_loss: 1.9268\n",
      "Epoch 54/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.9463 - val_loss: 1.9296\n",
      "Epoch 55/100\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.9429 - val_loss: 1.9282\n",
      "Epoch 56/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9536 - val_loss: 1.9293\n",
      "Epoch 57/100\n",
      "97/97 [==============================] - 1s 15ms/step - loss: 1.9551 - val_loss: 1.9300\n",
      "Epoch 58/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9514 - val_loss: 1.9330\n",
      "Epoch 59/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9684 - val_loss: 1.9276\n",
      "Epoch 60/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9574 - val_loss: 1.9292\n",
      "Epoch 61/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9574 - val_loss: 1.9303\n",
      "Epoch 62/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9513 - val_loss: 1.9276\n",
      "Epoch 63/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9535 - val_loss: 1.9294\n",
      "Epoch 64/100\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.9444 - val_loss: 1.9268\n",
      "Epoch 65/100\n",
      "97/97 [==============================] - 2s 21ms/step - loss: 1.9596 - val_loss: 1.9277\n",
      "Epoch 66/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.9359 - val_loss: 1.9247\n",
      "Epoch 67/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.9524 - val_loss: 1.9281\n",
      "Epoch 68/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9454 - val_loss: 1.9385\n",
      "Epoch 69/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9644 - val_loss: 1.9292\n",
      "Epoch 70/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9432 - val_loss: 1.9300\n",
      "Epoch 71/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9499 - val_loss: 1.9266\n",
      "Epoch 72/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9624 - val_loss: 1.9274\n",
      "Epoch 73/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9561 - val_loss: 1.9265\n",
      "Epoch 74/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9529 - val_loss: 1.9273\n",
      "Epoch 75/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9518 - val_loss: 1.9300\n",
      "Epoch 76/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9499 - val_loss: 1.9316\n",
      "Epoch 77/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9527 - val_loss: 1.9289\n",
      "Epoch 78/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9484 - val_loss: 1.9285\n",
      "Epoch 79/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9463 - val_loss: 1.9337\n",
      "Epoch 80/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9546 - val_loss: 1.9286\n",
      "Epoch 81/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9618 - val_loss: 1.9321\n",
      "Epoch 82/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9502 - val_loss: 1.9261\n",
      "Epoch 83/100\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.9478 - val_loss: 1.9269\n",
      "Epoch 84/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.9526 - val_loss: 1.9314\n",
      "Epoch 85/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.9640 - val_loss: 1.9245\n",
      "Epoch 86/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.9506 - val_loss: 1.9344\n",
      "Epoch 87/100\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.9621 - val_loss: 1.9323\n",
      "Epoch 88/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9431 - val_loss: 1.9278\n",
      "Epoch 89/100\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.9532 - val_loss: 1.9291\n",
      "Epoch 90/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9553 - val_loss: 1.9280\n",
      "Epoch 91/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9580 - val_loss: 1.9294\n",
      "Epoch 92/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9454 - val_loss: 1.9317\n",
      "Epoch 93/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9453 - val_loss: 1.9366\n",
      "Epoch 94/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9531 - val_loss: 1.9328\n",
      "Epoch 95/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.9452 - val_loss: 1.9306\n",
      "Epoch 96/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9554 - val_loss: 1.9295\n",
      "Epoch 97/100\n",
      "97/97 [==============================] - 2s 19ms/step - loss: 1.9591 - val_loss: 1.9297\n",
      "Epoch 98/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9485 - val_loss: 1.9280\n",
      "Epoch 99/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.9508 - val_loss: 1.9273\n",
      "Epoch 100/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.9468 - val_loss: 1.9277\n"
     ]
    }
   ],
   "source": [
    "hist = sequence_autoencoder.fit(X_train, y_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[mc, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.plot(hist.history['loss'], label='training')\n",
    "plt.plot(hist.history['val_loss'], label='testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'figures/{name}', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_autoencoder = load_model(f'Models/{name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46820841431810817"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sequence_autoencoder.predict(X_train).argmax(axis=-1)\n",
    "accuracy_score(y_train.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4697215025906736"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sequence_autoencoder.predict(X_test).argmax(axis=-1)\n",
    "accuracy_score(y_test.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 Embedding-GRU Encoder GRU-Dense Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3tnzvpqu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 207275<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 15.59MB of 15.59MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/scratch/users/udemir15/Bassline-Generator/generator/wandb/run-20210523_170234-3tnzvpqu/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/scratch/users/udemir15/Bassline-Generator/generator/wandb/run-20210523_170234-3tnzvpqu/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>1.95211</td></tr><tr><td>val_loss</td><td>1.92773</td></tr><tr><td>_runtime</td><td>649</td></tr><tr><td>_timestamp</td><td>1621779207</td></tr><tr><td>_step</td><td>99</td></tr><tr><td>best_val_loss</td><td>1.6757</td></tr><tr><td>best_epoch</td><td>2</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>val_loss</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>_runtime</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>_timestamp</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>_step</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">zesty-universe-4</strong>: <a href=\"https://wandb.ai/nbg/Keras_runs/runs/3tnzvpqu\" target=\"_blank\">https://wandb.ai/nbg/Keras_runs/runs/3tnzvpqu</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3tnzvpqu). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">noble-shadow-5</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/nbg/Keras_runs\" target=\"_blank\">https://wandb.ai/nbg/Keras_runs</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/nbg/Keras_runs/runs/1nwyjde6\" target=\"_blank\">https://wandb.ai/nbg/Keras_runs/runs/1nwyjde6</a><br/>\n",
       "                Run data is saved locally in <code>/scratch/users/udemir15/Bassline-Generator/generator/wandb/run-20210523_171331-1nwyjde6</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(1nwyjde6)</h1><iframe src=\"https://wandb.ai/nbg/Keras_runs/runs/1nwyjde6\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2ba2d188b990>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='Keras_runs', entity='nbg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NBG_gru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timesteps = 64  # Length of your sequences\n",
    "embed_size = 32\n",
    "latent_dim = 256\n",
    "dropout = 0\n",
    "\n",
    "inputs = Input(shape=(timesteps,))\n",
    "embedded = Embedding(vocab_size, embed_size)(inputs)\n",
    "#encoded = LSTM(latent_dim, return_sequences=True, dropout=dropout)(embedded)\n",
    "encoded = GRU(latent_dim, dropout=dropout)(embedded)\n",
    "\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "#decoded = LSTM(latent_dim, return_sequences=True, dropout=dropout)(decoded)\n",
    "decoded = GRU(latent_dim, return_sequences=True, dropout=dropout)(decoded)\n",
    "decoded = Dense(vocab_size, activation='softmax')(decoded)\n",
    "#decoded = argmax(decoded, axis=-1)\n",
    "#decoded = cast(decoded, float)\n",
    "#decoded = Reshape((decoded.shape[1], -1))(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 64, 32)            1216      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 256)               222720    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64, 256)           394752    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64, 38)            9766      \n",
      "=================================================================\n",
      "Total params: 628,454\n",
      "Trainable params: 628,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs, encoded)\n",
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(latent_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layers = sequence_autoencoder.layers[-3:]\n",
    "decoded_input = decoder_layers[0](encoded_input)\n",
    "for decoder_layer in decoder_layers[1:]:\n",
    "    decoded_input = decoder_layer(decoded_input)\n",
    "# Create the decoder model\n",
    "decoder = Model(encoded_input, decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 64, 32)            1216      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 256)               222720    \n",
      "=================================================================\n",
      "Total params: 223,936\n",
      "Trainable params: 223,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 256)]             0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64, 256)           394752    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64, 38)            9766      \n",
      "=================================================================\n",
      "Total params: 404,518\n",
      "Trainable params: 404,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-3\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "wandb.config.learning_rate = learning_rate\n",
    "wandb.config.epochs = epochs\n",
    "wandb.config.batch_size = batch_size\n",
    "wandb.config.model = name\n",
    "\n",
    "mc = ModelCheckpoint(f'Models/{name}.hdf5', monitor='val_loss')\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "sequence_autoencoder.compile(optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 5/97 [>.............................] - ETA: 7s - loss: 3.3130 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0167s vs `on_train_batch_end` time: 0.0403s). Check your callbacks.\n",
      "97/97 [==============================] - 5s 25ms/step - loss: 2.2368 - val_loss: 1.7472\n",
      "Epoch 2/100\n",
      "97/97 [==============================] - 2s 18ms/step - loss: 1.6990 - val_loss: 1.5855\n",
      "Epoch 3/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.5627 - val_loss: 1.4964\n",
      "Epoch 4/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.4840 - val_loss: 1.4545\n",
      "Epoch 5/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.4461 - val_loss: 1.4338\n",
      "Epoch 6/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.4251 - val_loss: 1.4145\n",
      "Epoch 7/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.4108 - val_loss: 1.4016\n",
      "Epoch 8/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.4000 - val_loss: 1.4005\n",
      "Epoch 9/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.3866 - val_loss: 1.3868\n",
      "Epoch 10/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3941 - val_loss: 1.3931\n",
      "Epoch 11/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3902 - val_loss: 1.3747\n",
      "Epoch 12/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.3646 - val_loss: 1.3760\n",
      "Epoch 13/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3751 - val_loss: 1.3760\n",
      "Epoch 14/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3623 - val_loss: 1.3588\n",
      "Epoch 15/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3526 - val_loss: 1.3658\n",
      "Epoch 16/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3491 - val_loss: 1.3650\n",
      "Epoch 17/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3475 - val_loss: 1.3528\n",
      "Epoch 18/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3609 - val_loss: 1.3808\n",
      "Epoch 19/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3716 - val_loss: 1.3530\n",
      "Epoch 20/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3401 - val_loss: 1.3455\n",
      "Epoch 21/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3302 - val_loss: 1.3409\n",
      "Epoch 22/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3409 - val_loss: 1.4194\n",
      "Epoch 23/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3951 - val_loss: 1.3441\n",
      "Epoch 24/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3540 - val_loss: 1.3440\n",
      "Epoch 25/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3252 - val_loss: 1.3385\n",
      "Epoch 26/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3143 - val_loss: 1.3365\n",
      "Epoch 27/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.3213 - val_loss: 1.3166\n",
      "Epoch 28/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.4975 - val_loss: 1.4027\n",
      "Epoch 29/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3750 - val_loss: 1.3620\n",
      "Epoch 30/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.4441 - val_loss: 1.3870\n",
      "Epoch 31/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.4176 - val_loss: 1.4152\n",
      "Epoch 32/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3986 - val_loss: 1.3785\n",
      "Epoch 33/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3747 - val_loss: 1.3652\n",
      "Epoch 34/100\n",
      "97/97 [==============================] - 1s 15ms/step - loss: 1.3661 - val_loss: 1.3671\n",
      "Epoch 35/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3568 - val_loss: 1.3540\n",
      "Epoch 36/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3527 - val_loss: 1.3785\n",
      "Epoch 37/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3612 - val_loss: 1.3491\n",
      "Epoch 38/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3314 - val_loss: 1.3436\n",
      "Epoch 39/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3371 - val_loss: 1.3393\n",
      "Epoch 40/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.3275 - val_loss: 1.3466\n",
      "Epoch 41/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3272 - val_loss: 1.3322\n",
      "Epoch 42/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3291 - val_loss: 1.3300\n",
      "Epoch 43/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3426 - val_loss: 1.3321\n",
      "Epoch 44/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3221 - val_loss: 1.3298\n",
      "Epoch 45/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3203 - val_loss: 1.3368\n",
      "Epoch 46/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.3160 - val_loss: 1.3230\n",
      "Epoch 47/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3181 - val_loss: 1.3251\n",
      "Epoch 48/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3031 - val_loss: 1.3175\n",
      "Epoch 49/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3034 - val_loss: 1.3321\n",
      "Epoch 50/100\n",
      "97/97 [==============================] - 2s 17ms/step - loss: 1.3071 - val_loss: 1.3286\n",
      "Epoch 51/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.4755 - val_loss: 1.5168\n",
      "Epoch 52/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.4893 - val_loss: 1.4351\n",
      "Epoch 53/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.4131 - val_loss: 1.4001\n",
      "Epoch 54/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3883 - val_loss: 1.4098\n",
      "Epoch 55/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.4080 - val_loss: 1.3985\n",
      "Epoch 56/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3919 - val_loss: 1.3755\n",
      "Epoch 57/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3697 - val_loss: 1.3734\n",
      "Epoch 58/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3623 - val_loss: 1.3650\n",
      "Epoch 59/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3733 - val_loss: 1.3572\n",
      "Epoch 60/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3504 - val_loss: 1.3559\n",
      "Epoch 61/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3766 - val_loss: 1.3647\n",
      "Epoch 62/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3613 - val_loss: 1.3604\n",
      "Epoch 63/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3449 - val_loss: 1.3529\n",
      "Epoch 64/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3670 - val_loss: 1.3742\n",
      "Epoch 65/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3610 - val_loss: 1.3584\n",
      "Epoch 66/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3416 - val_loss: 1.3460\n",
      "Epoch 67/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3288 - val_loss: 1.3443\n",
      "Epoch 68/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3271 - val_loss: 1.3342\n",
      "Epoch 69/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3315 - val_loss: 1.3400\n",
      "Epoch 70/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.4060 - val_loss: 1.4623\n",
      "Epoch 71/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.4267 - val_loss: 1.3960\n",
      "Epoch 72/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3883 - val_loss: 1.3863\n",
      "Epoch 73/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3773 - val_loss: 1.3813\n",
      "Epoch 74/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.3648 - val_loss: 1.3741\n",
      "Epoch 75/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3664 - val_loss: 1.3667\n",
      "Epoch 76/100\n",
      "97/97 [==============================] - 2s 16ms/step - loss: 1.3565 - val_loss: 1.3672\n",
      "Epoch 77/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3418 - val_loss: 1.3589\n",
      "Epoch 78/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3568 - val_loss: 1.3570\n",
      "Epoch 79/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3568 - val_loss: 1.3541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3433 - val_loss: 1.3596\n",
      "Epoch 81/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3498 - val_loss: 1.3548\n",
      "Epoch 82/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3471 - val_loss: 1.3444\n",
      "Epoch 83/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3371 - val_loss: 1.3433\n",
      "Epoch 84/100\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.3367 - val_loss: 1.3456\n",
      "Epoch 85/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3268 - val_loss: 1.4296\n",
      "Epoch 86/100\n",
      "97/97 [==============================] - 1s 14ms/step - loss: 1.3904 - val_loss: 1.3585\n",
      "Epoch 87/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3503 - val_loss: 1.3413\n",
      "Epoch 88/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3332 - val_loss: 1.3483\n",
      "Epoch 89/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3510 - val_loss: 1.3328\n",
      "Epoch 90/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3264 - val_loss: 1.3277\n",
      "Epoch 91/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3068 - val_loss: 1.3278\n",
      "Epoch 92/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3311 - val_loss: 1.3271\n",
      "Epoch 93/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3177 - val_loss: 1.3309\n",
      "Epoch 94/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3123 - val_loss: 1.3179\n",
      "Epoch 95/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3061 - val_loss: 1.3214\n",
      "Epoch 96/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3549 - val_loss: 1.3915\n",
      "Epoch 97/100\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.3721 - val_loss: 1.3567\n",
      "Epoch 98/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3450 - val_loss: 1.3426\n",
      "Epoch 99/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3236 - val_loss: 1.3198\n",
      "Epoch 100/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.3519 - val_loss: 1.3284\n"
     ]
    }
   ],
   "source": [
    "hist = sequence_autoencoder.fit(X_train, y_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[mc, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.plot(hist.history['loss'], label='training')\n",
    "plt.plot(hist.history['val_loss'], label='testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'figures/{name}', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_autoencoder = load_model(f'Models/{name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5053956106252024"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sequence_autoencoder.predict(X_train).argmax(axis=-1)\n",
    "accuracy_score(y_train.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50390625"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sequence_autoencoder.predict(X_test).argmax(axis=-1)\n",
    "accuracy_score(y_test.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Embedding-Conv1D-MaxPooling1D-Conv1D-MaxPooling1D-Conv1D Encoder Conv1D-Upsampling1D-Conv1D-Upsampling1D-Conv1D-Dense Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: raraz15 (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">eternal-meadow-10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/nbg/Keras_runs\" target=\"_blank\">https://wandb.ai/nbg/Keras_runs</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/nbg/Keras_runs/runs/18t93wvf\" target=\"_blank\">https://wandb.ai/nbg/Keras_runs/runs/18t93wvf</a><br/>\n",
       "                Run data is saved locally in <code>/scratch/users/udemir15/Bassline-Generator/generator/wandb/run-20210526_031716-18t93wvf</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(18t93wvf)</h1><iframe src=\"https://wandb.ai/nbg/Keras_runs/runs/18t93wvf\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2b86a5193310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='Keras_runs', entity='nbg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NBG_conv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timesteps = 64  # Length of your sequences\n",
    "embed_size = 32\n",
    "latent_dim = 256\n",
    "dropout = 0\n",
    "\n",
    "inputs = Input(shape=(timesteps,))\n",
    "embedded = Embedding(vocab_size, embed_size)(inputs)\n",
    "encoded = Conv1D(latent_dim, 3, activation='relu', padding='same')(embedded)\n",
    "encoded = MaxPooling1D(2, padding='same')(encoded)\n",
    "encoded = Conv1D(2 * latent_dim, 3, activation='relu', padding='same')(encoded)\n",
    "encoded = MaxPooling1D(2, padding='same')(encoded)\n",
    "\n",
    "decoded = Conv1D(2 * latent_dim, 3, activation='relu', padding='same')(encoded)\n",
    "decoded = UpSampling1D(2)(decoded)\n",
    "decoded = Conv1D(latent_dim, 3, activation='relu', padding='same')(decoded)\n",
    "decoded = UpSampling1D(2)(decoded)\n",
    "decoded = Conv1D(1, 3, activation='relu', padding='same')(decoded)\n",
    "decoded = Dense(vocab_size, activation='softmax')(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 64, 32)            1216      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 64, 256)           24832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 32, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 32, 512)           393728    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 512)           786944    \n",
      "_________________________________________________________________\n",
      "up_sampling1d (UpSampling1D) (None, 32, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 32, 256)           393472    \n",
      "_________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1 (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 64, 1)             769       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64, 38)            76        \n",
      "=================================================================\n",
      "Total params: 1,601,037\n",
      "Trainable params: 1,601,037\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs, encoded)\n",
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(timesteps//4,2*latent_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layers = sequence_autoencoder.layers[-6:]\n",
    "decoded_input = decoder_layers[0](encoded_input)\n",
    "for decoder_layer in decoder_layers[1:]:\n",
    "    decoded_input = decoder_layer(decoded_input)\n",
    "# Create the decoder model\n",
    "decoder = Model(encoded_input, decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 64, 32)            1216      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 64, 256)           24832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 32, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 32, 512)           393728    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 16, 512)           0         \n",
      "=================================================================\n",
      "Total params: 419,776\n",
      "Trainable params: 419,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 16, 512)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 512)           786944    \n",
      "_________________________________________________________________\n",
      "up_sampling1d (UpSampling1D) (None, 32, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 32, 256)           393472    \n",
      "_________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1 (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 64, 1)             769       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64, 38)            76        \n",
      "=================================================================\n",
      "Total params: 1,181,261\n",
      "Trainable params: 1,181,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-3\n",
    "epochs = 5000\n",
    "batch_size = 32\n",
    "wandb.config.learning_rate = learning_rate\n",
    "wandb.config.epochs = epochs\n",
    "wandb.config.batch_size = batch_size\n",
    "wandb.config.model = name\n",
    "\n",
    "mc = ModelCheckpoint(f'Models/{name}.hdf5', monitor='val_loss')\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "sequence_autoencoder.compile(optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "97/97 [==============================] - 3s 18ms/step - loss: 3.4752 - val_loss: 3.0354\n",
      "Epoch 2/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.9158 - val_loss: 2.5889\n",
      "Epoch 3/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.5211 - val_loss: 2.3023\n",
      "Epoch 4/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.2694 - val_loss: 2.1433\n",
      "Epoch 5/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.1391 - val_loss: 2.0614\n",
      "Epoch 6/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.0683 - val_loss: 2.0187\n",
      "Epoch 7/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.0352 - val_loss: 1.9954\n",
      "Epoch 8/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.0065 - val_loss: 1.9816\n",
      "Epoch 9/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 2.0104 - val_loss: 1.9724\n",
      "Epoch 10/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9838 - val_loss: 1.9664\n",
      "Epoch 11/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9831 - val_loss: 1.9620\n",
      "Epoch 12/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9819 - val_loss: 1.9588\n",
      "Epoch 13/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9760 - val_loss: 1.9562\n",
      "Epoch 14/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9759 - val_loss: 1.9543\n",
      "Epoch 15/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9930 - val_loss: 1.9527\n",
      "Epoch 16/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9792 - val_loss: 1.9514\n",
      "Epoch 17/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9677 - val_loss: 1.9504\n",
      "Epoch 18/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9669 - val_loss: 1.9496\n",
      "Epoch 19/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9789 - val_loss: 1.9488\n",
      "Epoch 20/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9752 - val_loss: 1.9483\n",
      "Epoch 21/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9910 - val_loss: 1.9479\n",
      "Epoch 22/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9758 - val_loss: 1.9474\n",
      "Epoch 23/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9885 - val_loss: 1.9471\n",
      "Epoch 24/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9690 - val_loss: 1.9468\n",
      "Epoch 25/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9678 - val_loss: 1.9464\n",
      "Epoch 26/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9771 - val_loss: 1.9462\n",
      "Epoch 27/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9684 - val_loss: 1.9461\n",
      "Epoch 28/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9682 - val_loss: 1.9459\n",
      "Epoch 29/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9631 - val_loss: 1.9457\n",
      "Epoch 30/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9763 - val_loss: 1.9456\n",
      "Epoch 31/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9718 - val_loss: 1.9454\n",
      "Epoch 32/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9608 - val_loss: 1.9453\n",
      "Epoch 33/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9811 - val_loss: 1.9452\n",
      "Epoch 34/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9748 - val_loss: 1.9451\n",
      "Epoch 35/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9600 - val_loss: 1.9452\n",
      "Epoch 36/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9621 - val_loss: 1.9451\n",
      "Epoch 37/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9801 - val_loss: 1.9450\n",
      "Epoch 38/5000\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.9586 - val_loss: 1.9449\n",
      "Epoch 39/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9692 - val_loss: 1.9451\n",
      "Epoch 40/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9548 - val_loss: 1.9450\n",
      "Epoch 41/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9599 - val_loss: 1.9449\n",
      "Epoch 42/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9649 - val_loss: 1.9450\n",
      "Epoch 43/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9679 - val_loss: 1.9449\n",
      "Epoch 44/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9743 - val_loss: 1.9448\n",
      "Epoch 45/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9700 - val_loss: 1.9450\n",
      "Epoch 46/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9752 - val_loss: 1.9449\n",
      "Epoch 47/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9743 - val_loss: 1.9448\n",
      "Epoch 48/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9704 - val_loss: 1.9448\n",
      "Epoch 49/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9628 - val_loss: 1.9449\n",
      "Epoch 50/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9694 - val_loss: 1.9448\n",
      "Epoch 51/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9675 - val_loss: 1.9449\n",
      "Epoch 52/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9730 - val_loss: 1.9447\n",
      "Epoch 53/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9647 - val_loss: 1.9447\n",
      "Epoch 54/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9663 - val_loss: 1.9447\n",
      "Epoch 55/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9633 - val_loss: 1.9447\n",
      "Epoch 56/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9707 - val_loss: 1.9447\n",
      "Epoch 57/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9677 - val_loss: 1.9447\n",
      "Epoch 58/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9663 - val_loss: 1.9447\n",
      "Epoch 59/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9710 - val_loss: 1.9447\n",
      "Epoch 60/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9640 - val_loss: 1.9448\n",
      "Epoch 61/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9663 - val_loss: 1.9449\n",
      "Epoch 62/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9627 - val_loss: 1.9448\n",
      "Epoch 63/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9544 - val_loss: 1.9448\n",
      "Epoch 64/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9794 - val_loss: 1.9450\n",
      "Epoch 65/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9658 - val_loss: 1.9448\n",
      "Epoch 66/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9680 - val_loss: 1.9448\n",
      "Epoch 67/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9711 - val_loss: 1.9448\n",
      "Epoch 68/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9626 - val_loss: 1.9449\n",
      "Epoch 69/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9746 - val_loss: 1.9448\n",
      "Epoch 70/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9586 - val_loss: 1.9449\n",
      "Epoch 71/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9617 - val_loss: 1.9450\n",
      "Epoch 72/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9754 - val_loss: 1.9448\n",
      "Epoch 73/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9737 - val_loss: 1.9449\n",
      "Epoch 74/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9588 - val_loss: 1.9448\n",
      "Epoch 75/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9694 - val_loss: 1.9448\n",
      "Epoch 76/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9729 - val_loss: 1.9448\n",
      "Epoch 77/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9754 - val_loss: 1.9448\n",
      "Epoch 78/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9678 - val_loss: 1.9448\n",
      "Epoch 79/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9552 - val_loss: 1.9449\n",
      "Epoch 80/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9586 - val_loss: 1.9449\n",
      "Epoch 81/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9622 - val_loss: 1.9448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9780 - val_loss: 1.9450\n",
      "Epoch 83/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9752 - val_loss: 1.9449\n",
      "Epoch 84/5000\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 1.9619 - val_loss: 1.9449\n",
      "Epoch 85/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9637 - val_loss: 1.9449\n",
      "Epoch 86/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9758 - val_loss: 1.9449\n",
      "Epoch 87/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9781 - val_loss: 1.9448\n",
      "Epoch 88/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9618 - val_loss: 1.9448\n",
      "Epoch 89/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9764 - val_loss: 1.9448\n",
      "Epoch 90/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9670 - val_loss: 1.9448\n",
      "Epoch 91/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9692 - val_loss: 1.9448\n",
      "Epoch 92/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9665 - val_loss: 1.9449\n",
      "Epoch 93/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9649 - val_loss: 1.9450\n",
      "Epoch 94/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9693 - val_loss: 1.9449\n",
      "Epoch 95/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9605 - val_loss: 1.9448\n",
      "Epoch 96/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9758 - val_loss: 1.9448\n",
      "Epoch 97/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9770 - val_loss: 1.9448\n",
      "Epoch 98/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9667 - val_loss: 1.9448\n",
      "Epoch 99/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9698 - val_loss: 1.9449\n",
      "Epoch 100/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9757 - val_loss: 1.9448\n",
      "Epoch 101/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9574 - val_loss: 1.9447\n",
      "Epoch 102/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9723 - val_loss: 1.9449\n",
      "Epoch 103/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9740 - val_loss: 1.9449\n",
      "Epoch 104/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9661 - val_loss: 1.9448\n",
      "Epoch 105/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9641 - val_loss: 1.9448\n",
      "Epoch 106/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9740 - val_loss: 1.9449\n",
      "Epoch 107/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9548 - val_loss: 1.9450\n",
      "Epoch 108/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9607 - val_loss: 1.9450\n",
      "Epoch 109/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9689 - val_loss: 1.9449\n",
      "Epoch 110/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9722 - val_loss: 1.9448\n",
      "Epoch 111/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9775 - val_loss: 1.9447\n",
      "Epoch 112/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9620 - val_loss: 1.9448\n",
      "Epoch 113/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9748 - val_loss: 1.9447\n",
      "Epoch 114/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9802 - val_loss: 1.9448\n",
      "Epoch 115/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9739 - val_loss: 1.9448\n",
      "Epoch 116/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9649 - val_loss: 1.9449\n",
      "Epoch 117/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9777 - val_loss: 1.9448\n",
      "Epoch 118/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9784 - val_loss: 1.9448\n",
      "Epoch 119/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9756 - val_loss: 1.9449\n",
      "Epoch 120/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9650 - val_loss: 1.9448\n",
      "Epoch 121/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9720 - val_loss: 1.9449\n",
      "Epoch 122/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9599 - val_loss: 1.9449\n",
      "Epoch 123/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9678 - val_loss: 1.9447\n",
      "Epoch 124/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9666 - val_loss: 1.9449\n",
      "Epoch 125/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9649 - val_loss: 1.9449\n",
      "Epoch 126/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9692 - val_loss: 1.9448\n",
      "Epoch 127/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9721 - val_loss: 1.9448\n",
      "Epoch 128/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9600 - val_loss: 1.9448\n",
      "Epoch 129/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9765 - val_loss: 1.9448\n",
      "Epoch 130/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9665 - val_loss: 1.9450\n",
      "Epoch 131/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9620 - val_loss: 1.9449\n",
      "Epoch 132/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9710 - val_loss: 1.9448\n",
      "Epoch 133/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9670 - val_loss: 1.9448\n",
      "Epoch 134/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9695 - val_loss: 1.9448\n",
      "Epoch 135/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9775 - val_loss: 1.9447\n",
      "Epoch 136/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9751 - val_loss: 1.9448\n",
      "Epoch 137/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9691 - val_loss: 1.9448\n",
      "Epoch 138/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9787 - val_loss: 1.9448\n",
      "Epoch 139/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9603 - val_loss: 1.9448\n",
      "Epoch 140/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9737 - val_loss: 1.9448\n",
      "Epoch 141/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9698 - val_loss: 1.9449\n",
      "Epoch 142/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9737 - val_loss: 1.9448\n",
      "Epoch 143/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9638 - val_loss: 1.9449\n",
      "Epoch 144/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9601 - val_loss: 1.9450\n",
      "Epoch 145/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9760 - val_loss: 1.9449\n",
      "Epoch 146/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9733 - val_loss: 1.9449\n",
      "Epoch 147/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9605 - val_loss: 1.9449\n",
      "Epoch 148/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9770 - val_loss: 1.9448\n",
      "Epoch 149/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9614 - val_loss: 1.9451\n",
      "Epoch 150/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9677 - val_loss: 1.9448\n",
      "Epoch 151/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9709 - val_loss: 1.9449\n",
      "Epoch 152/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9685 - val_loss: 1.9449\n",
      "Epoch 153/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9673 - val_loss: 1.9448\n",
      "Epoch 154/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9655 - val_loss: 1.9448\n",
      "Epoch 155/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9652 - val_loss: 1.9449\n",
      "Epoch 156/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9776 - val_loss: 1.9448\n",
      "Epoch 157/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9553 - val_loss: 1.9448\n",
      "Epoch 158/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9616 - val_loss: 1.9447\n",
      "Epoch 159/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9671 - val_loss: 1.9449\n",
      "Epoch 160/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9715 - val_loss: 1.9448\n",
      "Epoch 161/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9713 - val_loss: 1.9447\n",
      "Epoch 162/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9683 - val_loss: 1.9448\n",
      "Epoch 163/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9610 - val_loss: 1.9450\n",
      "Epoch 164/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9767 - val_loss: 1.9450\n",
      "Epoch 165/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9737 - val_loss: 1.9447\n",
      "Epoch 166/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9641 - val_loss: 1.9450\n",
      "Epoch 167/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9726 - val_loss: 1.9450\n",
      "Epoch 168/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9627 - val_loss: 1.9450\n",
      "Epoch 169/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9690 - val_loss: 1.9447\n",
      "Epoch 170/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9708 - val_loss: 1.9448\n",
      "Epoch 171/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9632 - val_loss: 1.9449\n",
      "Epoch 172/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9592 - val_loss: 1.9450\n",
      "Epoch 173/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9657 - val_loss: 1.9448\n",
      "Epoch 174/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9770 - val_loss: 1.9448\n",
      "Epoch 175/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9680 - val_loss: 1.9448\n",
      "Epoch 176/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9479 - val_loss: 1.9450\n",
      "Epoch 177/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9733 - val_loss: 1.9447\n",
      "Epoch 178/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9631 - val_loss: 1.9449\n",
      "Epoch 179/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9673 - val_loss: 1.9448\n",
      "Epoch 180/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9665 - val_loss: 1.9448\n",
      "Epoch 181/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9691 - val_loss: 1.9447\n",
      "Epoch 182/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9646 - val_loss: 1.9449\n",
      "Epoch 183/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9721 - val_loss: 1.9447\n",
      "Epoch 184/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9683 - val_loss: 1.9447\n",
      "Epoch 185/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9771 - val_loss: 1.9448\n",
      "Epoch 186/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9737 - val_loss: 1.9448\n",
      "Epoch 187/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9727 - val_loss: 1.9448\n",
      "Epoch 188/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9731 - val_loss: 1.9447\n",
      "Epoch 189/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9638 - val_loss: 1.9448\n",
      "Epoch 190/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9789 - val_loss: 1.9448\n",
      "Epoch 191/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9747 - val_loss: 1.9447\n",
      "Epoch 192/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9707 - val_loss: 1.9447\n",
      "Epoch 193/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9682 - val_loss: 1.9448\n",
      "Epoch 194/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9689 - val_loss: 1.9449\n",
      "Epoch 195/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9699 - val_loss: 1.9448\n",
      "Epoch 196/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9626 - val_loss: 1.9448\n",
      "Epoch 197/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9688 - val_loss: 1.9448\n",
      "Epoch 198/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9762 - val_loss: 1.9448\n",
      "Epoch 199/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9734 - val_loss: 1.9449\n",
      "Epoch 200/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9683 - val_loss: 1.9449\n",
      "Epoch 201/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9620 - val_loss: 1.9449\n",
      "Epoch 202/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9626 - val_loss: 1.9449\n",
      "Epoch 203/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9581 - val_loss: 1.9449\n",
      "Epoch 204/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9754 - val_loss: 1.9449\n",
      "Epoch 205/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9715 - val_loss: 1.9448\n",
      "Epoch 206/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9645 - val_loss: 1.9448\n",
      "Epoch 207/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9737 - val_loss: 1.9448\n",
      "Epoch 208/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9637 - val_loss: 1.9449\n",
      "Epoch 209/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9658 - val_loss: 1.9449\n",
      "Epoch 210/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9695 - val_loss: 1.9448\n",
      "Epoch 211/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9706 - val_loss: 1.9447\n",
      "Epoch 212/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9654 - val_loss: 1.9448\n",
      "Epoch 213/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9682 - val_loss: 1.9447\n",
      "Epoch 214/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9716 - val_loss: 1.9448\n",
      "Epoch 215/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9714 - val_loss: 1.9448\n",
      "Epoch 216/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9659 - val_loss: 1.9448\n",
      "Epoch 217/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9765 - val_loss: 1.9447\n",
      "Epoch 218/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9590 - val_loss: 1.9452\n",
      "Epoch 219/5000\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.9732 - val_loss: 1.9446\n",
      "Epoch 220/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9730 - val_loss: 1.9447\n",
      "Epoch 221/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9477 - val_loss: 1.9449\n",
      "Epoch 222/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9573 - val_loss: 1.9449\n",
      "Epoch 223/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9668 - val_loss: 1.9449\n",
      "Epoch 224/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9650 - val_loss: 1.9450\n",
      "Epoch 225/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9675 - val_loss: 1.9449\n",
      "Epoch 226/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9701 - val_loss: 1.9449\n",
      "Epoch 227/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9663 - val_loss: 1.9449\n",
      "Epoch 228/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9680 - val_loss: 1.9448\n",
      "Epoch 229/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9751 - val_loss: 1.9451\n",
      "Epoch 230/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9586 - val_loss: 1.9447\n",
      "Epoch 231/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9777 - val_loss: 1.9450\n",
      "Epoch 232/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9639 - val_loss: 1.9450\n",
      "Epoch 233/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9727 - val_loss: 1.9448\n",
      "Epoch 234/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9628 - val_loss: 1.9447\n",
      "Epoch 235/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9644 - val_loss: 1.9448\n",
      "Epoch 236/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9647 - val_loss: 1.9448\n",
      "Epoch 237/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9637 - val_loss: 1.9450\n",
      "Epoch 238/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9676 - val_loss: 1.9449\n",
      "Epoch 239/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9716 - val_loss: 1.9449\n",
      "Epoch 240/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9685 - val_loss: 1.9447\n",
      "Epoch 241/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9742 - val_loss: 1.9449\n",
      "Epoch 242/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9783 - val_loss: 1.9448\n",
      "Epoch 243/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9648 - val_loss: 1.9448\n",
      "Epoch 244/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9637 - val_loss: 1.9448\n",
      "Epoch 245/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9601 - val_loss: 1.9448\n",
      "Epoch 246/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9830 - val_loss: 1.9448\n",
      "Epoch 247/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9759 - val_loss: 1.9449\n",
      "Epoch 248/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9683 - val_loss: 1.9448\n",
      "Epoch 249/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9645 - val_loss: 1.9448\n",
      "Epoch 250/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9622 - val_loss: 1.9448\n",
      "Epoch 251/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9787 - val_loss: 1.9449\n",
      "Epoch 252/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9789 - val_loss: 1.9448\n",
      "Epoch 253/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9610 - val_loss: 1.9449\n",
      "Epoch 254/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9713 - val_loss: 1.9447\n",
      "Epoch 255/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9725 - val_loss: 1.9448\n",
      "Epoch 256/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9716 - val_loss: 1.9448\n",
      "Epoch 257/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9744 - val_loss: 1.9448\n",
      "Epoch 258/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9778 - val_loss: 1.9448\n",
      "Epoch 259/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9764 - val_loss: 1.9448\n",
      "Epoch 260/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9698 - val_loss: 1.9448\n",
      "Epoch 261/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9566 - val_loss: 1.9449\n",
      "Epoch 262/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9760 - val_loss: 1.9449\n",
      "Epoch 263/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9530 - val_loss: 1.9449\n",
      "Epoch 264/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9609 - val_loss: 1.9447\n",
      "Epoch 265/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9847 - val_loss: 1.9448\n",
      "Epoch 266/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9717 - val_loss: 1.9447\n",
      "Epoch 267/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9685 - val_loss: 1.9449\n",
      "Epoch 268/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9646 - val_loss: 1.9448\n",
      "Epoch 269/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9580 - val_loss: 1.9448\n",
      "Epoch 270/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9621 - val_loss: 1.9448\n",
      "Epoch 271/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9803 - val_loss: 1.9447\n",
      "Epoch 272/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9795 - val_loss: 1.9448\n",
      "Epoch 273/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9686 - val_loss: 1.9447\n",
      "Epoch 274/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9613 - val_loss: 1.9449\n",
      "Epoch 275/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9577 - val_loss: 1.9450\n",
      "Epoch 276/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9732 - val_loss: 1.9449\n",
      "Epoch 277/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9731 - val_loss: 1.9449\n",
      "Epoch 278/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9599 - val_loss: 1.9448\n",
      "Epoch 279/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9688 - val_loss: 1.9448\n",
      "Epoch 280/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9702 - val_loss: 1.9449\n",
      "Epoch 281/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9729 - val_loss: 1.9449\n",
      "Epoch 282/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9511 - val_loss: 1.9448\n",
      "Epoch 283/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9693 - val_loss: 1.9447\n",
      "Epoch 284/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9762 - val_loss: 1.9448\n",
      "Epoch 285/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9572 - val_loss: 1.9449\n",
      "Epoch 286/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9765 - val_loss: 1.9449\n",
      "Epoch 287/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9752 - val_loss: 1.9448\n",
      "Epoch 288/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9623 - val_loss: 1.9448\n",
      "Epoch 289/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9690 - val_loss: 1.9448\n",
      "Epoch 290/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9707 - val_loss: 1.9448\n",
      "Epoch 291/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9791 - val_loss: 1.9450\n",
      "Epoch 292/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9669 - val_loss: 1.9451\n",
      "Epoch 293/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9645 - val_loss: 1.9448\n",
      "Epoch 294/5000\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 1.9529 - val_loss: 1.9452\n",
      "Epoch 295/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9603 - val_loss: 1.9448\n",
      "Epoch 296/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9656 - val_loss: 1.9449\n",
      "Epoch 297/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9649 - val_loss: 1.9448\n",
      "Epoch 298/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9712 - val_loss: 1.9448\n",
      "Epoch 299/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9707 - val_loss: 1.9448\n",
      "Epoch 300/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9605 - val_loss: 1.9448\n",
      "Epoch 301/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9775 - val_loss: 1.9447\n",
      "Epoch 302/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9776 - val_loss: 1.9448\n",
      "Epoch 303/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9609 - val_loss: 1.9447\n",
      "Epoch 304/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9751 - val_loss: 1.9448\n",
      "Epoch 305/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9645 - val_loss: 1.9447\n",
      "Epoch 306/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9754 - val_loss: 1.9449\n",
      "Epoch 307/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9706 - val_loss: 1.9448\n",
      "Epoch 308/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9817 - val_loss: 1.9448\n",
      "Epoch 309/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9617 - val_loss: 1.9448\n",
      "Epoch 310/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9706 - val_loss: 1.9448\n",
      "Epoch 311/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9685 - val_loss: 1.9448\n",
      "Epoch 312/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9614 - val_loss: 1.9448\n",
      "Epoch 313/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9731 - val_loss: 1.9447\n",
      "Epoch 314/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9647 - val_loss: 1.9448\n",
      "Epoch 315/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9658 - val_loss: 1.9449\n",
      "Epoch 316/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9694 - val_loss: 1.9448\n",
      "Epoch 317/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9779 - val_loss: 1.9448\n",
      "Epoch 318/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9574 - val_loss: 1.9449\n",
      "Epoch 319/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9743 - val_loss: 1.9447\n",
      "Epoch 320/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9675 - val_loss: 1.9447\n",
      "Epoch 321/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9727 - val_loss: 1.9448\n",
      "Epoch 322/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9769 - val_loss: 1.9447\n",
      "Epoch 323/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9598 - val_loss: 1.9448\n",
      "Epoch 324/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9627 - val_loss: 1.9447\n",
      "Epoch 325/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9673 - val_loss: 1.9448\n",
      "Epoch 326/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9735 - val_loss: 1.9448\n",
      "Epoch 327/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9645 - val_loss: 1.9449\n",
      "Epoch 328/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9749 - val_loss: 1.9449\n",
      "Epoch 329/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9649 - val_loss: 1.9449\n",
      "Epoch 330/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9641 - val_loss: 1.9449\n",
      "Epoch 331/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9618 - val_loss: 1.9448\n",
      "Epoch 332/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9649 - val_loss: 1.9449\n",
      "Epoch 333/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9619 - val_loss: 1.9449\n",
      "Epoch 334/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9569 - val_loss: 1.9448\n",
      "Epoch 335/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9653 - val_loss: 1.9448\n",
      "Epoch 336/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9734 - val_loss: 1.9449\n",
      "Epoch 337/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9789 - val_loss: 1.9447\n",
      "Epoch 338/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9609 - val_loss: 1.9448\n",
      "Epoch 339/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9530 - val_loss: 1.9448\n",
      "Epoch 340/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9699 - val_loss: 1.9447\n",
      "Epoch 341/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9675 - val_loss: 1.9448\n",
      "Epoch 342/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9728 - val_loss: 1.9448\n",
      "Epoch 343/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9672 - val_loss: 1.9448\n",
      "Epoch 344/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9561 - val_loss: 1.9449\n",
      "Epoch 345/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9569 - val_loss: 1.9448\n",
      "Epoch 346/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9664 - val_loss: 1.9448\n",
      "Epoch 347/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9682 - val_loss: 1.9448\n",
      "Epoch 348/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9654 - val_loss: 1.9448\n",
      "Epoch 349/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9670 - val_loss: 1.9448\n",
      "Epoch 350/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9604 - val_loss: 1.9448\n",
      "Epoch 351/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9686 - val_loss: 1.9448\n",
      "Epoch 352/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9670 - val_loss: 1.9450\n",
      "Epoch 353/5000\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 1.9778 - val_loss: 1.9447\n",
      "Epoch 354/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9683 - val_loss: 1.9449\n",
      "Epoch 355/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9597 - val_loss: 1.9449\n",
      "Epoch 356/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9685 - val_loss: 1.9449\n",
      "Epoch 357/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9640 - val_loss: 1.9448\n",
      "Epoch 358/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9614 - val_loss: 1.9448\n",
      "Epoch 359/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9623 - val_loss: 1.9449\n",
      "Epoch 360/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9620 - val_loss: 1.9449\n",
      "Epoch 361/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9703 - val_loss: 1.9448\n",
      "Epoch 362/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9585 - val_loss: 1.9450\n",
      "Epoch 363/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9735 - val_loss: 1.9448\n",
      "Epoch 364/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9656 - val_loss: 1.9448\n",
      "Epoch 365/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9731 - val_loss: 1.9449\n",
      "Epoch 366/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9803 - val_loss: 1.9447\n",
      "Epoch 367/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9757 - val_loss: 1.9447\n",
      "Epoch 368/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9562 - val_loss: 1.9448\n",
      "Epoch 369/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9610 - val_loss: 1.9447\n",
      "Epoch 370/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9782 - val_loss: 1.9449\n",
      "Epoch 371/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9766 - val_loss: 1.9447\n",
      "Epoch 372/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9690 - val_loss: 1.9448\n",
      "Epoch 373/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9737 - val_loss: 1.9449\n",
      "Epoch 374/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9616 - val_loss: 1.9449\n",
      "Epoch 375/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9700 - val_loss: 1.9448\n",
      "Epoch 376/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9707 - val_loss: 1.9448\n",
      "Epoch 377/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9675 - val_loss: 1.9448\n",
      "Epoch 378/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9703 - val_loss: 1.9448\n",
      "Epoch 379/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9706 - val_loss: 1.9448\n",
      "Epoch 380/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9709 - val_loss: 1.9449\n",
      "Epoch 381/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9763 - val_loss: 1.9449\n",
      "Epoch 382/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9695 - val_loss: 1.9448\n",
      "Epoch 383/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9610 - val_loss: 1.9447\n",
      "Epoch 384/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9706 - val_loss: 1.9448\n",
      "Epoch 385/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9746 - val_loss: 1.9449\n",
      "Epoch 386/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9812 - val_loss: 1.9448\n",
      "Epoch 387/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9639 - val_loss: 1.9447\n",
      "Epoch 388/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9741 - val_loss: 1.9447\n",
      "Epoch 389/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9705 - val_loss: 1.9448\n",
      "Epoch 390/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9670 - val_loss: 1.9450\n",
      "Epoch 391/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9793 - val_loss: 1.9447\n",
      "Epoch 392/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9769 - val_loss: 1.9447\n",
      "Epoch 393/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9681 - val_loss: 1.9450\n",
      "Epoch 394/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9681 - val_loss: 1.9449\n",
      "Epoch 395/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9749 - val_loss: 1.9448\n",
      "Epoch 396/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9695 - val_loss: 1.9448\n",
      "Epoch 397/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9744 - val_loss: 1.9448\n",
      "Epoch 398/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9641 - val_loss: 1.9448\n",
      "Epoch 399/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9763 - val_loss: 1.9448\n",
      "Epoch 400/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9702 - val_loss: 1.9448\n",
      "Epoch 401/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9620 - val_loss: 1.9447\n",
      "Epoch 402/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9696 - val_loss: 1.9448\n",
      "Epoch 403/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9771 - val_loss: 1.9448\n",
      "Epoch 404/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9628 - val_loss: 1.9448\n",
      "Epoch 405/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9746 - val_loss: 1.9448\n",
      "Epoch 406/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9634 - val_loss: 1.9447\n",
      "Epoch 407/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9690 - val_loss: 1.9448\n",
      "Epoch 408/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9633 - val_loss: 1.9448\n",
      "Epoch 409/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9679 - val_loss: 1.9447\n",
      "Epoch 410/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9705 - val_loss: 1.9447\n",
      "Epoch 411/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9632 - val_loss: 1.9449\n",
      "Epoch 412/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9653 - val_loss: 1.9448\n",
      "Epoch 413/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9620 - val_loss: 1.9449\n",
      "Epoch 414/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9761 - val_loss: 1.9447\n",
      "Epoch 415/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9707 - val_loss: 1.9449\n",
      "Epoch 416/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9631 - val_loss: 1.9449\n",
      "Epoch 417/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9675 - val_loss: 1.9448\n",
      "Epoch 418/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9616 - val_loss: 1.9449\n",
      "Epoch 419/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9725 - val_loss: 1.9449\n",
      "Epoch 420/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9730 - val_loss: 1.9448\n",
      "Epoch 421/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9658 - val_loss: 1.9448\n",
      "Epoch 422/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9523 - val_loss: 1.9447\n",
      "Epoch 423/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9552 - val_loss: 1.9450\n",
      "Epoch 424/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9748 - val_loss: 1.9449\n",
      "Epoch 425/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9702 - val_loss: 1.9448\n",
      "Epoch 426/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9618 - val_loss: 1.9448\n",
      "Epoch 427/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9644 - val_loss: 1.9446\n",
      "Epoch 428/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9611 - val_loss: 1.9450\n",
      "Epoch 429/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9669 - val_loss: 1.9448\n",
      "Epoch 430/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9582 - val_loss: 1.9449\n",
      "Epoch 431/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9675 - val_loss: 1.9448\n",
      "Epoch 432/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9908 - val_loss: 1.9447\n",
      "Epoch 433/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9621 - val_loss: 1.9448\n",
      "Epoch 434/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9615 - val_loss: 1.9449\n",
      "Epoch 435/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9629 - val_loss: 1.9449\n",
      "Epoch 436/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9686 - val_loss: 1.9449\n",
      "Epoch 437/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9696 - val_loss: 1.9448\n",
      "Epoch 438/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9672 - val_loss: 1.9448\n",
      "Epoch 439/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9596 - val_loss: 1.9449\n",
      "Epoch 440/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9699 - val_loss: 1.9448\n",
      "Epoch 441/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9732 - val_loss: 1.9447\n",
      "Epoch 442/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9686 - val_loss: 1.9449\n",
      "Epoch 443/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9624 - val_loss: 1.9450\n",
      "Epoch 444/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9714 - val_loss: 1.9449\n",
      "Epoch 445/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9597 - val_loss: 1.9448\n",
      "Epoch 446/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9615 - val_loss: 1.9450\n",
      "Epoch 447/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9595 - val_loss: 1.9448\n",
      "Epoch 448/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9717 - val_loss: 1.9449\n",
      "Epoch 449/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9705 - val_loss: 1.9449\n",
      "Epoch 450/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9795 - val_loss: 1.9448\n",
      "Epoch 451/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9697 - val_loss: 1.9448\n",
      "Epoch 452/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9762 - val_loss: 1.9448\n",
      "Epoch 453/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9625 - val_loss: 1.9447\n",
      "Epoch 454/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9707 - val_loss: 1.9448\n",
      "Epoch 455/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9585 - val_loss: 1.9448\n",
      "Epoch 456/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9662 - val_loss: 1.9448\n",
      "Epoch 457/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9741 - val_loss: 1.9449\n",
      "Epoch 458/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9626 - val_loss: 1.9449\n",
      "Epoch 459/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9681 - val_loss: 1.9449\n",
      "Epoch 460/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9648 - val_loss: 1.9449\n",
      "Epoch 461/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9659 - val_loss: 1.9448\n",
      "Epoch 462/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9582 - val_loss: 1.9448\n",
      "Epoch 463/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9653 - val_loss: 1.9448\n",
      "Epoch 464/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9775 - val_loss: 1.9448\n",
      "Epoch 465/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9694 - val_loss: 1.9448\n",
      "Epoch 466/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9607 - val_loss: 1.9449\n",
      "Epoch 467/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9569 - val_loss: 1.9449\n",
      "Epoch 468/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9702 - val_loss: 1.9449\n",
      "Epoch 469/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9714 - val_loss: 1.9448\n",
      "Epoch 470/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9821 - val_loss: 1.9447\n",
      "Epoch 471/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9680 - val_loss: 1.9448\n",
      "Epoch 472/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9678 - val_loss: 1.9448\n",
      "Epoch 473/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9701 - val_loss: 1.9447\n",
      "Epoch 474/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9552 - val_loss: 1.9448\n",
      "Epoch 475/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9715 - val_loss: 1.9449\n",
      "Epoch 476/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9671 - val_loss: 1.9449\n",
      "Epoch 477/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9597 - val_loss: 1.9448\n",
      "Epoch 478/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9650 - val_loss: 1.9449\n",
      "Epoch 479/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9703 - val_loss: 1.9448\n",
      "Epoch 480/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9718 - val_loss: 1.9448\n",
      "Epoch 481/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9643 - val_loss: 1.9449\n",
      "Epoch 482/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9645 - val_loss: 1.9449\n",
      "Epoch 483/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9798 - val_loss: 1.9447\n",
      "Epoch 484/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9725 - val_loss: 1.9447\n",
      "Epoch 485/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9594 - val_loss: 1.9449\n",
      "Epoch 486/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9642 - val_loss: 1.9448\n",
      "Epoch 487/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9799 - val_loss: 1.9447\n",
      "Epoch 488/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9592 - val_loss: 1.9449\n",
      "Epoch 489/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9646 - val_loss: 1.9449\n",
      "Epoch 490/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9658 - val_loss: 1.9448\n",
      "Epoch 491/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9725 - val_loss: 1.9449\n",
      "Epoch 492/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9580 - val_loss: 1.9449\n",
      "Epoch 493/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9619 - val_loss: 1.9449\n",
      "Epoch 494/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9696 - val_loss: 1.9448\n",
      "Epoch 495/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9780 - val_loss: 1.9448\n",
      "Epoch 496/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9624 - val_loss: 1.9448\n",
      "Epoch 497/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9632 - val_loss: 1.9448\n",
      "Epoch 498/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9878 - val_loss: 1.9448\n",
      "Epoch 499/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9784 - val_loss: 1.9448\n",
      "Epoch 500/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9637 - val_loss: 1.9448\n",
      "Epoch 501/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9616 - val_loss: 1.9448\n",
      "Epoch 502/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9659 - val_loss: 1.9448\n",
      "Epoch 503/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9630 - val_loss: 1.9448\n",
      "Epoch 504/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9668 - val_loss: 1.9449\n",
      "Epoch 505/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9719 - val_loss: 1.9449\n",
      "Epoch 506/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9775 - val_loss: 1.9448\n",
      "Epoch 507/5000\n",
      "97/97 [==============================] - 1s 13ms/step - loss: 1.9750 - val_loss: 1.9448\n",
      "Epoch 508/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9714 - val_loss: 1.9447\n",
      "Epoch 509/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9747 - val_loss: 1.9448\n",
      "Epoch 510/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9814 - val_loss: 1.9447\n",
      "Epoch 511/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9646 - val_loss: 1.9448\n",
      "Epoch 512/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9661 - val_loss: 1.9448\n",
      "Epoch 513/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9652 - val_loss: 1.9448\n",
      "Epoch 514/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9618 - val_loss: 1.9449\n",
      "Epoch 515/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9628 - val_loss: 1.9448\n",
      "Epoch 516/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9624 - val_loss: 1.9449\n",
      "Epoch 517/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9675 - val_loss: 1.9447\n",
      "Epoch 518/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9792 - val_loss: 1.9447\n",
      "Epoch 519/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9659 - val_loss: 1.9447\n",
      "Epoch 520/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9592 - val_loss: 1.9449\n",
      "Epoch 521/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9629 - val_loss: 1.9450\n",
      "Epoch 522/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9578 - val_loss: 1.9450\n",
      "Epoch 523/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9678 - val_loss: 1.9448\n",
      "Epoch 524/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9664 - val_loss: 1.9448\n",
      "Epoch 525/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9772 - val_loss: 1.9448\n",
      "Epoch 526/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9660 - val_loss: 1.9448\n",
      "Epoch 527/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9717 - val_loss: 1.9447\n",
      "Epoch 528/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9619 - val_loss: 1.9449\n",
      "Epoch 529/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9704 - val_loss: 1.9449\n",
      "Epoch 530/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9653 - val_loss: 1.9447\n",
      "Epoch 531/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9750 - val_loss: 1.9447\n",
      "Epoch 532/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9641 - val_loss: 1.9448\n",
      "Epoch 533/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9611 - val_loss: 1.9450\n",
      "Epoch 534/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9711 - val_loss: 1.9449\n",
      "Epoch 535/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9705 - val_loss: 1.9449\n",
      "Epoch 536/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9702 - val_loss: 1.9449\n",
      "Epoch 537/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9670 - val_loss: 1.9449\n",
      "Epoch 538/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9723 - val_loss: 1.9447\n",
      "Epoch 539/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9688 - val_loss: 1.9447\n",
      "Epoch 540/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9710 - val_loss: 1.9449\n",
      "Epoch 541/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9675 - val_loss: 1.9448\n",
      "Epoch 542/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9643 - val_loss: 1.9449\n",
      "Epoch 543/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9606 - val_loss: 1.9448\n",
      "Epoch 544/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9796 - val_loss: 1.9448\n",
      "Epoch 545/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9596 - val_loss: 1.9448\n",
      "Epoch 546/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9818 - val_loss: 1.9449\n",
      "Epoch 547/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9647 - val_loss: 1.9448\n",
      "Epoch 548/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9715 - val_loss: 1.9448\n",
      "Epoch 549/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9600 - val_loss: 1.9449\n",
      "Epoch 550/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9807 - val_loss: 1.9447\n",
      "Epoch 551/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9678 - val_loss: 1.9448\n",
      "Epoch 552/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9764 - val_loss: 1.9448\n",
      "Epoch 553/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9655 - val_loss: 1.9448\n",
      "Epoch 554/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9776 - val_loss: 1.9447\n",
      "Epoch 555/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9707 - val_loss: 1.9448\n",
      "Epoch 556/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9713 - val_loss: 1.9448\n",
      "Epoch 557/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9737 - val_loss: 1.9448\n",
      "Epoch 558/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9757 - val_loss: 1.9448\n",
      "Epoch 559/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9668 - val_loss: 1.9448\n",
      "Epoch 560/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9611 - val_loss: 1.9448\n",
      "Epoch 561/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9610 - val_loss: 1.9448\n",
      "Epoch 562/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9653 - val_loss: 1.9449\n",
      "Epoch 563/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9680 - val_loss: 1.9449\n",
      "Epoch 564/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9737 - val_loss: 1.9448\n",
      "Epoch 565/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9625 - val_loss: 1.9448\n",
      "Epoch 566/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9766 - val_loss: 1.9449\n",
      "Epoch 567/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9554 - val_loss: 1.9449\n",
      "Epoch 568/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9703 - val_loss: 1.9448\n",
      "Epoch 569/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9587 - val_loss: 1.9449\n",
      "Epoch 570/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9680 - val_loss: 1.9449\n",
      "Epoch 571/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9584 - val_loss: 1.9449\n",
      "Epoch 572/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9669 - val_loss: 1.9448\n",
      "Epoch 573/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9748 - val_loss: 1.9448\n",
      "Epoch 574/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9564 - val_loss: 1.9447\n",
      "Epoch 575/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9586 - val_loss: 1.9449\n",
      "Epoch 576/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9704 - val_loss: 1.9449\n",
      "Epoch 577/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9657 - val_loss: 1.9449\n",
      "Epoch 578/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9805 - val_loss: 1.9448\n",
      "Epoch 579/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9566 - val_loss: 1.9449\n",
      "Epoch 580/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9645 - val_loss: 1.9449\n",
      "Epoch 581/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9762 - val_loss: 1.9449\n",
      "Epoch 582/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9764 - val_loss: 1.9447\n",
      "Epoch 583/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9664 - val_loss: 1.9448\n",
      "Epoch 584/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9692 - val_loss: 1.9447\n",
      "Epoch 585/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9636 - val_loss: 1.9447\n",
      "Epoch 586/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9590 - val_loss: 1.9449\n",
      "Epoch 587/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9615 - val_loss: 1.9448\n",
      "Epoch 588/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9666 - val_loss: 1.9449\n",
      "Epoch 589/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9559 - val_loss: 1.9448\n",
      "Epoch 590/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9596 - val_loss: 1.9449\n",
      "Epoch 591/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9775 - val_loss: 1.9448\n",
      "Epoch 592/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9576 - val_loss: 1.9449\n",
      "Epoch 593/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9620 - val_loss: 1.9448\n",
      "Epoch 594/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9709 - val_loss: 1.9447\n",
      "Epoch 595/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9681 - val_loss: 1.9448\n",
      "Epoch 596/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9767 - val_loss: 1.9448\n",
      "Epoch 597/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9664 - val_loss: 1.9448\n",
      "Epoch 598/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9617 - val_loss: 1.9448\n",
      "Epoch 599/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9677 - val_loss: 1.9448\n",
      "Epoch 600/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9606 - val_loss: 1.9449\n",
      "Epoch 601/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9687 - val_loss: 1.9448\n",
      "Epoch 602/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9667 - val_loss: 1.9450\n",
      "Epoch 603/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9555 - val_loss: 1.9447\n",
      "Epoch 604/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9733 - val_loss: 1.9447\n",
      "Epoch 605/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9769 - val_loss: 1.9449\n",
      "Epoch 606/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9722 - val_loss: 1.9447\n",
      "Epoch 607/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9667 - val_loss: 1.9448\n",
      "Epoch 608/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9641 - val_loss: 1.9447\n",
      "Epoch 609/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9612 - val_loss: 1.9448\n",
      "Epoch 610/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9689 - val_loss: 1.9448\n",
      "Epoch 611/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9735 - val_loss: 1.9448\n",
      "Epoch 612/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9711 - val_loss: 1.9448\n",
      "Epoch 613/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9729 - val_loss: 1.9448\n",
      "Epoch 614/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9703 - val_loss: 1.9448\n",
      "Epoch 615/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9686 - val_loss: 1.9448\n",
      "Epoch 616/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9786 - val_loss: 1.9448\n",
      "Epoch 617/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9775 - val_loss: 1.9447\n",
      "Epoch 618/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9568 - val_loss: 1.9448\n",
      "Epoch 619/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9539 - val_loss: 1.9450\n",
      "Epoch 620/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9772 - val_loss: 1.9450\n",
      "Epoch 621/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9675 - val_loss: 1.9450\n",
      "Epoch 622/5000\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 1.9660 - val_loss: 1.9448\n",
      "Epoch 623/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9742 - val_loss: 1.9448\n",
      "Epoch 624/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9699 - val_loss: 1.9448\n",
      "Epoch 625/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9700 - val_loss: 1.9449\n",
      "Epoch 626/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9593 - val_loss: 1.9449\n",
      "Epoch 627/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9720 - val_loss: 1.9450\n",
      "Epoch 628/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9677 - val_loss: 1.9449\n",
      "Epoch 629/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9782 - val_loss: 1.9449\n",
      "Epoch 630/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9629 - val_loss: 1.9448\n",
      "Epoch 631/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9760 - val_loss: 1.9448\n",
      "Epoch 632/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9582 - val_loss: 1.9449\n",
      "Epoch 633/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9650 - val_loss: 1.9449\n",
      "Epoch 634/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9795 - val_loss: 1.9449\n",
      "Epoch 635/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9611 - val_loss: 1.9448\n",
      "Epoch 636/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9632 - val_loss: 1.9448\n",
      "Epoch 637/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9580 - val_loss: 1.9448\n",
      "Epoch 638/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9659 - val_loss: 1.9450\n",
      "Epoch 639/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9673 - val_loss: 1.9448\n",
      "Epoch 640/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9751 - val_loss: 1.9449\n",
      "Epoch 641/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9609 - val_loss: 1.9449\n",
      "Epoch 642/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9746 - val_loss: 1.9449\n",
      "Epoch 643/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9578 - val_loss: 1.9449\n",
      "Epoch 644/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9628 - val_loss: 1.9449\n",
      "Epoch 645/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9757 - val_loss: 1.9448\n",
      "Epoch 646/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9620 - val_loss: 1.9449\n",
      "Epoch 647/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9618 - val_loss: 1.9448\n",
      "Epoch 648/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9802 - val_loss: 1.9447\n",
      "Epoch 649/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9614 - val_loss: 1.9448\n",
      "Epoch 650/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9712 - val_loss: 1.9448\n",
      "Epoch 651/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9704 - val_loss: 1.9448\n",
      "Epoch 652/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9692 - val_loss: 1.9448\n",
      "Epoch 653/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9545 - val_loss: 1.9448\n",
      "Epoch 654/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9560 - val_loss: 1.9449\n",
      "Epoch 655/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9682 - val_loss: 1.9447\n",
      "Epoch 656/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9678 - val_loss: 1.9448\n",
      "Epoch 657/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9597 - val_loss: 1.9450\n",
      "Epoch 658/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9848 - val_loss: 1.9449\n",
      "Epoch 659/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9615 - val_loss: 1.9449\n",
      "Epoch 660/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9666 - val_loss: 1.9448\n",
      "Epoch 661/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9689 - val_loss: 1.9447\n",
      "Epoch 662/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9642 - val_loss: 1.9448\n",
      "Epoch 663/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9710 - val_loss: 1.9448\n",
      "Epoch 664/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9584 - val_loss: 1.9448\n",
      "Epoch 665/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9765 - val_loss: 1.9448\n",
      "Epoch 666/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9703 - val_loss: 1.9448\n",
      "Epoch 667/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9685 - val_loss: 1.9449\n",
      "Epoch 668/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9760 - val_loss: 1.9448\n",
      "Epoch 669/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9594 - val_loss: 1.9448\n",
      "Epoch 670/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9740 - val_loss: 1.9448\n",
      "Epoch 671/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9682 - val_loss: 1.9449\n",
      "Epoch 672/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9762 - val_loss: 1.9448\n",
      "Epoch 673/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9814 - val_loss: 1.9448\n",
      "Epoch 674/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9570 - val_loss: 1.9449\n",
      "Epoch 675/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9678 - val_loss: 1.9449\n",
      "Epoch 676/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9618 - val_loss: 1.9448\n",
      "Epoch 677/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9627 - val_loss: 1.9449\n",
      "Epoch 678/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9642 - val_loss: 1.9448\n",
      "Epoch 679/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9678 - val_loss: 1.9448\n",
      "Epoch 680/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9760 - val_loss: 1.9448\n",
      "Epoch 681/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9550 - val_loss: 1.9449\n",
      "Epoch 682/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9736 - val_loss: 1.9448\n",
      "Epoch 683/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9628 - val_loss: 1.9450\n",
      "Epoch 684/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9663 - val_loss: 1.9448\n",
      "Epoch 685/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9735 - val_loss: 1.9448\n",
      "Epoch 686/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9680 - val_loss: 1.9448\n",
      "Epoch 687/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9767 - val_loss: 1.9448\n",
      "Epoch 688/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9578 - val_loss: 1.9448\n",
      "Epoch 689/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9657 - val_loss: 1.9448\n",
      "Epoch 690/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9680 - val_loss: 1.9449\n",
      "Epoch 691/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9730 - val_loss: 1.9450\n",
      "Epoch 692/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9702 - val_loss: 1.9449\n",
      "Epoch 693/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9647 - val_loss: 1.9448\n",
      "Epoch 694/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9671 - val_loss: 1.9448\n",
      "Epoch 695/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9717 - val_loss: 1.9448\n",
      "Epoch 696/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9635 - val_loss: 1.9449\n",
      "Epoch 697/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9717 - val_loss: 1.9449\n",
      "Epoch 698/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9547 - val_loss: 1.9448\n",
      "Epoch 699/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9667 - val_loss: 1.9449\n",
      "Epoch 700/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9671 - val_loss: 1.9447\n",
      "Epoch 701/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9676 - val_loss: 1.9449\n",
      "Epoch 702/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9846 - val_loss: 1.9449\n",
      "Epoch 703/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9785 - val_loss: 1.9448\n",
      "Epoch 704/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9665 - val_loss: 1.9447\n",
      "Epoch 705/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9761 - val_loss: 1.9449\n",
      "Epoch 706/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9630 - val_loss: 1.9448\n",
      "Epoch 707/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9680 - val_loss: 1.9449\n",
      "Epoch 708/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9758 - val_loss: 1.9449\n",
      "Epoch 709/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9744 - val_loss: 1.9447\n",
      "Epoch 710/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9740 - val_loss: 1.9448\n",
      "Epoch 711/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9804 - val_loss: 1.9450\n",
      "Epoch 712/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9539 - val_loss: 1.9449\n",
      "Epoch 713/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9697 - val_loss: 1.9447\n",
      "Epoch 714/5000\n",
      "97/97 [==============================] - 1s 9ms/step - loss: 1.9617 - val_loss: 1.9452\n",
      "Epoch 715/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9711 - val_loss: 1.9449\n",
      "Epoch 716/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9644 - val_loss: 1.9448\n",
      "Epoch 717/5000\n",
      "97/97 [==============================] - 1s 10ms/step - loss: 1.9558 - val_loss: 1.9449\n",
      "Epoch 718/5000\n",
      "97/97 [==============================] - 1s 11ms/step - loss: 1.9713 - val_loss: 1.9448\n",
      "Epoch 719/5000\n",
      "26/97 [=======>......................] - ETA: 0s - loss: 1.9324"
     ]
    }
   ],
   "source": [
    "hist = sequence_autoencoder.fit(X_train, y_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[mc, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.plot(hist.history['loss'], label='training')\n",
    "plt.plot(hist.history['val_loss'], label='testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'figures/{name}', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_autoencoder = load_model(f'Models/{name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sequence_autoencoder.predict(X_train).argmax(axis=-1)\n",
    "accuracy_score(y_train.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sequence_autoencoder.predict(X_test).argmax(axis=-1)\n",
    "accuracy_score(y_test.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "train_encoded = encoder(X_train).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-b279aa5c1370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianMixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/kuacc/apps/anaconda/3.6/envs/root_env/lib/python3.7/site-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kuacc/apps/anaconda/3.6/envs/root_env/lib/python3.7/site-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdo_init\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mlower_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfty\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdo_init\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_bound_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kuacc/apps/anaconda/3.6/envs/root_env/lib/python3.7/site-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36m_initialize_parameters\u001b[0;34m(self, X, random_state)\u001b[0m\n\u001b[1;32m    154\u001b[0m                              % self.init_params)\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kuacc/apps/anaconda/3.6/envs/root_env/lib/python3.7/site-packages/sklearn/mixture/_gaussian_mixture.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, X, resp)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariances_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovariances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             self.precisions_cholesky_ = _compute_precision_cholesky(\n\u001b[0;32m--> 649\u001b[0;31m                 covariances, self.covariance_type)\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             self.precisions_cholesky_ = np.array(\n",
      "\u001b[0;32m/kuacc/apps/anaconda/3.6/envs/root_env/lib/python3.7/site-packages/sklearn/mixture/_gaussian_mixture.py\u001b[0m in \u001b[0;36m_compute_precision_cholesky\u001b[0;34m(covariances, covariance_type)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovariances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0mcov_chol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimate_precision_error_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kuacc/apps/anaconda/3.6/envs/root_env/lib/python3.7/site-packages/scipy/linalg/decomp_cholesky.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \"\"\"\n\u001b[1;32m     90\u001b[0m     c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n\u001b[0;32m---> 91\u001b[0;31m                          check_finite=check_finite)\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kuacc/apps/anaconda/3.6/envs/root_env/lib/python3.7/site-packages/scipy/linalg/decomp_cholesky.py\u001b[0m in \u001b[0;36m_cholesky\u001b[0;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"\"\"Common code for cholesky() and cho_factor().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray_chkfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \"\"\"\n\u001b[1;32m    483\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllFloat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         raise ValueError(\n\u001b[1;32m    486\u001b[0m             \"array must not contain infs or NaNs\")\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gm1 = GaussianMixture(n_components=5)\n",
    "gm1.fit(train_encoded.reshape(num_train, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples1 = gm1.sample(10)\n",
    "\n",
    "samples1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = samples1[0].reshape(10, 16, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = decoder(trials).numpy().argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = replace_with_dict(preds, v2n_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preds).to_csv(\"FirstOut.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"FirstOut.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Embedding-LSTM Encoder Embedding-LSTM-Dense Decoder Bassline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(minor_data, minor_data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = X_train.shape[0]\n",
    "\n",
    "num_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timesteps = 64\n",
    "\n",
    "n2v_mapping['<bon/>'] = 38\n",
    "v2n_mapping = {value:key for key, value in n2v_mapping.items()}\n",
    "vocab_size = len(n2v_mapping)\n",
    "\n",
    "encoder_inputs_training = X_train\n",
    "decoder_inputs_training = np.ones((num_train, timesteps))\n",
    "decoder_inputs_training[:,0] *= n2v_mapping['<bon/>']\n",
    "decoder_inputs_training[:,1:] = X_train[:,:-1]\n",
    "#decoder_outputs_training = np.ones((num_train, timesteps))\n",
    "#decoder_outputs_training[:,-1] *= n2v_mapping['<bon/>']\n",
    "#decoder_outputs_training[:,:-1] = X_train\n",
    "#decoder_outputs_training = one_hot_encode(decoder_outputs_training, vocab_size)\n",
    "decoder_outputs_training = one_hot_encode(X_train, vocab_size)\n",
    "\n",
    "encoder_inputs_testing = X_test\n",
    "decoder_inputs_testing = np.ones((num_test, timesteps))\n",
    "decoder_inputs_testing[:,0] *= n2v_mapping['<bon/>']\n",
    "decoder_inputs_testing[:,1:] = X_test[:,:-1]\n",
    "#decoder_outputs_testing = np.ones((num_test, timesteps))\n",
    "#decoder_outputs_testing[:,-1] *= n2v_mapping['<bon/>']\n",
    "#decoder_outputs_testing[:,:-1] = X_test[:,1:]\n",
    "#decoder_outputs_testing = one_hot_encode(decoder_outputs_testing, vocab_size)\n",
    "decoder_outputs_testing = one_hot_encode(X_test, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='Keras_runs', entity='nbg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NBG_lm_lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 32\n",
    "latent_dim = 256\n",
    "dropout = 0.\n",
    "\n",
    "encoder_inputs = Input(shape=(timesteps,))\n",
    "embedding = Embedding(vocab_size, embed_size)\n",
    "encoder_embedding_outputs = embedding(encoder_inputs)\n",
    "encoder = LSTM(latent_dim, return_state=True, dropout=dropout)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embedding_outputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(timesteps,))\n",
    "decoder_embedding_outputs = embedding(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=dropout)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding_outputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 5e-3\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "wandb.config.learning_rate = learning_rate\n",
    "wandb.config.epochs = epochs\n",
    "wandb.config.batch_size = batch_size\n",
    "wandb.config.model = name\n",
    "\n",
    "mc = ModelCheckpoint(f'Models/{name}.hdf5', monitor='val_loss')\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(optimizer, loss='categorical_crossentropy')\n",
    "hist = model.fit([encoder_inputs_training, decoder_inputs_training], decoder_outputs_training,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 validation_data=([encoder_inputs_testing, decoder_inputs_testing], decoder_outputs_testing),\n",
    "                 callbacks=[mc, WandbCallback()]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.plot(hist.history['loss'], label='training')\n",
    "plt.plot(hist.history['val_loss'], label='testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'figures/{name}', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding_outputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((num_train, timesteps))\n",
    "train_encoded = encoder_model.predict(encoder_inputs_training[:,:,np.newaxis])\n",
    "train_inputs = np.ones((num_train, timesteps + 1)) * n2v_mapping['<bon/>']\n",
    "for i in range(timesteps):\n",
    "    decoded_results = decoder_model.predict([train_inputs[:,i+1].reshape(-1, 1)] + train_encoded)\n",
    "    train_encoded = decoded_results[1:]\n",
    "    #preds = decoded_results[0].argmax(-1)\n",
    "    train_inputs[:,i+1] = decoded_results[0].argmax(-1).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(encoder_inputs_training.reshape(-1), train_inputs[:,1:].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((num_test, timesteps))\n",
    "test_encoded = encoder_model.predict(encoder_inputs_testing[:,:,np.newaxis])\n",
    "test_inputs = np.ones((num_test, timesteps + 1)) * n2v_mapping['<bon/>']\n",
    "for i in range(timesteps):\n",
    "    decoded_results = decoder_model.predict([test_inputs[:,i+1].reshape(-1, 1)] + test_encoded)\n",
    "    test_encoded = decoded_results[1:]\n",
    "    #preds = decoded_results[0].argmax(-1)\n",
    "    test_inputs[:,i+1] = decoded_results[0].argmax(-1).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(encoder_inputs_testing.reshape(-1), test_inputs[:,1:].reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6: Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: raraz15 (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">astral-mountain-9</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/nbg/Keras_runs\" target=\"_blank\">https://wandb.ai/nbg/Keras_runs</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/nbg/Keras_runs/runs/3k0mjjx1\" target=\"_blank\">https://wandb.ai/nbg/Keras_runs/runs/3k0mjjx1</a><br/>\n",
       "                Run data is saved locally in <code>/scratch/users/udemir15/Bassline-Generator/generator/wandb/run-20210526_012150-3k0mjjx1</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(3k0mjjx1)</h1><iframe src=\"https://wandb.ai/nbg/Keras_runs/runs/3k0mjjx1\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2b6d6daa28d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='Keras_runs', entity='nbg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NBG_vae_dense'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 64\n",
    "intermediate_dim = 64\n",
    "latent_dim = 2\n",
    "embed_size = 16\n",
    "\n",
    "ohe_inputs = Input(shape=(original_dim, ))\n",
    "embeddings = Embedding(vocab_size, embed_size)(ohe_inputs)\n",
    "h = Dense(intermediate_dim, activation='relu')(embeddings)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_sigma = Dense(latent_dim)(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lambda_3/strided_slice:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(original_dim, latent_dim),\n",
    "                              mean=0., stddev=0.1)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "z = Lambda(sampling)([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "encoder = Model(ohe_inputs, [z_mean, z_log_sigma, z], name='encoder')\n",
    "\n",
    "# Create decoder\n",
    "latent_inputs = Input(shape=(original_dim, latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(vocab_size, activation='softmax')(x)\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(ohe_inputs)[2])\n",
    "vae = Model(ohe_inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = keras.losses.sparse_categorical_crossentropy(ohe_inputs, outputs)\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_mlp\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            [(None, 64, 2), (Non 1956        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, 64, 38)       2662        encoder[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 64, 16)       608         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 64, 64)       1088        embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 64, 2)        130         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64, 2)        130         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (None, 64, 2)        0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.square_2 (TFOpLambda)   (None, 64, 2)        0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_4 (TFOpLambda) (None, 64, 2)        0           tf.__operators__.add_4[0][0]     \n",
      "                                                                 tf.math.square_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.exp_2 (TFOpLambda)      (None, 64, 2)        0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast_3 (TFOpLambda)          (None, 64)           0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.convert_to_tensor_9 (TFOpLam (None, 64, 38)       0           decoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_5 (TFOpLambda) (None, 64, 2)        0           tf.math.subtract_4[0][0]         \n",
      "                                                                 tf.math.exp_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.keras.backend.sparse_categor (None, 64)           0           tf.cast_3[0][0]                  \n",
      "                                                                 tf.convert_to_tensor_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_2 (TFOpLambd (None, 64)           0           tf.math.subtract_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_4 (TFOpLambda) (None, 64)           0           tf.keras.backend.sparse_categoric\n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_5 (TFOpLambda) (None, 64)           0           tf.math.reduce_sum_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (None, 64)           0           tf.math.multiply_4[0][0]         \n",
      "                                                                 tf.math.multiply_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean_2 (TFOpLamb ()                   0           tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_loss_2 (AddLoss)            ()                   0           tf.math.reduce_mean_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 4,618\n",
      "Trainable params: 4,618\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      [(None, 64, 2)]           0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64, 64)            192       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64, 38)            2470      \n",
      "=================================================================\n",
      "Total params: 2,662\n",
      "Trainable params: 2,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "97/97 [==============================] - 1s 12ms/step - loss: 81.4104 - val_loss: 58.3397\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 2/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 43.3466 - val_loss: 32.6552\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 3/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 29.0697 - val_loss: 25.5377\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 4/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 23.4405 - val_loss: 20.6682\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 5/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 19.1679 - val_loss: 17.2321\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 6/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 16.1745 - val_loss: 14.6365\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 7/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 13.8151 - val_loss: 12.5273\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 8/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 11.9335 - val_loss: 10.8657\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 9/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 10.3996 - val_loss: 9.5404\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 10/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 9.1649 - val_loss: 8.4510\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 11/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 8.1872 - val_loss: 7.5867\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 12/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 7.3398 - val_loss: 6.8271\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 13/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 6.6638 - val_loss: 6.2025\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 14/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 6.0420 - val_loss: 5.6704\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 15/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 5.5276 - val_loss: 5.2040\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 16/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 5.1011 - val_loss: 4.8127\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 17/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 4.7062 - val_loss: 4.4555\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 18/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 4.3628 - val_loss: 4.1387\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 19/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 4.0714 - val_loss: 3.8980\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 20/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 3.8066 - val_loss: 3.6390\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 21/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 3.5769 - val_loss: 3.4189\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 22/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 3.3559 - val_loss: 3.2175\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 3.1695 - val_loss: 3.0410\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 24/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.9944 - val_loss: 2.8869\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 25/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 2.8297 - val_loss: 2.7220\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 26/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.6850 - val_loss: 2.5879\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 27/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.5539 - val_loss: 2.4428\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 28/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.4291 - val_loss: 2.3340\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 29/100\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 2.3145 - val_loss: 2.2264\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 30/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.2156 - val_loss: 2.1344\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 31/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 2.1226 - val_loss: 2.0506\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 32/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 2.0327 - val_loss: 1.9648\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 33/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 1.9493 - val_loss: 1.8858\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 34/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.8767 - val_loss: 1.8153\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 35/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.8092 - val_loss: 1.7363\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 36/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.7399 - val_loss: 1.6879\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 37/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.6730 - val_loss: 1.6247\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 38/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.6098 - val_loss: 1.5621\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 39/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.5582 - val_loss: 1.5057\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 40/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.5069 - val_loss: 1.4633\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 41/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.4565 - val_loss: 1.4054\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 42/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.4051 - val_loss: 1.3624\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 43/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.3739 - val_loss: 1.3200\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 44/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.3342 - val_loss: 1.2890\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.2855 - val_loss: 1.2479\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 46/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 1.2439 - val_loss: 1.2104\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 47/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 1.2108 - val_loss: 1.1796\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 48/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 1.1816 - val_loss: 1.1478\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 49/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.1445 - val_loss: 1.1103\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 50/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.1187 - val_loss: 1.0853\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 51/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.1066 - val_loss: 1.0866\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 52/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.0750 - val_loss: 1.0314\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 53/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 1.0326 - val_loss: 1.0259\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 54/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 1.0216 - val_loss: 0.9766\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 55/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.9820 - val_loss: 0.9669\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 56/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 0.9527 - val_loss: 0.9417\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 57/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.9365 - val_loss: 0.9587\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 58/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.9137 - val_loss: 0.9069\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 59/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.8982 - val_loss: 0.8837\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 60/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.8786 - val_loss: 0.8531\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 61/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.8502 - val_loss: 0.8491\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 62/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.8425 - val_loss: 0.8726\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 63/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.8437 - val_loss: 0.8410\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 64/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.8184 - val_loss: 0.7893\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 65/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.8200 - val_loss: 0.7770\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 66/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 0.7653 - val_loss: 0.7509\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.7540 - val_loss: 0.7336\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 68/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.8157 - val_loss: 0.7335\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 69/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.7410 - val_loss: 0.7498\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 70/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.7169 - val_loss: 0.6990\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 71/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.6971 - val_loss: 0.6955\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 72/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.6832 - val_loss: 0.6686\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 73/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.7239 - val_loss: 0.6967\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 74/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.6715 - val_loss: 0.6720\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 75/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.6412 - val_loss: 0.6584\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 76/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.6940 - val_loss: 0.6341\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 77/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.6312 - val_loss: 0.6036\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 78/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.6224 - val_loss: 0.6268\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 79/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.6159 - val_loss: 0.6617\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 80/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.6042 - val_loss: 0.5851\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 81/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 0.5901 - val_loss: 0.5844\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 82/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.6111 - val_loss: 0.5865\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 83/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.5671 - val_loss: 0.5373\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 84/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.6030 - val_loss: 0.5725\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 85/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.5510 - val_loss: 0.5421\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 86/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.6144 - val_loss: 0.5629\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 87/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.5530 - val_loss: 0.5284\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 88/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.5594 - val_loss: 0.6232\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.5400 - val_loss: 0.5106\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 90/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.5367 - val_loss: 0.5348\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 91/100\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 0.5909 - val_loss: 0.5184\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 92/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 0.5355 - val_loss: 0.5211\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 93/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 0.4972 - val_loss: 0.5006\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 94/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.5360 - val_loss: 0.4992\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 95/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.5079 - val_loss: 0.4902\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 96/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.4790 - val_loss: 0.5127\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 97/100\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 0.4755 - val_loss: 0.4736\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 98/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.4932 - val_loss: 0.4911\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 99/100\n",
      "97/97 [==============================] - 1s 5ms/step - loss: 0.4966 - val_loss: 0.4810\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 100/100\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.4792 - val_loss: 0.4573\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-3\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "#wandb.config.learning_rate = learning_rate\n",
    "#wandb.config.epochs = epochs\n",
    "#wandb.config.batch_size = batch_size\n",
    "#wandb.config.model = name\n",
    "\n",
    "mc = ModelCheckpoint(f'Models/{name}.hdf5', monitor='val_loss', save_format=\"tf\")\n",
    "\n",
    "hist = vae.fit(X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test),\n",
    "              callbacks=[mc]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.plot(hist.history['loss'], label='training')\n",
    "plt.plot(hist.history['val_loss'], label='testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'figures/{name}', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "vae = load_model(f'Models/{name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999696307094267"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = vae(X_train).numpy().argmax(-1)\n",
    "\n",
    "accuracy_score(X_train.reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999595207253886"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = vae(X_test).numpy().argmax(-1)\n",
    "\n",
    "accuracy_score(X_test.reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=30\n",
    "\n",
    "grid_x = np.linspace(-15, 15, n)\n",
    "grid_y = np.linspace(-15, 15, n)\n",
    "lst=[]\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]] * 64)[np.newaxis,:,:]\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        lst.append(x_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = decoder.predict(np.random.normal(size=(10, 64, 2)))\n",
    "preds.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation = decoder(np.random.normal(size=(10, 64, 2))).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = encoder(y_train)[2].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[1,2]] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.predict(train_encoded).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = encoder.predict(y_test, batch_size=32)[2]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
