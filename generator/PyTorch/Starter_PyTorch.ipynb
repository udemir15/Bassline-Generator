{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: raraz15 (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import models\n",
    "import encoders\n",
    "import decoders\n",
    "from training import train, test, checkpoint\n",
    "from dataloaders import load_data, create_loaders\n",
    "\n",
    "WANDB_API_KEY= '52c84ab3f3b5c1f999c7f5f389f5e423f46fc04a'\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 38\n",
      "Sequence Length: 64\n",
      "Number of data points: 3859\n"
     ]
    }
   ],
   "source": [
    "#dataset_path = '/projects/bassline_transcription/data/datasets'\n",
    "dataset_path = '/scratch/users/udemir15/ELEC491/bassline_transcription/data/datasets'\n",
    "dataset_name = 'traxsource_0-5000_bassline_representations'\n",
    "M = 8 \n",
    "\n",
    "\n",
    "data_params = {'dataset_path': dataset_path,\n",
    "               'dataset_name': dataset_name,\n",
    "               'scale_type': 'min',\n",
    "               'M': M,\n",
    "               'batch_size': 128}\n",
    "\n",
    "X = load_data(data_params)\n",
    "\n",
    "frequencies = np.unique(X, return_counts=True)\n",
    "\n",
    "#X = X.reshape(-1, 64)\n",
    "\n",
    "K = X.max()+1 # Number of classes, assumes consecutive\n",
    "sequence_length = X.shape[1]\n",
    "\n",
    "print('Number of classes: {}\\nSequence Length: {}'.format(K, sequence_length))\n",
    "print('Number of data points: {}'.format(X.shape[0]))\n",
    "\n",
    "train_loader, test_loader = create_loaders(X, data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = np.unique(X, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_weights = torch.FloatTensor(1 / (frequencies[1] / frequencies[1].max())).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9208e+00, 1.9974e+03, 1.2291e+03, 2.8461e+02, 2.4316e+02, 8.1703e+01,\n",
       "        1.1390e+02, 3.9748e+01, 3.3660e+01, 5.1144e+01, 2.3824e+01, 2.2456e+01,\n",
       "        6.3654e+00, 2.5358e+01, 2.7591e+01, 2.0903e+01, 5.5018e+01, 3.5307e+01,\n",
       "        6.4580e+01, 3.2271e+01, 5.1545e+01, 1.0318e+02, 7.1062e+01, 8.7248e+01,\n",
       "        2.5862e+01, 1.3775e+02, 1.8247e+02, 1.4141e+02, 5.7656e+02, 4.1274e+02,\n",
       "        7.0792e+02, 5.2513e+02, 1.5115e+03, 3.8570e+03, 1.4526e+03, 3.4954e+03,\n",
       "        2.0337e+03, 1.0000e+00], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig ,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.bar(*frequencies)\n",
    "\n",
    "plt.savefig('data_distrib.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_params = {'num_embeddings': K,\n",
    "                    'embedding_dim': 32}\n",
    "\n",
    "encoder_params = {'input_size': embedding_params['embedding_dim'],\n",
    "                  'hidden_size': embedding_params['embedding_dim'],\n",
    "                  'dropout': 0.5,  \n",
    "                  'num_layers': 16,              \n",
    "                  'batch_size': data_params['batch_size'],\n",
    "                  'device':device}\n",
    "\n",
    "#StackedUnidirLSTMDenseDecoder / StackedUnidirectionalLSTMDecoder\n",
    "decoder_params = {'input_size': encoder_params['hidden_size'],\n",
    "                  'output_size': K,\n",
    "                 'num_layers': encoder_params['num_layers'], \n",
    "                 'dropout': 0.5,\n",
    "                 'batch_size': data_params['batch_size'],\n",
    "                 'sequence_length': sequence_length,\n",
    "                 'device':device,\n",
    "                 'teacher_forcing_ratio': 0.5}\n",
    "\n",
    "#StackedUnidirectionalLSTMDecoderwithEmbedding \n",
    "\"\"\"decoder_params = {'num_embeddings': K,\n",
    "                  'embedding_dim': 16,\n",
    "                  'hidden_size': 128,                  \n",
    "                  'num_layers': encoder_params['num_layers'], \n",
    "                  'dropout': 0.2,\n",
    "                  'batch_size': data_params['batch_size'],\n",
    "                  'sequence_length': sequence_length,\n",
    "                  'device':device}\"\"\"\n",
    "\n",
    "train_params = {'batch_size': data_params['batch_size'],\n",
    "                'N_epochs': 500}\n",
    "\n",
    "all_params = {'embedding_params':embedding_params,'encoder_params': encoder_params,\n",
    "              'decoder_params':decoder_params, 'train_params':train_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder = encoders.StackedBidirLSTMEncoder(**encoder_params).to(device)\n",
    "\n",
    "decoder = decoders.StackedUnidirLSTMDecoderwithEmbedding(**decoder_params).to(device)\n",
    "\n",
    "model = models.AutoEncoder(encoder, decoder, device).to(device)\n",
    "print(model)\n",
    "print('Number of parameters: {}'.format(sum([parameter.numel() for parameter in model.parameters()])))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanillaAutoEncoder(\n",
      "  (embedding): Embedding(38, 32)\n",
      "  (encoder): StackedBidirLSTMEncoder(\n",
      "    (net): LSTMnetwork(\n",
      "      (lstm): LSTM(32, 32, num_layers=16, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): StackedUnidirLSTMDenseDecoder(\n",
      "    (rnn): StackedUnidirLSTMDecoder(\n",
      "      (rnn): LSTMnetwork(\n",
      "        (lstm): LSTM(32, 32, num_layers=16, batch_first=True, dropout=0.5)\n",
      "      )\n",
      "    )\n",
      "    (dense): Linear(in_features=32, out_features=38, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 530854\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(**embedding_params)\n",
    "\n",
    "encoder = encoders.StackedBidirLSTMEncoder(**encoder_params).to(device)\n",
    "\n",
    "#decoder = decoders.StackedUnidirLSTMDecoder(**decoder_params).to(device)\n",
    "decoder = decoders.StackedUnidirLSTMDenseDecoder(**decoder_params).to(device)\n",
    "\n",
    "model = models.VanillaAutoEncoder(embedding, encoder, decoder).to(device)\n",
    "print(model)\n",
    "print('Number of parameters: {}'.format(sum([parameter.numel() for parameter in model.parameters()])))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean', weight=cross_entropy_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">26_05__12_04</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/nbg/PyTorch_experiments\" target=\"_blank\">https://wandb.ai/nbg/PyTorch_experiments</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/nbg/PyTorch_experiments/runs/31ph5bhp\" target=\"_blank\">https://wandb.ai/nbg/PyTorch_experiments/runs/31ph5bhp</a><br/>\n",
       "                Run data is saved locally in <code>/scratch/users/udemir15/ELEC491/Bassline-Generator/generator/PyTorch/wandb/run-20210526_120414-31ph5bhp</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss Before Training: 3.642147\n",
      "Test Accuracy Before Training: 0.015\n",
      "Initial Sample:\n",
      "tensor([12, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14], device='cuda:0')\n",
      "Epoch: 0, train_loss: 3.642235, train_acc: 0.021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/500 [00:11<1:33:56, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\n",
      "tensor([14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 26/500 [03:53<1:09:30,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, train_loss: 3.623479, train_acc: 0.017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 51/500 [07:39<1:09:34,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, train_loss: 3.617061, train_acc: 0.017\n",
      "Sample:\n",
      "tensor([23, 23, 23, 23, 23, 23, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28, 28, 28, 28, 28], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 76/500 [11:18<1:00:58,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75, train_loss: 3.614012, train_acc: 0.017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 101/500 [15:00<58:39,  8.82s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, train_loss: 3.604882, train_acc: 0.017\n",
      "Sample:\n",
      "tensor([28, 28, 28, 28, 28, 36, 36, 26, 26, 36,  3,  3,  3,  3,  3,  3, 34,  1,\n",
      "        32, 28, 35, 33, 33, 26, 26,  3,  3,  3,  3,  3,  3, 34, 34, 28, 28, 28,\n",
      "        36, 36, 36, 26, 36,  3,  3,  3,  3,  3,  3, 34, 28, 28, 28, 28, 36, 36,\n",
      "        36, 36,  3,  3,  3,  3,  3,  3, 34, 28], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 126/500 [18:39<55:17,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125, train_loss: 3.599464, train_acc: 0.020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 151/500 [22:22<51:56,  8.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, train_loss: 3.581792, train_acc: 0.026\n",
      "Sample:\n",
      "tensor([ 0, 32, 32, 34, 34,  3,  4,  4, 32, 32, 28, 35,  2, 26, 26, 26, 26, 28,\n",
      "        18,  3,  3,  3, 28,  4, 32, 32, 28, 35,  2, 26, 26, 26, 26, 28, 18,  3,\n",
      "         3,  3, 28,  4, 32, 32, 28, 35,  2, 26, 26, 26, 26, 28, 18,  3,  3,  3,\n",
      "         4, 34, 32, 28, 28, 35,  2, 26, 26, 26], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 176/500 [26:03<48:58,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 175, train_loss: 3.568173, train_acc: 0.035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 201/500 [29:48<44:35,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200, train_loss: 3.631113, train_acc: 0.030\n",
      "Sample:\n",
      "tensor([ 3,  3, 32, 36, 33,  4, 36, 33, 32,  2,  2, 34,  3, 36, 36, 13,  4, 32,\n",
      "        32, 36, 33, 34,  0,  0,  4,  3, 24, 32,  2,  2,  3,  3,  3, 36, 36, 13,\n",
      "        34, 32, 28, 33, 35, 34,  0, 13,  4,  3, 27, 32,  2,  2,  3,  3,  3, 36,\n",
      "        36, 13, 34, 32, 28, 13, 35, 34,  0, 13], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 226/500 [33:24<41:22,  9.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 225, train_loss: 3.602243, train_acc: 0.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 251/500 [37:02<36:41,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 250, train_loss: 3.591975, train_acc: 0.027\n",
      "Sample:\n",
      "tensor([14, 14, 34, 34, 34, 30, 26, 26,  3,  3,  3, 28, 28, 28, 28, 28, 28, 27,\n",
      "        26, 26, 21,  3,  3,  3, 28, 28, 28, 28, 28, 28, 28, 27, 26, 26, 26,  3,\n",
      "         3,  3, 28, 28, 28, 28, 28, 28, 27, 26, 26,  3,  3,  3,  3, 28, 28, 28,\n",
      "        28, 28, 27, 26, 26,  3,  3,  3,  3, 28], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 276/500 [40:43<32:53,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 275, train_loss: 3.583166, train_acc: 0.032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 301/500 [44:23<29:26,  8.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300, train_loss: 3.566500, train_acc: 0.032\n",
      "Sample:\n",
      "tensor([ 1, 34, 28, 28, 34, 34, 28, 34, 32,  0,  0,  2, 26, 26, 21,  0,  3, 32,\n",
      "        28, 13, 34,  3, 28, 28, 34, 32, 28,  2,  2, 26, 26, 14,  3,  3, 28, 18,\n",
      "        34,  3, 28, 28, 34, 28, 28,  2, 26, 26, 21,  0,  3, 28, 28, 34, 34, 28,\n",
      "        34, 28, 28,  2,  2, 26, 21,  0,  3,  3], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 326/500 [48:03<25:50,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 325, train_loss: 3.609621, train_acc: 0.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 351/500 [51:44<22:11,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 350, train_loss: 3.576027, train_acc: 0.022\n",
      "Sample:\n",
      "tensor([33,  1, 34, 34,  3,  3, 28, 28, 34, 28, 28, 34, 34, 34,  3,  3, 28, 28,\n",
      "         5, 34, 28, 28, 34, 34,  3,  3,  3, 28, 36, 34, 28, 28, 28, 34, 34,  3,\n",
      "         3,  3, 28, 14, 34, 28, 28, 28, 34, 34,  3,  3, 28, 28,  9, 34, 28, 28,\n",
      "        28, 34, 34,  3,  3, 28, 28,  9, 34, 28], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 376/500 [55:23<18:21,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 375, train_loss: 3.632541, train_acc: 0.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 401/500 [59:04<14:34,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 400, train_loss: 3.556675, train_acc: 0.025\n",
      "Sample:\n",
      "tensor([33,  0,  3, 32, 32,  0, 34,  3, 28, 28, 34, 28, 28,  2,  2, 26, 25,  3,\n",
      "         3, 28,  0, 34, 34, 28, 28,  0, 28,  2,  2, 26, 26,  3,  3, 28,  0, 13,\n",
      "        34, 28, 28,  0, 28,  2,  2, 26, 26,  3,  3, 28,  0,  3, 34, 28, 28,  0,\n",
      "        28,  2,  2, 26, 26,  3,  3, 28,  0,  3], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 410/500 [1:00:24<12:58,  8.65s/it]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 226359<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-71b28e6e1de4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'N_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/users/udemir15/ELEC491/Bassline-Generator/generator/PyTorch/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "project_name = 'PyTorch_experiments'\n",
    "\n",
    "model_name = dt.datetime.strftime(dt.datetime.now(),\"%d_%m__%H_%M\")\n",
    "\n",
    "with wandb.init(project=project_name, name=model_name, config=all_params, entity='nbg'):\n",
    "\n",
    "    test_loss, test_acc = test(model, test_loader, criterion, device)\n",
    "    print('Test Loss Before Training: {:.6f}'.format(test_loss))\n",
    "    print('Test Accuracy Before Training: {:.3f}'.format(test_acc))\n",
    "    wandb.log({'test_loss': test_loss, 'test_accuracy': test_acc})\n",
    "    \n",
    "    samples = model.sample(24)\n",
    "    print('Initial Sample:\\n{}'.format(samples[0]))\n",
    "    wandb.log({'samples': samples})\n",
    "\n",
    "    for epoch in tqdm(range(train_params['N_epochs'])):\n",
    "\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "        test_loss, test_acc = test(model, test_loader, criterion, device)\n",
    "        \n",
    "        model.decoder.update_teacher_forcing_ratio(decoder_params['teacher_forcing_ratio'],\n",
    "                                                   epoch, train_params['N_epochs']-100)        \n",
    "        \n",
    "        wandb.log({'train_loss': train_loss, 'train_accuracy': train_acc,\n",
    "                  'test_loss': test_loss, 'test_accuracy': test_acc})\n",
    "\n",
    "        if not (epoch % 25):\n",
    "            print('Epoch: {}, train_loss: {:.6f}, train_acc: {:.3f}'.format(epoch, train_loss, train_acc))\n",
    "            checkpoint(model_name, model, optimizer, epoch)\n",
    "            \n",
    "        if not (epoch % 50):\n",
    "            samples = model.sample(24)\n",
    "            print('Sample:\\n{}'.format(samples[0]))\n",
    "            wandb.log({'samples': samples})\n",
    "            \n",
    "    test_loss, test_acc = test(model, test_loader, criterion, device)\n",
    "    print('Test Loss After Training: {:.6f}'.format(test_loss))\n",
    "    print('Test Accuracy After Training: {:.3f}'.format(test_acc))\n",
    "    wandb.log({'test_loss': test_loss, 'test_accuracy': test_acc})\n",
    "    \n",
    "    samples = model.sample(24)\n",
    "    print('Final Sample:\\n{}'.format(samples[0]))\n",
    "    wandb.log({'samples': samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(model, test_loader, criterion, device)\n",
    "\n",
    "for epoch in tqdm(range(train_params['N_epochs'])):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    if epoch+1 % 5:\n",
    "        print('Epoch: {}, train_loss: {:.6f}, acc: {:.3f}'.format(epoch+1, train_loss, train_acc))\n",
    "\n",
    "test_loss, test_acc = test(model, test_loader, criterion, device)\n",
    "print('Test Loss After Training: {:.6f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = len(samples[0])\n",
    "#inp = torch.zeros(K, T)\n",
    "inp = torch.randn(K, T)\n",
    "\n",
    "for t, c in enumerate(samples[0]):    \n",
    "    inp[c,t] = 1\n",
    "    #inp[c,t] = torch.max(inp)\n",
    "    \n",
    "    \n",
    "    \n",
    "# 1,K,T\n",
    "inp = inp.unsqueeze(0).cuda()\n",
    "output= samples[0].unsqueeze(0).cuda()\n",
    "print(inp.shape, output.shape)\n",
    "\n",
    "loss = criterion(inp, output)\n",
    "print(loss.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder_params = {'num_embeddings': K,\n",
    "                  'embedding_dim': 16, #??\n",
    "                  'hidden_size': 128,\n",
    "                  'dropout': 0,  \n",
    "                  'num_layers': 4,              \n",
    "                  'batch_size': data_params['batch_size'],\n",
    "                  'device':device}\n",
    "\n",
    "decoder_params = {'input_size': encoder_params['hidden_size'],\n",
    "                  'output_size': K,\n",
    "                 'num_layers': encoder_params['num_layers'], \n",
    "                 'dropout': 0,\n",
    "                 'batch_size': data_params['batch_size'],\n",
    "                 'sequence_length': sequence_length,\n",
    "                 'device':device}\n",
    "\n",
    "train_params = {'batch_size': data_params['batch_size'],\n",
    "                'N_epochs': 10}\n",
    "\n",
    "all_params = {'encoder_params': encoder_params, 'decoder_params':decoder_params, 'train_params':train_params}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
