{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.backend import argmax, cast\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Input, Embedding, RepeatVector, Reshape, Conv1D, MaxPooling1D, UpSampling1D, Bidirectional, Lambda, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_df = pd.read_csv(\"Datasets/bassline_representations_min.csv\", header=None)\n",
    "major_df = pd.read_csv(\"Datasets/bassline_representations_maj.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((208, 64), (69, 64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_df.shape, major_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat((minor_df, major_df))\n",
    "minor_data = minor_df.values\n",
    "major_data = major_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = np.unique(all_data)\n",
    "vocab = np.arange(len(notes))\n",
    "\n",
    "n2v_mapping = dict(zip(notes, vocab))\n",
    "v2n_mapping = dict(zip(vocab, notes))\n",
    "\n",
    "vocab_size = len(n2v_mapping)\n",
    "\n",
    "def replace_with_dict(ar, dic):\n",
    "    # Extract out keys and values\n",
    "    k = np.array(list(dic.keys()))\n",
    "    v = np.array(list(dic.values()))\n",
    "\n",
    "    # Get argsort indices\n",
    "    sidx = k.argsort()\n",
    "\n",
    "    # Drop the magic bomb with searchsorted to get the corresponding\n",
    "    # places for a in keys (using sorter since a is not necessarily sorted).\n",
    "    # Then trace it back to original order with indexing into sidx\n",
    "    # Finally index into values for desired output.\n",
    "    return v[sidx[np.searchsorted(k,ar,sorter=sidx)]]\n",
    "\n",
    "minor_data = replace_with_dict(minor_data, n2v_mapping).astype(int)\n",
    "major_data = replace_with_dict(major_data, n2v_mapping).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(minor_data, minor_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def replace_continue(arr, cont_token):\n",
    "#    for r in arr:\n",
    "#        for idx, el in enumerate(r[1:]):\n",
    "#            if el == cont_token:\n",
    "#                r[idx + 1] = r[idx]\n",
    "#\n",
    "#cont_token = 35\n",
    "#\n",
    "#\n",
    "#replace_continue(X_train, cont_token)\n",
    "#replace_continue(y_train, cont_token)\n",
    "#replace_continue(X_test, cont_token)\n",
    "#replace_continue(y_test, cont_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, encode_length):\n",
    "    return np.eye(encode_length)[arr.astype(np.uint)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = one_hot_encode(y_train, vocab_size)\n",
    "y_test = one_hot_encode(y_test, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Model 1: Embedding-LSTM Encoder, LSTM-Dense Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NBG_lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 64  # Length of your sequences\n",
    "embed_size = 32\n",
    "latent_dim = 256\n",
    "dropout = 0\n",
    "\n",
    "inputs = Input(shape=(timesteps,))\n",
    "embedded = Embedding(vocab_size, embed_size)(inputs)\n",
    "#encoded = LSTM(latent_dim, return_sequences=True, dropout=dropout)(embedded)\n",
    "encoded = LSTM(latent_dim, dropout=dropout)(embedded)\n",
    "\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "#decoded = LSTM(latent_dim, return_sequences=True, dropout=dropout)(decoded)\n",
    "decoded = LSTM(latent_dim, return_sequences=True, dropout=dropout)(decoded)\n",
    "decoded = Dense(vocab_size, activation='softmax')(decoded)\n",
    "#decoded = argmax(decoded, axis=-1)\n",
    "#decoded = cast(decoded, float)\n",
    "#decoded = Reshape((decoded.shape[1], -1))(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 64, 32)            1152      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               295936    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64, 36)            9252      \n",
      "=================================================================\n",
      "Total params: 831,652\n",
      "Trainable params: 831,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs, encoded)\n",
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(latent_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layers = sequence_autoencoder.layers[-3:]\n",
    "decoded_input = decoder_layers[0](encoded_input)\n",
    "for decoder_layer in decoder_layers[1:]:\n",
    "    decoded_input = decoder_layer(decoded_input)\n",
    "# Create the decoder model\n",
    "decoder = Model(encoded_input, decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 64, 32)            1152      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               295936    \n",
      "=================================================================\n",
      "Total params: 297,088\n",
      "Trainable params: 297,088\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 256)]             0         \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64, 256)           525312    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64, 36)            9252      \n",
      "=================================================================\n",
      "Total params: 534,564\n",
      "Trainable params: 534,564\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "\n",
    "mc = ModelCheckpoint(f'Models/{name}.hdf5', monitor='val_loss')\n",
    "\n",
    "optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "sequence_autoencoder.compile(optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "6/6 [==============================] - 11s 873ms/step - loss: 3.2156 - val_loss: 2.0326\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 2.0760 - val_loss: 1.9369\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0343 - val_loss: 1.9256\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0184 - val_loss: 1.9259\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.9888 - val_loss: 1.9332\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 2.0040 - val_loss: 1.9124\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 2.0092 - val_loss: 1.9233\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0110 - val_loss: 1.9085\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 2.0079 - val_loss: 1.9092\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0138 - val_loss: 1.9025\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.9672 - val_loss: 1.9276\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.9616 - val_loss: 1.9025\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9663 - val_loss: 1.9121\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.9644 - val_loss: 1.9077\n",
      "Epoch 15/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.9424 - val_loss: 1.9119\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.9499 - val_loss: 1.8909\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.9564 - val_loss: 1.8954\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.9511 - val_loss: 1.8978\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.8927 - val_loss: 1.8902\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.8957 - val_loss: 1.8579\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.9160 - val_loss: 1.8813\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9530 - val_loss: 1.9505\n",
      "Epoch 23/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9132 - val_loss: 1.9871\n",
      "Epoch 24/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.8933 - val_loss: 1.9549\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.9508 - val_loss: 1.8995\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.8601 - val_loss: 1.8674\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 1.8438 - val_loss: 1.8876\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.8488 - val_loss: 1.8525\n",
      "Epoch 29/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.8806 - val_loss: 1.8423\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.8487 - val_loss: 1.8175\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.8170 - val_loss: 1.8189\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.7948 - val_loss: 1.8136\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.7855 - val_loss: 1.7925\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.8043 - val_loss: 1.8031\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.7823 - val_loss: 1.7785\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.7499 - val_loss: 1.7883\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.7738 - val_loss: 1.7742\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.7769 - val_loss: 1.7674\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.7307 - val_loss: 1.7604\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.7148 - val_loss: 1.7532\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.7544 - val_loss: 1.7452\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.7747 - val_loss: 1.7934\n",
      "Epoch 43/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.7733 - val_loss: 1.8062\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.7554 - val_loss: 1.7636\n",
      "Epoch 45/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.7486 - val_loss: 1.7390\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.6636 - val_loss: 1.7572\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.7003 - val_loss: 1.7424\n",
      "Epoch 48/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.7132 - val_loss: 1.7330\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.6731 - val_loss: 1.7400\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.6896 - val_loss: 1.7319\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.6301 - val_loss: 1.7168\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.6479 - val_loss: 1.7407\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.6383 - val_loss: 1.7279\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.6314 - val_loss: 1.7384\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.6517 - val_loss: 1.7512\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.6205 - val_loss: 1.7192\n",
      "Epoch 57/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.6229 - val_loss: 1.7364\n",
      "Epoch 58/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.6077 - val_loss: 1.7094\n",
      "Epoch 59/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.5987 - val_loss: 1.7476\n",
      "Epoch 60/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.5879 - val_loss: 1.7412\n",
      "Epoch 61/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.5989 - val_loss: 1.7257\n",
      "Epoch 62/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.5872 - val_loss: 1.7264\n",
      "Epoch 63/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 1.5530 - val_loss: 1.7272\n",
      "Epoch 64/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.5348 - val_loss: 1.7582\n",
      "Epoch 65/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.5253 - val_loss: 1.7418\n",
      "Epoch 66/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.5271 - val_loss: 1.7753\n",
      "Epoch 67/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.5529 - val_loss: 1.7775\n",
      "Epoch 68/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.5680 - val_loss: 1.8183\n",
      "Epoch 69/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.5925 - val_loss: 1.8077\n",
      "Epoch 70/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.6349 - val_loss: 1.7906\n",
      "Epoch 71/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.5836 - val_loss: 1.7864\n",
      "Epoch 72/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.5644 - val_loss: 1.7677\n",
      "Epoch 73/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.5400 - val_loss: 1.7680\n",
      "Epoch 74/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.5260 - val_loss: 1.7711\n",
      "Epoch 75/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.4985 - val_loss: 1.7727\n",
      "Epoch 76/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.5210 - val_loss: 1.7869\n",
      "Epoch 77/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.4973 - val_loss: 1.7832\n",
      "Epoch 78/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.4614 - val_loss: 1.7818\n",
      "Epoch 79/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.4715 - val_loss: 1.7822\n",
      "Epoch 80/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4705 - val_loss: 1.7841\n",
      "Epoch 81/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4362 - val_loss: 1.7821\n",
      "Epoch 82/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.4365 - val_loss: 1.8025\n",
      "Epoch 83/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.4499 - val_loss: 1.7951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4291 - val_loss: 1.8017\n",
      "Epoch 85/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.4570 - val_loss: 1.8191\n",
      "Epoch 86/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.4556 - val_loss: 1.7993\n",
      "Epoch 87/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.4458 - val_loss: 1.8066\n",
      "Epoch 88/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.4505 - val_loss: 1.7961\n",
      "Epoch 89/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.4292 - val_loss: 1.7964\n",
      "Epoch 90/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.4226 - val_loss: 1.8180\n",
      "Epoch 91/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.4303 - val_loss: 1.8102\n",
      "Epoch 92/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4232 - val_loss: 1.8268\n",
      "Epoch 93/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.3805 - val_loss: 1.8476\n",
      "Epoch 94/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.4146 - val_loss: 1.8446\n",
      "Epoch 95/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4085 - val_loss: 1.8514\n",
      "Epoch 96/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.4291 - val_loss: 1.8456\n",
      "Epoch 97/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.4159 - val_loss: 1.8393\n",
      "Epoch 98/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4283 - val_loss: 1.8250\n",
      "Epoch 99/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 1.4265 - val_loss: 1.8168\n",
      "Epoch 100/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.4277 - val_loss: 1.8271\n",
      "Epoch 101/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3992 - val_loss: 1.8009\n",
      "Epoch 102/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.4116 - val_loss: 1.8274\n",
      "Epoch 103/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3971 - val_loss: 1.8138\n",
      "Epoch 104/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.4003 - val_loss: 1.8354\n",
      "Epoch 105/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3867 - val_loss: 1.8342\n",
      "Epoch 106/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3955 - val_loss: 1.8592\n",
      "Epoch 107/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.4168 - val_loss: 1.8275\n",
      "Epoch 108/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3959 - val_loss: 1.8587\n",
      "Epoch 109/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.3994 - val_loss: 1.8531\n",
      "Epoch 110/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.3893 - val_loss: 1.8199\n",
      "Epoch 111/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.4014 - val_loss: 1.8127\n",
      "Epoch 112/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3791 - val_loss: 1.8391\n",
      "Epoch 113/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3772 - val_loss: 1.8407\n",
      "Epoch 114/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3654 - val_loss: 1.8350\n",
      "Epoch 115/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3641 - val_loss: 1.8513\n",
      "Epoch 116/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.3767 - val_loss: 1.8797\n",
      "Epoch 117/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3525 - val_loss: 1.8801\n",
      "Epoch 118/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3739 - val_loss: 1.9179\n",
      "Epoch 119/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3749 - val_loss: 1.9160\n",
      "Epoch 120/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3843 - val_loss: 1.8883\n",
      "Epoch 121/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3379 - val_loss: 1.9173\n",
      "Epoch 122/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.3664 - val_loss: 1.9462\n",
      "Epoch 123/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3675 - val_loss: 1.9465\n",
      "Epoch 124/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3344 - val_loss: 1.9054\n",
      "Epoch 125/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3462 - val_loss: 1.9088\n",
      "Epoch 126/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3372 - val_loss: 1.9448\n",
      "Epoch 127/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3194 - val_loss: 1.9203\n",
      "Epoch 128/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3027 - val_loss: 1.9667\n",
      "Epoch 129/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3341 - val_loss: 1.9795\n",
      "Epoch 130/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3440 - val_loss: 1.9700\n",
      "Epoch 131/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3547 - val_loss: 1.9702\n",
      "Epoch 132/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3534 - val_loss: 1.9615\n",
      "Epoch 133/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3315 - val_loss: 2.0102\n",
      "Epoch 134/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3782 - val_loss: 1.9438\n",
      "Epoch 135/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 1.3670 - val_loss: 1.9404\n",
      "Epoch 136/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3293 - val_loss: 1.9649\n",
      "Epoch 137/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3541 - val_loss: 1.9641\n",
      "Epoch 138/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3392 - val_loss: 1.9458\n",
      "Epoch 139/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3386 - val_loss: 1.9650\n",
      "Epoch 140/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3302 - val_loss: 1.9523\n",
      "Epoch 141/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3083 - val_loss: 1.9576\n",
      "Epoch 142/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3102 - val_loss: 1.9524\n",
      "Epoch 143/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2874 - val_loss: 1.9616\n",
      "Epoch 144/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3009 - val_loss: 1.9706\n",
      "Epoch 145/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2949 - val_loss: 1.9749\n",
      "Epoch 146/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3069 - val_loss: 1.9638\n",
      "Epoch 147/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2895 - val_loss: 2.0042\n",
      "Epoch 148/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.2915 - val_loss: 1.9945\n",
      "Epoch 149/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.2684 - val_loss: 2.0219\n",
      "Epoch 150/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2909 - val_loss: 2.0302\n",
      "Epoch 151/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.2600 - val_loss: 2.0392\n",
      "Epoch 152/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.2727 - val_loss: 2.0138\n",
      "Epoch 153/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.3149 - val_loss: 1.9852\n",
      "Epoch 154/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.2866 - val_loss: 1.9942\n",
      "Epoch 155/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2830 - val_loss: 2.0250\n",
      "Epoch 156/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2838 - val_loss: 2.0221\n",
      "Epoch 157/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2592 - val_loss: 2.0508\n",
      "Epoch 158/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.2622 - val_loss: 2.0654\n",
      "Epoch 159/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.2565 - val_loss: 2.0276\n",
      "Epoch 160/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2672 - val_loss: 2.0778\n",
      "Epoch 161/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2242 - val_loss: 2.0994\n",
      "Epoch 162/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2392 - val_loss: 2.0565\n",
      "Epoch 163/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2202 - val_loss: 2.1070\n",
      "Epoch 164/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2118 - val_loss: 2.0627\n",
      "Epoch 165/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2377 - val_loss: 2.1003\n",
      "Epoch 166/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2524 - val_loss: 2.0776\n",
      "Epoch 167/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2430 - val_loss: 2.0466\n",
      "Epoch 168/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2394 - val_loss: 2.0208\n",
      "Epoch 169/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.2643 - val_loss: 2.0398\n",
      "Epoch 170/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2624 - val_loss: 2.0202\n",
      "Epoch 171/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 1.2533 - val_loss: 2.0171\n",
      "Epoch 172/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2355 - val_loss: 2.0261\n",
      "Epoch 173/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2412 - val_loss: 2.0245\n",
      "Epoch 174/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2212 - val_loss: 2.0458\n",
      "Epoch 175/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.2226 - val_loss: 2.0396\n",
      "Epoch 176/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2325 - val_loss: 2.0545\n",
      "Epoch 177/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.1977 - val_loss: 2.0625\n",
      "Epoch 178/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1867 - val_loss: 2.0615\n",
      "Epoch 179/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2029 - val_loss: 2.1135\n",
      "Epoch 180/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2200 - val_loss: 2.0912\n",
      "Epoch 181/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1828 - val_loss: 2.0669\n",
      "Epoch 182/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2087 - val_loss: 2.1297\n",
      "Epoch 183/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.2302 - val_loss: 2.0592\n",
      "Epoch 184/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1750 - val_loss: 2.1280\n",
      "Epoch 185/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1693 - val_loss: 2.1353\n",
      "Epoch 186/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1756 - val_loss: 2.1730\n",
      "Epoch 187/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1454 - val_loss: 2.1539\n",
      "Epoch 188/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1569 - val_loss: 2.1343\n",
      "Epoch 189/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1369 - val_loss: 2.1331\n",
      "Epoch 190/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.1696 - val_loss: 2.1318\n",
      "Epoch 191/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1608 - val_loss: 2.1145\n",
      "Epoch 192/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1629 - val_loss: 2.0986\n",
      "Epoch 193/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1492 - val_loss: 2.0740\n",
      "Epoch 194/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1625 - val_loss: 2.1043\n",
      "Epoch 195/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1542 - val_loss: 2.1057\n",
      "Epoch 196/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2484 - val_loss: 2.0755\n",
      "Epoch 197/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1600 - val_loss: 2.0399\n",
      "Epoch 198/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.1526 - val_loss: 2.1221\n",
      "Epoch 199/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.1669 - val_loss: 2.0764\n",
      "Epoch 200/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1523 - val_loss: 2.1366\n",
      "Epoch 201/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1499 - val_loss: 2.0813\n",
      "Epoch 202/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.1786 - val_loss: 2.0924\n",
      "Epoch 203/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.1554 - val_loss: 2.1390\n",
      "Epoch 204/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1509 - val_loss: 2.0665\n",
      "Epoch 205/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.1696 - val_loss: 2.0926\n",
      "Epoch 206/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1462 - val_loss: 2.0862\n",
      "Epoch 207/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 1.1586 - val_loss: 2.0098\n",
      "Epoch 208/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1479 - val_loss: 2.0324\n",
      "Epoch 209/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1317 - val_loss: 2.0791\n",
      "Epoch 210/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.0859 - val_loss: 2.0262\n",
      "Epoch 211/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.0834 - val_loss: 2.0467\n",
      "Epoch 212/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.0718 - val_loss: 2.1089\n",
      "Epoch 213/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.0283 - val_loss: 2.1281\n",
      "Epoch 214/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.0331 - val_loss: 2.2367\n",
      "Epoch 215/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.0368 - val_loss: 2.2208\n",
      "Epoch 216/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.0396 - val_loss: 2.1604\n",
      "Epoch 217/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.0539 - val_loss: 2.1737\n",
      "Epoch 218/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.0754 - val_loss: 2.1327\n",
      "Epoch 219/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.0689 - val_loss: 2.1970\n",
      "Epoch 220/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.0632 - val_loss: 2.1583\n",
      "Epoch 221/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.0526 - val_loss: 2.2393\n",
      "Epoch 222/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.0338 - val_loss: 2.1792\n",
      "Epoch 223/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.0149 - val_loss: 2.1535\n",
      "Epoch 224/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.9907 - val_loss: 2.1468\n",
      "Epoch 225/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.0046 - val_loss: 2.1520\n",
      "Epoch 226/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.9885 - val_loss: 2.1701\n",
      "Epoch 227/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.9784 - val_loss: 2.1940\n",
      "Epoch 228/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9601 - val_loss: 2.2114\n",
      "Epoch 229/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.9639 - val_loss: 2.2489\n",
      "Epoch 230/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9339 - val_loss: 2.2915\n",
      "Epoch 231/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.9511 - val_loss: 2.2454\n",
      "Epoch 232/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.9368 - val_loss: 2.2500\n",
      "Epoch 233/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.9215 - val_loss: 2.2605\n",
      "Epoch 234/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.9396 - val_loss: 2.2673\n",
      "Epoch 235/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.9145 - val_loss: 2.3067\n",
      "Epoch 236/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.9044 - val_loss: 2.2926\n",
      "Epoch 237/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8952 - val_loss: 2.2749\n",
      "Epoch 238/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8970 - val_loss: 2.3214\n",
      "Epoch 239/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8838 - val_loss: 2.3064\n",
      "Epoch 240/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.8878 - val_loss: 2.3494\n",
      "Epoch 241/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.8643 - val_loss: 2.3357\n",
      "Epoch 242/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.8623 - val_loss: 2.3656\n",
      "Epoch 243/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.8675 - val_loss: 2.3218\n",
      "Epoch 244/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.8571 - val_loss: 2.3544\n",
      "Epoch 245/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.8947 - val_loss: 2.4511\n",
      "Epoch 246/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.0265 - val_loss: 2.3664\n",
      "Epoch 247/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.9981 - val_loss: 2.3121\n",
      "Epoch 248/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 42ms/step - loss: 0.9786 - val_loss: 2.2778\n",
      "Epoch 249/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.0110 - val_loss: 2.2734\n",
      "Epoch 250/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.9734 - val_loss: 2.2486\n",
      "Epoch 251/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9484 - val_loss: 2.2650\n",
      "Epoch 252/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.9350 - val_loss: 2.2700\n",
      "Epoch 253/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.8973 - val_loss: 2.2888\n",
      "Epoch 254/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.8673 - val_loss: 2.3405\n",
      "Epoch 255/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.8871 - val_loss: 2.3286\n",
      "Epoch 256/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.8849 - val_loss: 2.3520\n",
      "Epoch 257/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.8767 - val_loss: 2.4099\n",
      "Epoch 258/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.8679 - val_loss: 2.3757\n",
      "Epoch 259/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8738 - val_loss: 2.3807\n",
      "Epoch 260/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8505 - val_loss: 2.3855\n",
      "Epoch 261/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.8150 - val_loss: 2.4504\n",
      "Epoch 262/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.8360 - val_loss: 2.3272\n",
      "Epoch 263/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1224 - val_loss: 2.3139\n",
      "Epoch 264/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1340 - val_loss: 2.2875\n",
      "Epoch 265/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.0969 - val_loss: 2.3000\n",
      "Epoch 266/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.0329 - val_loss: 2.3375\n",
      "Epoch 267/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.9858 - val_loss: 2.3498\n",
      "Epoch 268/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.9760 - val_loss: 2.2687\n",
      "Epoch 269/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.9872 - val_loss: 2.3309\n",
      "Epoch 270/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.9623 - val_loss: 2.2667\n",
      "Epoch 271/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.9624 - val_loss: 2.2329\n",
      "Epoch 272/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.9539 - val_loss: 2.2961\n",
      "Epoch 273/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9194 - val_loss: 2.3683\n",
      "Epoch 274/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.1238 - val_loss: 2.2970\n",
      "Epoch 275/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.0612 - val_loss: 2.3470\n",
      "Epoch 276/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.0855 - val_loss: 2.3336\n",
      "Epoch 277/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.0124 - val_loss: 2.3415\n",
      "Epoch 278/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.9966 - val_loss: 2.3262\n",
      "Epoch 279/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.9746 - val_loss: 2.3257\n",
      "Epoch 280/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.9437 - val_loss: 2.3184\n",
      "Epoch 281/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.9159 - val_loss: 2.3178\n",
      "Epoch 282/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8908 - val_loss: 2.3403\n",
      "Epoch 283/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.8875 - val_loss: 2.3458\n",
      "Epoch 284/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.8646 - val_loss: 2.3830\n",
      "Epoch 285/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.8533 - val_loss: 2.4052\n",
      "Epoch 286/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.8468 - val_loss: 2.4253\n",
      "Epoch 287/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.8449 - val_loss: 2.4269\n",
      "Epoch 288/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.8301 - val_loss: 2.4459\n",
      "Epoch 289/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.8363 - val_loss: 2.4428\n",
      "Epoch 290/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.7902 - val_loss: 2.4487\n",
      "Epoch 291/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.8174 - val_loss: 2.4151\n",
      "Epoch 292/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.7971 - val_loss: 2.4480\n",
      "Epoch 293/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.8019 - val_loss: 2.4490\n",
      "Epoch 294/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.7832 - val_loss: 2.4648\n",
      "Epoch 295/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7940 - val_loss: 2.4602\n",
      "Epoch 296/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.7724 - val_loss: 2.4919\n",
      "Epoch 297/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7991 - val_loss: 2.4837\n",
      "Epoch 298/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.7628 - val_loss: 2.5310\n",
      "Epoch 299/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.7659 - val_loss: 2.5259\n",
      "Epoch 300/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.7818 - val_loss: 2.5446\n",
      "Epoch 301/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.7745 - val_loss: 2.5507\n",
      "Epoch 302/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7544 - val_loss: 2.6080\n",
      "Epoch 303/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.7529 - val_loss: 2.5590\n",
      "Epoch 304/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7603 - val_loss: 2.6041\n",
      "Epoch 305/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.7453 - val_loss: 2.5817\n",
      "Epoch 306/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.7607 - val_loss: 2.6236\n",
      "Epoch 307/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.7350 - val_loss: 2.5789\n",
      "Epoch 308/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.7545 - val_loss: 2.6430\n",
      "Epoch 309/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7831 - val_loss: 2.6197\n",
      "Epoch 310/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.7759 - val_loss: 2.5865\n",
      "Epoch 311/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.7558 - val_loss: 2.6646\n",
      "Epoch 312/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7819 - val_loss: 2.6345\n",
      "Epoch 313/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7823 - val_loss: 2.6547\n",
      "Epoch 314/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.8015 - val_loss: 2.6421\n",
      "Epoch 315/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.7806 - val_loss: 2.6739\n",
      "Epoch 316/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.7467 - val_loss: 2.6655\n",
      "Epoch 317/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.7266 - val_loss: 2.6492\n",
      "Epoch 318/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.7607 - val_loss: 2.6792\n",
      "Epoch 319/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.7433 - val_loss: 2.6756\n",
      "Epoch 320/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7358 - val_loss: 2.6861\n",
      "Epoch 321/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.7280 - val_loss: 2.6844\n",
      "Epoch 322/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.7386 - val_loss: 2.6846\n",
      "Epoch 323/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.7232 - val_loss: 2.6535\n",
      "Epoch 324/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.7118 - val_loss: 2.7009\n",
      "Epoch 325/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7094 - val_loss: 2.6937\n",
      "Epoch 326/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7179 - val_loss: 2.7357\n",
      "Epoch 327/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.7386 - val_loss: 2.6566\n",
      "Epoch 328/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.7505 - val_loss: 2.7178\n",
      "Epoch 329/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.7267 - val_loss: 2.7291\n",
      "Epoch 330/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 44ms/step - loss: 0.7456 - val_loss: 2.7543\n",
      "Epoch 331/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7507 - val_loss: 2.7134\n",
      "Epoch 332/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.7668 - val_loss: 2.7462\n",
      "Epoch 333/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.7496 - val_loss: 2.6486\n",
      "Epoch 334/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.9048 - val_loss: 2.5209\n",
      "Epoch 335/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.0693 - val_loss: 2.4589\n",
      "Epoch 336/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.0787 - val_loss: 2.4151\n",
      "Epoch 337/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1013 - val_loss: 2.4097\n",
      "Epoch 338/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1230 - val_loss: 2.3865\n",
      "Epoch 339/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.1027 - val_loss: 2.3665\n",
      "Epoch 340/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1293 - val_loss: 2.8210\n",
      "Epoch 341/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3320 - val_loss: 2.3327\n",
      "Epoch 342/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2827 - val_loss: 2.3549\n",
      "Epoch 343/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2175 - val_loss: 2.4329\n",
      "Epoch 344/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3237 - val_loss: 2.3193\n",
      "Epoch 345/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2290 - val_loss: 2.2885\n",
      "Epoch 346/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.1520 - val_loss: 2.2785\n",
      "Epoch 347/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.1286 - val_loss: 2.1935\n",
      "Epoch 348/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.0738 - val_loss: 2.2307\n",
      "Epoch 349/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.0884 - val_loss: 2.2300\n",
      "Epoch 350/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.9981 - val_loss: 2.2001\n",
      "Epoch 351/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.9969 - val_loss: 2.2219\n",
      "Epoch 352/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.0073 - val_loss: 2.2360\n",
      "Epoch 353/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9973 - val_loss: 2.2225\n",
      "Epoch 354/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.0077 - val_loss: 2.2406\n",
      "Epoch 355/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.9788 - val_loss: 2.2633\n",
      "Epoch 356/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9340 - val_loss: 2.2875\n",
      "Epoch 357/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.9608 - val_loss: 2.2849\n",
      "Epoch 358/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.9127 - val_loss: 2.3089\n",
      "Epoch 359/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.9045 - val_loss: 2.3291\n",
      "Epoch 360/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8745 - val_loss: 2.3410\n",
      "Epoch 361/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.8614 - val_loss: 2.3464\n",
      "Epoch 362/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.8786 - val_loss: 2.3654\n",
      "Epoch 363/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.8573 - val_loss: 2.3853\n",
      "Epoch 364/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.8607 - val_loss: 2.3991\n",
      "Epoch 365/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.8502 - val_loss: 2.3964\n",
      "Epoch 366/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.8255 - val_loss: 2.4037\n",
      "Epoch 367/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.8201 - val_loss: 2.4146\n",
      "Epoch 368/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.8134 - val_loss: 2.4396\n",
      "Epoch 369/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.8169 - val_loss: 2.4155\n",
      "Epoch 370/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8133 - val_loss: 2.4364\n",
      "Epoch 371/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.8066 - val_loss: 2.4452\n",
      "Epoch 372/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7941 - val_loss: 2.4798\n",
      "Epoch 373/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.7946 - val_loss: 2.4802\n",
      "Epoch 374/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8141 - val_loss: 2.5096\n",
      "Epoch 375/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.8058 - val_loss: 2.4979\n",
      "Epoch 376/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7884 - val_loss: 2.5342\n",
      "Epoch 377/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7684 - val_loss: 2.5165\n",
      "Epoch 378/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7824 - val_loss: 2.5519\n",
      "Epoch 379/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.7699 - val_loss: 2.5613\n",
      "Epoch 380/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7826 - val_loss: 2.5412\n",
      "Epoch 381/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7550 - val_loss: 2.5804\n",
      "Epoch 382/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7617 - val_loss: 2.5864\n",
      "Epoch 383/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.7859 - val_loss: 2.5884\n",
      "Epoch 384/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7794 - val_loss: 2.6039\n",
      "Epoch 385/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7326 - val_loss: 2.5886\n",
      "Epoch 386/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.7347 - val_loss: 2.6301\n",
      "Epoch 387/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.7455 - val_loss: 2.6022\n",
      "Epoch 388/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7270 - val_loss: 2.6392\n",
      "Epoch 389/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7523 - val_loss: 2.6291\n",
      "Epoch 390/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7402 - val_loss: 2.6882\n",
      "Epoch 391/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.7323 - val_loss: 2.6315\n",
      "Epoch 392/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7615 - val_loss: 2.6712\n",
      "Epoch 393/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.7469 - val_loss: 2.6356\n",
      "Epoch 394/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7398 - val_loss: 2.6715\n",
      "Epoch 395/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.7550 - val_loss: 2.6564\n",
      "Epoch 396/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.7316 - val_loss: 2.6846\n",
      "Epoch 397/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.7441 - val_loss: 2.6803\n",
      "Epoch 398/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7409 - val_loss: 2.6839\n",
      "Epoch 399/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.7634 - val_loss: 2.6446\n",
      "Epoch 400/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.7385 - val_loss: 2.6682\n",
      "Epoch 401/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.7337 - val_loss: 2.6584\n",
      "Epoch 402/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.7265 - val_loss: 2.6207\n",
      "Epoch 403/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.8666 - val_loss: 2.6031\n",
      "Epoch 404/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.0325 - val_loss: 2.6245\n",
      "Epoch 405/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1238 - val_loss: 2.4992\n",
      "Epoch 406/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.1037 - val_loss: 2.4853\n",
      "Epoch 407/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.0252 - val_loss: 2.4646\n",
      "Epoch 408/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.0871 - val_loss: 2.4165\n",
      "Epoch 409/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.2922 - val_loss: 2.3393\n",
      "Epoch 410/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.3781 - val_loss: 2.3326\n",
      "Epoch 411/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3954 - val_loss: 2.3409\n",
      "Epoch 412/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3188 - val_loss: 2.2824\n",
      "Epoch 413/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3077 - val_loss: 2.2106\n",
      "Epoch 414/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2627 - val_loss: 2.2378\n",
      "Epoch 415/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2088 - val_loss: 2.1965\n",
      "Epoch 416/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1449 - val_loss: 2.1823\n",
      "Epoch 417/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.0890 - val_loss: 2.2195\n",
      "Epoch 418/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.0932 - val_loss: 2.2000\n",
      "Epoch 419/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.0583 - val_loss: 2.2366\n",
      "Epoch 420/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.0556 - val_loss: 2.2430\n",
      "Epoch 421/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.1206 - val_loss: 2.2538\n",
      "Epoch 422/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.0881 - val_loss: 2.2847\n",
      "Epoch 423/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 1.0906 - val_loss: 2.2620\n",
      "Epoch 424/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.0465 - val_loss: 2.2562\n",
      "Epoch 425/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.0222 - val_loss: 2.2781\n",
      "Epoch 426/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.9799 - val_loss: 2.2989\n",
      "Epoch 427/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.9978 - val_loss: 2.2920\n",
      "Epoch 428/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.9548 - val_loss: 2.2975\n",
      "Epoch 429/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.9435 - val_loss: 2.3328\n",
      "Epoch 430/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9419 - val_loss: 2.3426\n",
      "Epoch 431/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.9325 - val_loss: 2.3763\n",
      "Epoch 432/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.9180 - val_loss: 2.3731\n",
      "Epoch 433/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.8818 - val_loss: 2.3778\n",
      "Epoch 434/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.8676 - val_loss: 2.3775\n",
      "Epoch 435/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8665 - val_loss: 2.3873\n",
      "Epoch 436/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.8346 - val_loss: 2.3855\n",
      "Epoch 437/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.8170 - val_loss: 2.3867\n",
      "Epoch 438/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.8177 - val_loss: 2.4069\n",
      "Epoch 439/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.8317 - val_loss: 2.4326\n",
      "Epoch 440/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.8355 - val_loss: 2.4489\n",
      "Epoch 441/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.8137 - val_loss: 2.4544\n",
      "Epoch 442/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.8284 - val_loss: 2.4638\n",
      "Epoch 443/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7820 - val_loss: 2.4754\n",
      "Epoch 444/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.7939 - val_loss: 2.4867\n",
      "Epoch 445/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8001 - val_loss: 2.5127\n",
      "Epoch 446/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7870 - val_loss: 2.5092\n",
      "Epoch 447/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7545 - val_loss: 2.5043\n",
      "Epoch 448/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.7523 - val_loss: 2.5503\n",
      "Epoch 449/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.7623 - val_loss: 2.5822\n",
      "Epoch 450/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.7509 - val_loss: 2.5469\n",
      "Epoch 451/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7683 - val_loss: 2.5308\n",
      "Epoch 452/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7551 - val_loss: 2.5569\n",
      "Epoch 453/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7600 - val_loss: 2.5648\n",
      "Epoch 454/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7748 - val_loss: 2.5778\n",
      "Epoch 455/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7677 - val_loss: 2.5791\n",
      "Epoch 456/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7599 - val_loss: 2.6118\n",
      "Epoch 457/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.8683 - val_loss: 2.6290\n",
      "Epoch 458/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.8613 - val_loss: 2.6223\n",
      "Epoch 459/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.8101 - val_loss: 2.5968\n",
      "Epoch 460/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.8155 - val_loss: 2.5863\n",
      "Epoch 461/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.8191 - val_loss: 2.6179\n",
      "Epoch 462/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.8100 - val_loss: 2.6431\n",
      "Epoch 463/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.8744 - val_loss: 2.6121\n",
      "Epoch 464/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8393 - val_loss: 2.6402\n",
      "Epoch 465/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8228 - val_loss: 2.6557\n",
      "Epoch 466/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.8072 - val_loss: 2.6381\n",
      "Epoch 467/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.8229 - val_loss: 2.6312\n",
      "Epoch 468/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7907 - val_loss: 2.6157\n",
      "Epoch 469/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.7892 - val_loss: 2.6274\n",
      "Epoch 470/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7735 - val_loss: 2.6490\n",
      "Epoch 471/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.7382 - val_loss: 2.6482\n",
      "Epoch 472/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.7510 - val_loss: 2.6170\n",
      "Epoch 473/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7457 - val_loss: 2.6376\n",
      "Epoch 474/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.7050 - val_loss: 2.6479\n",
      "Epoch 475/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7030 - val_loss: 2.6600\n",
      "Epoch 476/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.7175 - val_loss: 2.6822\n",
      "Epoch 477/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.7393 - val_loss: 2.6484\n",
      "Epoch 478/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.9094 - val_loss: 2.5925\n",
      "Epoch 479/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2275 - val_loss: 2.4618\n",
      "Epoch 480/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1870 - val_loss: 2.4314\n",
      "Epoch 481/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3149 - val_loss: 2.4571\n",
      "Epoch 482/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.3612 - val_loss: 2.3699\n",
      "Epoch 483/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3946 - val_loss: 2.3271\n",
      "Epoch 484/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3001 - val_loss: 2.3038\n",
      "Epoch 485/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.2056 - val_loss: 2.2769\n",
      "Epoch 486/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.1531 - val_loss: 2.2936\n",
      "Epoch 487/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1665 - val_loss: 2.2958\n",
      "Epoch 488/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1288 - val_loss: 2.3134\n",
      "Epoch 489/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.0827 - val_loss: 2.3122\n",
      "Epoch 490/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.0206 - val_loss: 2.3502\n",
      "Epoch 491/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9716 - val_loss: 2.3866\n",
      "Epoch 492/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.9557 - val_loss: 2.3989\n",
      "Epoch 493/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.9537 - val_loss: 2.4366\n",
      "Epoch 494/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 37ms/step - loss: 0.9532 - val_loss: 2.4350\n",
      "Epoch 495/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.9367 - val_loss: 2.4383\n",
      "Epoch 496/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.8534 - val_loss: 2.4496\n",
      "Epoch 497/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.8534 - val_loss: 2.4599\n",
      "Epoch 498/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.8455 - val_loss: 2.4730\n",
      "Epoch 499/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.8485 - val_loss: 2.4890\n",
      "Epoch 500/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.8530 - val_loss: 2.5198\n"
     ]
    }
   ],
   "source": [
    "hist = sequence_autoencoder.fit(X_train, y_train,\n",
    "                epochs=500,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder'), mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.plot(hist.history['loss'], label='training')\n",
    "plt.plot(hist.history['val_loss'], label='testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'figures/{name}', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_autoencoder = load_model(f'Models/{name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6790286144578314"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sequence_autoencoder.predict(X_train).argmax(axis=-1)\n",
    "accuracy_score(y_train.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44568452380952384"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sequence_autoencoder.predict(X_test).argmax(axis=-1)\n",
    "accuracy_score(y_test.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Embedding-BiLSTM-BiLSTM Encoder, LSTM-Dense Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NBG_bilstm_stacked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 64  # Length of your sequences\n",
    "embed_size = 16\n",
    "latent_dim = 256\n",
    "dropout = 0\n",
    "\n",
    "inputs = Input(shape=(timesteps,))\n",
    "embedded = Embedding(vocab_size, embed_size)(inputs)\n",
    "encoded = Bidirectional(LSTM(latent_dim, return_sequences=True, dropout=dropout))(embedded)\n",
    "encoded = Bidirectional(LSTM(latent_dim, dropout=dropout))(embedded)\n",
    "\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "#decoded = LSTM(latent_dim, return_sequences=True, dropout=dropout)(decoded)\n",
    "decoded = LSTM(latent_dim, return_sequences=True, dropout=dropout)(decoded)\n",
    "decoded = Dense(vocab_size, activation='softmax')(decoded)\n",
    "#decoded = argmax(decoded, axis=-1)\n",
    "#decoded = cast(decoded, float)\n",
    "#decoded = Reshape((decoded.shape[1], -1))(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 64, 16)            576       \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               559104    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 64, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64, 256)           787456    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64, 36)            9252      \n",
      "=================================================================\n",
      "Total params: 1,356,388\n",
      "Trainable params: 1,356,388\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs, encoded)\n",
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(2*latent_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layers = sequence_autoencoder.layers[-3:]\n",
    "decoded_input = decoder_layers[0](encoded_input)\n",
    "for decoder_layer in decoder_layers[1:]:\n",
    "    decoded_input = decoder_layer(decoded_input)\n",
    "# Create the decoder model\n",
    "decoder = Model(encoded_input, decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 64, 16)            576       \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               559104    \n",
      "=================================================================\n",
      "Total params: 559,680\n",
      "Trainable params: 559,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 64, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64, 256)           787456    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64, 36)            9252      \n",
      "=================================================================\n",
      "Total params: 796,708\n",
      "Trainable params: 796,708\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "\n",
    "mc = ModelCheckpoint(f'Models/{name}.hdf5', monitor='val_loss')\n",
    "\n",
    "optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "sequence_autoencoder.compile(optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "6/6 [==============================] - 5s 346ms/step - loss: 3.2251 - val_loss: 2.0501\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 2.0680 - val_loss: 1.9696\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 2.0222 - val_loss: 1.9292\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9673 - val_loss: 1.9290\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0210 - val_loss: 1.9163\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 2.0077 - val_loss: 1.9186\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 2.0119 - val_loss: 1.9072\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 2.0287 - val_loss: 1.9018\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.9878 - val_loss: 1.9068\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 2.0013 - val_loss: 1.8907\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 1.9649 - val_loss: 1.8782\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.9408 - val_loss: 1.8871\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.9246 - val_loss: 1.8941\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.9664 - val_loss: 1.8818\n",
      "Epoch 15/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.9454 - val_loss: 1.8656\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.9249 - val_loss: 1.8668\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.9237 - val_loss: 1.8578\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 1.9177 - val_loss: 1.8438\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.8951 - val_loss: 1.8496\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.8787 - val_loss: 1.8378\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.8717 - val_loss: 1.8189\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.8429 - val_loss: 1.8163\n",
      "Epoch 23/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.8862 - val_loss: 1.8903\n",
      "Epoch 24/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 1.8684 - val_loss: 1.8398\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.8907 - val_loss: 1.8275\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.8268 - val_loss: 1.7885\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.8058 - val_loss: 1.7999\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.7908 - val_loss: 1.7758\n",
      "Epoch 29/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.7889 - val_loss: 1.7459\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.7160 - val_loss: 1.7492\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.7241 - val_loss: 1.7576\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.7565 - val_loss: 1.7377\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.7158 - val_loss: 1.7304\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.7498 - val_loss: 1.7230\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.6919 - val_loss: 1.7278\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.7341 - val_loss: 1.7277\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.7471 - val_loss: 1.7256\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.7067 - val_loss: 1.7156\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.7001 - val_loss: 1.7189\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.6831 - val_loss: 1.7200\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.7080 - val_loss: 1.7342\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.6703 - val_loss: 1.7262\n",
      "Epoch 43/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.6729 - val_loss: 1.7191\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.6364 - val_loss: 1.7402\n",
      "Epoch 45/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.6442 - val_loss: 1.7313\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.6596 - val_loss: 1.7659\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.6640 - val_loss: 1.7288\n",
      "Epoch 48/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.6289 - val_loss: 1.7379\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.6169 - val_loss: 1.7181\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.6085 - val_loss: 1.7145\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.5918 - val_loss: 1.7350\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.5962 - val_loss: 1.7379\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.5851 - val_loss: 1.7323\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.5747 - val_loss: 1.7402\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.5715 - val_loss: 1.7297\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 1.5523 - val_loss: 1.7299\n",
      "Epoch 57/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.5489 - val_loss: 1.7331\n",
      "Epoch 58/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.5661 - val_loss: 1.7414\n",
      "Epoch 59/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.5477 - val_loss: 1.7585\n",
      "Epoch 60/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.5402 - val_loss: 1.7210\n",
      "Epoch 61/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.5513 - val_loss: 1.7369\n",
      "Epoch 62/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.5375 - val_loss: 1.7420\n",
      "Epoch 63/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.5710 - val_loss: 1.7081\n",
      "Epoch 64/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.5356 - val_loss: 1.7361\n",
      "Epoch 65/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.5289 - val_loss: 1.7435\n",
      "Epoch 66/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.5490 - val_loss: 1.7246\n",
      "Epoch 67/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.4697 - val_loss: 1.7230\n",
      "Epoch 68/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.4925 - val_loss: 1.7476\n",
      "Epoch 69/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.5122 - val_loss: 1.7257\n",
      "Epoch 70/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.5016 - val_loss: 1.7282\n",
      "Epoch 71/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.4783 - val_loss: 1.7529\n",
      "Epoch 72/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.4734 - val_loss: 1.7340\n",
      "Epoch 73/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.4957 - val_loss: 1.7343\n",
      "Epoch 74/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.4878 - val_loss: 1.7528\n",
      "Epoch 75/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.4946 - val_loss: 1.7543\n",
      "Epoch 76/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 1.4559 - val_loss: 1.7468\n",
      "Epoch 77/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.5123 - val_loss: 1.7162\n",
      "Epoch 78/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.5012 - val_loss: 1.7620\n",
      "Epoch 79/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.4966 - val_loss: 1.7221\n",
      "Epoch 80/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.5160 - val_loss: 1.7541\n",
      "Epoch 81/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.4844 - val_loss: 1.7344\n",
      "Epoch 82/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4536 - val_loss: 1.7167\n",
      "Epoch 83/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.4717 - val_loss: 1.7096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.4359 - val_loss: 1.7174\n",
      "Epoch 85/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.4515 - val_loss: 1.7275\n",
      "Epoch 86/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.4205 - val_loss: 1.7322\n",
      "Epoch 87/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.4082 - val_loss: 1.7320\n",
      "Epoch 88/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 1.4097 - val_loss: 1.7235\n",
      "Epoch 89/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3954 - val_loss: 1.7275\n",
      "Epoch 90/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.4116 - val_loss: 1.7403\n",
      "Epoch 91/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.4005 - val_loss: 1.7557\n",
      "Epoch 92/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3868 - val_loss: 1.7386\n",
      "Epoch 93/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3755 - val_loss: 1.7389\n",
      "Epoch 94/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.3824 - val_loss: 1.7461\n",
      "Epoch 95/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.3763 - val_loss: 1.7577\n",
      "Epoch 96/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.3900 - val_loss: 1.7580\n",
      "Epoch 97/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.3791 - val_loss: 1.7606\n",
      "Epoch 98/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.3605 - val_loss: 1.7604\n",
      "Epoch 99/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3843 - val_loss: 1.7770\n",
      "Epoch 100/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.3748 - val_loss: 1.7774\n",
      "Epoch 101/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.3667 - val_loss: 1.7575\n",
      "Epoch 102/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.3482 - val_loss: 1.7765\n",
      "Epoch 103/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.3631 - val_loss: 1.7768\n",
      "Epoch 104/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.3466 - val_loss: 1.7736\n",
      "Epoch 105/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.3770 - val_loss: 1.7900\n",
      "Epoch 106/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.3779 - val_loss: 1.7772\n",
      "Epoch 107/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.3458 - val_loss: 1.7478\n",
      "Epoch 108/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.3714 - val_loss: 1.7544\n",
      "Epoch 109/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.3887 - val_loss: 1.7610\n",
      "Epoch 110/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.3893 - val_loss: 1.7662\n",
      "Epoch 111/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.4087 - val_loss: 1.7831\n",
      "Epoch 112/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.4077 - val_loss: 1.7782\n",
      "Epoch 113/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.3947 - val_loss: 1.7788\n",
      "Epoch 114/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.3721 - val_loss: 1.7891\n",
      "Epoch 115/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.3624 - val_loss: 1.7994\n",
      "Epoch 116/500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 1.3690 - val_loss: 1.7903\n",
      "Epoch 117/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.3706 - val_loss: 1.8013\n",
      "Epoch 118/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.3365 - val_loss: 1.7855\n",
      "Epoch 119/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.3746 - val_loss: 1.8021\n",
      "Epoch 120/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.3917 - val_loss: 1.8028\n",
      "Epoch 121/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.3546 - val_loss: 1.7947\n",
      "Epoch 122/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.3655 - val_loss: 1.8061\n",
      "Epoch 123/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.3342 - val_loss: 1.8078\n",
      "Epoch 124/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.3680 - val_loss: 1.8145\n",
      "Epoch 125/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.3539 - val_loss: 1.8029\n",
      "Epoch 126/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.3316 - val_loss: 1.8187\n",
      "Epoch 127/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.3455 - val_loss: 1.8171\n",
      "Epoch 128/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.3372 - val_loss: 1.8034\n",
      "Epoch 129/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.3083 - val_loss: 1.8138\n",
      "Epoch 130/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.3312 - val_loss: 1.8320\n",
      "Epoch 131/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.3490 - val_loss: 1.8303\n",
      "Epoch 132/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.3425 - val_loss: 1.8603\n",
      "Epoch 133/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.3738 - val_loss: 1.8492\n",
      "Epoch 134/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.4217 - val_loss: 1.8393\n",
      "Epoch 135/500\n",
      "6/6 [==============================] - 0s 88ms/step - loss: 1.4071 - val_loss: 1.8316\n",
      "Epoch 136/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.4060 - val_loss: 1.8599\n",
      "Epoch 137/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3986 - val_loss: 1.8260\n",
      "Epoch 138/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3731 - val_loss: 1.8394\n",
      "Epoch 139/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.4003 - val_loss: 1.8134\n",
      "Epoch 140/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4037 - val_loss: 1.8024\n",
      "Epoch 141/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.3956 - val_loss: 1.7866\n",
      "Epoch 142/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.3321 - val_loss: 1.8185\n",
      "Epoch 143/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.3432 - val_loss: 1.8301\n",
      "Epoch 144/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3436 - val_loss: 1.8359\n",
      "Epoch 145/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.3268 - val_loss: 1.8080\n",
      "Epoch 146/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3382 - val_loss: 1.8358\n",
      "Epoch 147/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3026 - val_loss: 1.8288\n",
      "Epoch 148/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3078 - val_loss: 1.8396\n",
      "Epoch 149/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3114 - val_loss: 1.8284\n",
      "Epoch 150/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.3241 - val_loss: 1.8622\n",
      "Epoch 151/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3035 - val_loss: 1.8732\n",
      "Epoch 152/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2866 - val_loss: 1.8671\n",
      "Epoch 153/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.2902 - val_loss: 1.8381\n",
      "Epoch 154/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.2652 - val_loss: 1.8691\n",
      "Epoch 155/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.2596 - val_loss: 1.8609\n",
      "Epoch 156/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.3051 - val_loss: 1.8866\n",
      "Epoch 157/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.2738 - val_loss: 1.8984\n",
      "Epoch 158/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.2506 - val_loss: 1.8760\n",
      "Epoch 159/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.2673 - val_loss: 1.8485\n",
      "Epoch 160/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.2452 - val_loss: 1.8829\n",
      "Epoch 161/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.2696 - val_loss: 1.8864\n",
      "Epoch 162/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.2571 - val_loss: 1.9255\n",
      "Epoch 163/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.2574 - val_loss: 1.9388\n",
      "Epoch 164/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 1.2359 - val_loss: 1.9649\n",
      "Epoch 165/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.2575 - val_loss: 1.8984\n",
      "Epoch 166/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2657 - val_loss: 1.9428\n",
      "Epoch 167/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.2541 - val_loss: 1.8919\n",
      "Epoch 168/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2661 - val_loss: 1.8913\n",
      "Epoch 169/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2297 - val_loss: 1.9309\n",
      "Epoch 170/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.2712 - val_loss: 1.8916\n",
      "Epoch 171/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3140 - val_loss: 1.8769\n",
      "Epoch 172/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3083 - val_loss: 1.9045\n",
      "Epoch 173/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2681 - val_loss: 1.9373\n",
      "Epoch 174/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2882 - val_loss: 1.9314\n",
      "Epoch 175/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2529 - val_loss: 1.9409\n",
      "Epoch 176/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2413 - val_loss: 1.9503\n",
      "Epoch 177/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2061 - val_loss: 1.9590\n",
      "Epoch 178/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2296 - val_loss: 1.9575\n",
      "Epoch 179/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.2061 - val_loss: 1.9801\n",
      "Epoch 180/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.1939 - val_loss: 2.0049\n",
      "Epoch 181/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.1629 - val_loss: 1.9757\n",
      "Epoch 182/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.1989 - val_loss: 1.9738\n",
      "Epoch 183/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.1864 - val_loss: 2.0057\n",
      "Epoch 184/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.1687 - val_loss: 1.9863\n",
      "Epoch 185/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.1532 - val_loss: 2.0021\n",
      "Epoch 186/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.1328 - val_loss: 1.9882\n",
      "Epoch 187/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.1388 - val_loss: 2.0519\n",
      "Epoch 188/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.1851 - val_loss: 1.9783\n",
      "Epoch 189/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2966 - val_loss: 1.8978\n",
      "Epoch 190/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.2805 - val_loss: 1.9301\n",
      "Epoch 191/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.3993 - val_loss: 1.9230\n",
      "Epoch 192/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.4482 - val_loss: 1.8797\n",
      "Epoch 193/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 1.4320 - val_loss: 1.8694\n",
      "Epoch 194/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3524 - val_loss: 1.8749\n",
      "Epoch 195/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3216 - val_loss: 1.8875\n",
      "Epoch 196/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2587 - val_loss: 1.8853\n",
      "Epoch 197/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2329 - val_loss: 1.8882\n",
      "Epoch 198/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2199 - val_loss: 1.8748\n",
      "Epoch 199/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1956 - val_loss: 1.9079\n",
      "Epoch 200/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.2056 - val_loss: 1.8886\n",
      "Epoch 201/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1820 - val_loss: 1.9488\n",
      "Epoch 202/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1666 - val_loss: 1.9992\n",
      "Epoch 203/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.3135 - val_loss: 1.9279\n",
      "Epoch 204/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 1.3974 - val_loss: 1.8615\n",
      "Epoch 205/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.4007 - val_loss: 1.8247\n",
      "Epoch 206/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.3911 - val_loss: 1.8319\n",
      "Epoch 207/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.3923 - val_loss: 1.8259\n",
      "Epoch 208/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.3188 - val_loss: 1.8397\n",
      "Epoch 209/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.3146 - val_loss: 1.8440\n",
      "Epoch 210/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.2795 - val_loss: 1.8637\n",
      "Epoch 211/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.2578 - val_loss: 1.8866\n",
      "Epoch 212/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2559 - val_loss: 1.8916\n",
      "Epoch 213/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.2203 - val_loss: 1.8862\n",
      "Epoch 214/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.1999 - val_loss: 1.9104\n",
      "Epoch 215/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.1854 - val_loss: 1.8996\n",
      "Epoch 216/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.1793 - val_loss: 1.9129\n",
      "Epoch 217/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.1721 - val_loss: 1.9296\n",
      "Epoch 218/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.1884 - val_loss: 1.9377\n",
      "Epoch 219/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.1651 - val_loss: 1.9598\n",
      "Epoch 220/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.1698 - val_loss: 1.9256\n",
      "Epoch 221/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.2107 - val_loss: 1.8989\n",
      "Epoch 222/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.1689 - val_loss: 1.8830\n",
      "Epoch 223/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.1427 - val_loss: 1.9069\n",
      "Epoch 224/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.1741 - val_loss: 1.9067\n",
      "Epoch 225/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.1316 - val_loss: 1.9198\n",
      "Epoch 226/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.0982 - val_loss: 1.9199\n",
      "Epoch 227/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.1068 - val_loss: 1.9182\n",
      "Epoch 228/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.0892 - val_loss: 1.9298\n",
      "Epoch 229/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.0789 - val_loss: 1.9462\n",
      "Epoch 230/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.0374 - val_loss: 1.9636\n",
      "Epoch 231/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.0144 - val_loss: 1.9745\n",
      "Epoch 232/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.0275 - val_loss: 1.9807\n",
      "Epoch 233/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.0012 - val_loss: 1.9937\n",
      "Epoch 234/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.0233 - val_loss: 2.0059\n",
      "Epoch 235/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9981 - val_loss: 2.0255\n",
      "Epoch 236/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.0078 - val_loss: 2.0301\n",
      "Epoch 237/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.9773 - val_loss: 2.0481\n",
      "Epoch 238/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.9801 - val_loss: 2.0606\n",
      "Epoch 239/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.9587 - val_loss: 2.0821\n",
      "Epoch 240/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.9832 - val_loss: 2.0741\n",
      "Epoch 241/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.9702 - val_loss: 2.0814\n",
      "Epoch 242/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.9726 - val_loss: 2.0653\n",
      "Epoch 243/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.9659 - val_loss: 2.0694\n",
      "Epoch 244/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.9587 - val_loss: 2.0730\n",
      "Epoch 245/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.9657 - val_loss: 2.1164\n",
      "Epoch 246/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.9518 - val_loss: 2.0926\n",
      "Epoch 247/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.9390 - val_loss: 2.1193\n",
      "Epoch 248/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 48ms/step - loss: 0.9433 - val_loss: 2.1034\n",
      "Epoch 249/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.9485 - val_loss: 2.1502\n",
      "Epoch 250/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.9922 - val_loss: 2.1399\n",
      "Epoch 251/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.0013 - val_loss: 2.1125\n",
      "Epoch 252/500\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 0.9950 - val_loss: 2.1214\n",
      "Epoch 253/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.9798 - val_loss: 2.1274\n",
      "Epoch 254/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.9633 - val_loss: 2.1284\n",
      "Epoch 255/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.9559 - val_loss: 2.1187\n",
      "Epoch 256/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.9507 - val_loss: 2.1325\n",
      "Epoch 257/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.9359 - val_loss: 2.1363\n",
      "Epoch 258/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.9677 - val_loss: 2.1761\n",
      "Epoch 259/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.9705 - val_loss: 2.1849\n",
      "Epoch 260/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.9861 - val_loss: 2.1776\n",
      "Epoch 261/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.9817 - val_loss: 2.1719\n",
      "Epoch 262/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.9472 - val_loss: 2.1802\n",
      "Epoch 263/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.0014 - val_loss: 2.1678\n",
      "Epoch 264/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.9486 - val_loss: 2.2060\n",
      "Epoch 265/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.9285 - val_loss: 2.1748\n",
      "Epoch 266/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.9166 - val_loss: 2.2079\n",
      "Epoch 267/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.9275 - val_loss: 2.1865\n",
      "Epoch 268/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.8904 - val_loss: 2.2067\n",
      "Epoch 269/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.9037 - val_loss: 2.2380\n",
      "Epoch 270/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.8859 - val_loss: 2.2250\n",
      "Epoch 271/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.8677 - val_loss: 2.2636\n",
      "Epoch 272/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.8540 - val_loss: 2.2043\n",
      "Epoch 273/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.8532 - val_loss: 2.2170\n",
      "Epoch 274/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.9078 - val_loss: 2.1902\n",
      "Epoch 275/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.9002 - val_loss: 2.2406\n",
      "Epoch 276/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.9115 - val_loss: 2.2152\n",
      "Epoch 277/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.9200 - val_loss: 2.2292\n",
      "Epoch 278/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.8805 - val_loss: 2.2995\n",
      "Epoch 279/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.9055 - val_loss: 2.2234\n",
      "Epoch 280/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.9091 - val_loss: 2.2304\n",
      "Epoch 281/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.9259 - val_loss: 2.2526\n",
      "Epoch 282/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.9326 - val_loss: 2.2528\n",
      "Epoch 283/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.8964 - val_loss: 2.2797\n",
      "Epoch 284/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.9055 - val_loss: 2.2507\n",
      "Epoch 285/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.9132 - val_loss: 2.2322\n",
      "Epoch 286/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.0408 - val_loss: 2.1433\n",
      "Epoch 287/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.1061 - val_loss: 2.1268\n",
      "Epoch 288/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.0642 - val_loss: 2.0839\n",
      "Epoch 289/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.0392 - val_loss: 2.0725\n",
      "Epoch 290/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.0450 - val_loss: 2.0748\n",
      "Epoch 291/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.9984 - val_loss: 2.0954\n",
      "Epoch 292/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.9787 - val_loss: 2.1292\n",
      "Epoch 293/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.9763 - val_loss: 2.0996\n",
      "Epoch 294/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.9454 - val_loss: 2.1047\n",
      "Epoch 295/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.9697 - val_loss: 2.1140\n",
      "Epoch 296/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.9455 - val_loss: 2.1149\n",
      "Epoch 297/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.9259 - val_loss: 2.1163\n",
      "Epoch 298/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.0834 - val_loss: 2.1328\n",
      "Epoch 299/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.2609 - val_loss: 2.0685\n",
      "Epoch 300/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.1533 - val_loss: 2.0705\n",
      "Epoch 301/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.1590 - val_loss: 2.1486\n",
      "Epoch 302/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4112 - val_loss: 2.1807\n",
      "Epoch 303/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.4192 - val_loss: 2.0918\n",
      "Epoch 304/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.3785 - val_loss: 2.0551\n",
      "Epoch 305/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.3141 - val_loss: 2.0184\n",
      "Epoch 306/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.2851 - val_loss: 1.9929\n",
      "Epoch 307/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.2291 - val_loss: 1.9947\n",
      "Epoch 308/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.1929 - val_loss: 2.0178\n",
      "Epoch 309/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.1606 - val_loss: 2.0067\n",
      "Epoch 310/500\n",
      "6/6 [==============================] - 0s 90ms/step - loss: 1.1308 - val_loss: 2.0359\n",
      "Epoch 311/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1291 - val_loss: 2.0740\n",
      "Epoch 312/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.0841 - val_loss: 2.0666\n",
      "Epoch 313/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.0231 - val_loss: 2.0333\n",
      "Epoch 314/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.0074 - val_loss: 2.0418\n",
      "Epoch 315/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.0073 - val_loss: 2.0592\n",
      "Epoch 316/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.0334 - val_loss: 2.0808\n",
      "Epoch 317/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.0150 - val_loss: 2.0642\n",
      "Epoch 318/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.9857 - val_loss: 2.0826\n",
      "Epoch 319/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.9743 - val_loss: 2.0986\n",
      "Epoch 320/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.9603 - val_loss: 2.1165\n",
      "Epoch 321/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.9346 - val_loss: 2.1085\n",
      "Epoch 322/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.9337 - val_loss: 2.1148\n",
      "Epoch 323/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.9360 - val_loss: 2.1098\n",
      "Epoch 324/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.9382 - val_loss: 2.1094\n",
      "Epoch 325/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.9207 - val_loss: 2.1266\n",
      "Epoch 326/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.8904 - val_loss: 2.1376\n",
      "Epoch 327/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.9167 - val_loss: 2.1463\n",
      "Epoch 328/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.8747 - val_loss: 2.1478\n",
      "Epoch 329/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.8686 - val_loss: 2.1673\n",
      "Epoch 330/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 45ms/step - loss: 0.8606 - val_loss: 2.1616\n",
      "Epoch 331/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.8677 - val_loss: 2.1775\n",
      "Epoch 332/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.8767 - val_loss: 2.1786\n",
      "Epoch 333/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.9150 - val_loss: 2.1613\n",
      "Epoch 334/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.9261 - val_loss: 2.1727\n",
      "Epoch 335/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.9504 - val_loss: 2.1442\n",
      "Epoch 336/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.9838 - val_loss: 2.1991\n",
      "Epoch 337/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.0462 - val_loss: 2.1350\n",
      "Epoch 338/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.0093 - val_loss: 2.1191\n",
      "Epoch 339/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.9768 - val_loss: 2.1615\n",
      "Epoch 340/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.0258 - val_loss: 2.1223\n",
      "Epoch 341/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 1.0130 - val_loss: 2.1177\n",
      "Epoch 342/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.9664 - val_loss: 2.1068\n",
      "Epoch 343/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.9467 - val_loss: 2.1143\n",
      "Epoch 344/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.9635 - val_loss: 2.1308\n",
      "Epoch 345/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.9280 - val_loss: 2.1247\n",
      "Epoch 346/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.9270 - val_loss: 2.1325\n",
      "Epoch 347/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.9147 - val_loss: 2.1365\n",
      "Epoch 348/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.8998 - val_loss: 2.1660\n",
      "Epoch 349/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.9166 - val_loss: 2.1846\n",
      "Epoch 350/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.8866 - val_loss: 2.2071\n",
      "Epoch 351/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8647 - val_loss: 2.1967\n",
      "Epoch 352/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.8585 - val_loss: 2.2469\n",
      "Epoch 353/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.8494 - val_loss: 2.2265\n",
      "Epoch 354/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.8238 - val_loss: 2.2501\n",
      "Epoch 355/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.8343 - val_loss: 2.2437\n",
      "Epoch 356/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.8198 - val_loss: 2.2766\n",
      "Epoch 357/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.8017 - val_loss: 2.2707\n",
      "Epoch 358/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.8576 - val_loss: 2.2681\n",
      "Epoch 359/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.9024 - val_loss: 2.2469\n",
      "Epoch 360/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.9164 - val_loss: 2.2473\n",
      "Epoch 361/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.8816 - val_loss: 2.2284\n",
      "Epoch 362/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.8753 - val_loss: 2.2226\n",
      "Epoch 363/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.8550 - val_loss: 2.2428\n",
      "Epoch 364/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.8634 - val_loss: 2.2517\n",
      "Epoch 365/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.9453 - val_loss: 2.2555\n",
      "Epoch 366/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.9147 - val_loss: 2.2456\n",
      "Epoch 367/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.9298 - val_loss: 2.2551\n",
      "Epoch 368/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.8870 - val_loss: 2.2999\n",
      "Epoch 369/500\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.8642 - val_loss: 2.2996\n",
      "Epoch 370/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.8748 - val_loss: 2.2728\n",
      "Epoch 371/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.8372 - val_loss: 2.3067\n",
      "Epoch 372/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.8457 - val_loss: 2.2833\n",
      "Epoch 373/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.8270 - val_loss: 2.3169\n",
      "Epoch 374/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.8116 - val_loss: 2.2941\n",
      "Epoch 375/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8030 - val_loss: 2.3409\n",
      "Epoch 376/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8000 - val_loss: 2.3332\n",
      "Epoch 377/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.8033 - val_loss: 2.3281\n",
      "Epoch 378/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.7855 - val_loss: 2.3434\n",
      "Epoch 379/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.7874 - val_loss: 2.3337\n",
      "Epoch 380/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.7679 - val_loss: 2.3447\n",
      "Epoch 381/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.8013 - val_loss: 2.3690\n",
      "Epoch 382/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.8131 - val_loss: 2.3598\n",
      "Epoch 383/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.8243 - val_loss: 2.3532\n",
      "Epoch 384/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.7942 - val_loss: 2.3639\n",
      "Epoch 385/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.7887 - val_loss: 2.3642\n",
      "Epoch 386/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.7893 - val_loss: 2.3595\n",
      "Epoch 387/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.8042 - val_loss: 2.4022\n",
      "Epoch 388/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.7781 - val_loss: 2.4170\n",
      "Epoch 389/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.7967 - val_loss: 2.4300\n",
      "Epoch 390/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.7705 - val_loss: 2.4424\n",
      "Epoch 391/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.7670 - val_loss: 2.4529\n",
      "Epoch 392/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.7572 - val_loss: 2.4750\n",
      "Epoch 393/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.7517 - val_loss: 2.4852\n",
      "Epoch 394/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.7447 - val_loss: 2.4456\n",
      "Epoch 395/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.7421 - val_loss: 2.4582\n",
      "Epoch 396/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.7422 - val_loss: 2.4634\n",
      "Epoch 397/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.7344 - val_loss: 2.4775\n",
      "Epoch 398/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.7556 - val_loss: 2.4640\n",
      "Epoch 399/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.7547 - val_loss: 2.4920\n",
      "Epoch 400/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.7686 - val_loss: 2.4843\n",
      "Epoch 401/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.7384 - val_loss: 2.4872\n",
      "Epoch 402/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.7216 - val_loss: 2.5113\n",
      "Epoch 403/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.7318 - val_loss: 2.4943\n",
      "Epoch 404/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.7611 - val_loss: 2.5034\n",
      "Epoch 405/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.7106 - val_loss: 2.5034\n",
      "Epoch 406/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.7374 - val_loss: 2.5362\n",
      "Epoch 407/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.7240 - val_loss: 2.5142\n",
      "Epoch 408/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.7467 - val_loss: 2.5423\n",
      "Epoch 409/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.7135 - val_loss: 2.5371\n",
      "Epoch 410/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.7038 - val_loss: 2.5745\n",
      "Epoch 411/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.7298 - val_loss: 2.5648\n",
      "Epoch 412/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 33ms/step - loss: 0.7297 - val_loss: 2.5695\n",
      "Epoch 413/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.7334 - val_loss: 2.5619\n",
      "Epoch 414/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7703 - val_loss: 2.5377\n",
      "Epoch 415/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7481 - val_loss: 2.5467\n",
      "Epoch 416/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7464 - val_loss: 2.5129\n",
      "Epoch 417/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.7324 - val_loss: 2.5385\n",
      "Epoch 418/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7445 - val_loss: 2.5375\n",
      "Epoch 419/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.7066 - val_loss: 2.5473\n",
      "Epoch 420/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.7034 - val_loss: 2.5543\n",
      "Epoch 421/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.7142 - val_loss: 2.5585\n",
      "Epoch 422/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.7305 - val_loss: 2.5754\n",
      "Epoch 423/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.7265 - val_loss: 2.5598\n",
      "Epoch 424/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.7212 - val_loss: 2.5789\n",
      "Epoch 425/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.7220 - val_loss: 2.5512\n",
      "Epoch 426/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.7141 - val_loss: 2.5762\n",
      "Epoch 427/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.7285 - val_loss: 2.5713\n",
      "Epoch 428/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7226 - val_loss: 2.5509\n",
      "Epoch 429/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7298 - val_loss: 2.5842\n",
      "Epoch 430/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7026 - val_loss: 2.5722\n",
      "Epoch 431/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.7027 - val_loss: 2.6064\n",
      "Epoch 432/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.7019 - val_loss: 2.5910\n",
      "Epoch 433/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.6990 - val_loss: 2.5896\n",
      "Epoch 434/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.6971 - val_loss: 2.5919\n",
      "Epoch 435/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.7147 - val_loss: 2.5943\n",
      "Epoch 436/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.6914 - val_loss: 2.5783\n",
      "Epoch 437/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.6933 - val_loss: 2.6005\n",
      "Epoch 438/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.6951 - val_loss: 2.5658\n",
      "Epoch 439/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6941 - val_loss: 2.6390\n",
      "Epoch 440/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7204 - val_loss: 2.6172\n",
      "Epoch 441/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.6884 - val_loss: 2.6309\n",
      "Epoch 442/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.7134 - val_loss: 2.6731\n",
      "Epoch 443/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.7430 - val_loss: 2.6485\n",
      "Epoch 444/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.7258 - val_loss: 2.6826\n",
      "Epoch 445/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.7081 - val_loss: 2.6457\n",
      "Epoch 446/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.7201 - val_loss: 2.6489\n",
      "Epoch 447/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.6975 - val_loss: 2.6349\n",
      "Epoch 448/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.7043 - val_loss: 2.5871\n",
      "Epoch 449/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.7098 - val_loss: 2.6260\n",
      "Epoch 450/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.6887 - val_loss: 2.5923\n",
      "Epoch 451/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.6896 - val_loss: 2.5967\n",
      "Epoch 452/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.6878 - val_loss: 2.6573\n",
      "Epoch 453/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.6735 - val_loss: 2.6557\n",
      "Epoch 454/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.6923 - val_loss: 2.6526\n",
      "Epoch 455/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.6819 - val_loss: 2.6559\n",
      "Epoch 456/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.7009 - val_loss: 2.6883\n",
      "Epoch 457/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.6744 - val_loss: 2.6860\n",
      "Epoch 458/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.6749 - val_loss: 2.7227\n",
      "Epoch 459/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.6670 - val_loss: 2.7012\n",
      "Epoch 460/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.6987 - val_loss: 2.7447\n",
      "Epoch 461/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7103 - val_loss: 2.7073\n",
      "Epoch 462/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.6868 - val_loss: 2.7413\n",
      "Epoch 463/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.6834 - val_loss: 2.6782\n",
      "Epoch 464/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.6911 - val_loss: 2.7352\n",
      "Epoch 465/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.6708 - val_loss: 2.6955\n",
      "Epoch 466/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.6757 - val_loss: 2.6981\n",
      "Epoch 467/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6753 - val_loss: 2.6933\n",
      "Epoch 468/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6765 - val_loss: 2.6712\n",
      "Epoch 469/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.6812 - val_loss: 2.7119\n",
      "Epoch 470/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.6669 - val_loss: 2.7019\n",
      "Epoch 471/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.6625 - val_loss: 2.7183\n",
      "Epoch 472/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6577 - val_loss: 2.7221\n",
      "Epoch 473/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6698 - val_loss: 2.7561\n",
      "Epoch 474/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6790 - val_loss: 2.7219\n",
      "Epoch 475/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6755 - val_loss: 2.7657\n",
      "Epoch 476/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.6909 - val_loss: 2.7474\n",
      "Epoch 477/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6825 - val_loss: 2.7623\n",
      "Epoch 478/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6719 - val_loss: 2.7237\n",
      "Epoch 479/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.7059 - val_loss: 2.7371\n",
      "Epoch 480/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.7105 - val_loss: 2.7193\n",
      "Epoch 481/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.6912 - val_loss: 2.7090\n",
      "Epoch 482/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.6943 - val_loss: 2.7171\n",
      "Epoch 483/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.6823 - val_loss: 2.7544\n",
      "Epoch 484/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.6981 - val_loss: 2.7078\n",
      "Epoch 485/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.6938 - val_loss: 2.7427\n",
      "Epoch 486/500\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.7463 - val_loss: 2.7120\n",
      "Epoch 487/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.7591 - val_loss: 2.7155\n",
      "Epoch 488/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7475 - val_loss: 2.6688\n",
      "Epoch 489/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.7322 - val_loss: 2.6532\n",
      "Epoch 490/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7201 - val_loss: 2.6874\n",
      "Epoch 491/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.7284 - val_loss: 2.6493\n",
      "Epoch 492/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.7250 - val_loss: 2.7114\n",
      "Epoch 493/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6991 - val_loss: 2.7049\n",
      "Epoch 494/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 38ms/step - loss: 0.6976 - val_loss: 2.7404\n",
      "Epoch 495/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.7024 - val_loss: 2.7311\n",
      "Epoch 496/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.6888 - val_loss: 2.7324\n",
      "Epoch 497/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.6932 - val_loss: 2.7652\n",
      "Epoch 498/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.6800 - val_loss: 2.7680\n",
      "Epoch 499/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.6738 - val_loss: 2.7907\n",
      "Epoch 500/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.6708 - val_loss: 2.7876\n"
     ]
    }
   ],
   "source": [
    "hist = sequence_autoencoder.fit(X_train, y_train,\n",
    "                epochs=500,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder'), mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.plot(hist.history['loss'], label='training')\n",
    "plt.plot(hist.history['val_loss'], label='testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'figures/{name}', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_autoencoder = load_model(f'Models/{name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6993599397590361"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sequence_autoencoder.predict(X_train).argmax(axis=-1)\n",
    "accuracy_score(y_train.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44642857142857145"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sequence_autoencoder.predict(X_test).argmax(axis=-1)\n",
    "accuracy_score(y_test.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 Embedding-GRU Encoder GRU-Dense Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NBG_gru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 64  # Length of your sequences\n",
    "embed_size = 32\n",
    "latent_dim = 256\n",
    "dropout = 0\n",
    "\n",
    "inputs = Input(shape=(timesteps,))\n",
    "embedded = Embedding(vocab_size, embed_size)(inputs)\n",
    "#encoded = LSTM(latent_dim, return_sequences=True, dropout=dropout)(embedded)\n",
    "encoded = GRU(latent_dim, dropout=dropout)(embedded)\n",
    "\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "#decoded = LSTM(latent_dim, return_sequences=True, dropout=dropout)(decoded)\n",
    "decoded = GRU(latent_dim, return_sequences=True, dropout=dropout)(decoded)\n",
    "decoded = Dense(vocab_size, activation='softmax')(decoded)\n",
    "#decoded = argmax(decoded, axis=-1)\n",
    "#decoded = cast(decoded, float)\n",
    "#decoded = Reshape((decoded.shape[1], -1))(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 64, 32)            1152      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 256)               222720    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64, 256)           394752    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64, 36)            9252      \n",
      "=================================================================\n",
      "Total params: 627,876\n",
      "Trainable params: 627,876\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs, encoded)\n",
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(latent_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layers = sequence_autoencoder.layers[-3:]\n",
    "decoded_input = decoder_layers[0](encoded_input)\n",
    "for decoder_layer in decoder_layers[1:]:\n",
    "    decoded_input = decoder_layer(decoded_input)\n",
    "# Create the decoder model\n",
    "decoder = Model(encoded_input, decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 64, 32)            1152      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 256)               222720    \n",
      "=================================================================\n",
      "Total params: 223,872\n",
      "Trainable params: 223,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 256)]             0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64, 256)           394752    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64, 36)            9252      \n",
      "=================================================================\n",
      "Total params: 404,004\n",
      "Trainable params: 404,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "\n",
    "mc = ModelCheckpoint(f'Models/{name}.hdf5', monitor='val_loss')\n",
    "\n",
    "optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "sequence_autoencoder.compile(optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "6/6 [==============================] - 4s 239ms/step - loss: 3.6563 - val_loss: 2.3175\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.4409 - val_loss: 1.9796\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0341 - val_loss: 1.9585\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0312 - val_loss: 1.9442\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 2.0234 - val_loss: 1.9470\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.9915 - val_loss: 1.9094\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.9582 - val_loss: 1.9307\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0229 - val_loss: 1.8891\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9923 - val_loss: 1.9212\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9434 - val_loss: 1.8720\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9387 - val_loss: 1.8568\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.8945 - val_loss: 1.8560\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.8643 - val_loss: 1.8090\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.8578 - val_loss: 1.7958\n",
      "Epoch 15/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.8130 - val_loss: 1.7847\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.7586 - val_loss: 1.7400\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.7357 - val_loss: 1.7498\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.7414 - val_loss: 1.7400\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.7316 - val_loss: 1.7272\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.7310 - val_loss: 1.6886\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.6662 - val_loss: 1.6901\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.6847 - val_loss: 1.6705\n",
      "Epoch 23/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.6649 - val_loss: 1.6686\n",
      "Epoch 24/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.6569 - val_loss: 1.6617\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.6039 - val_loss: 1.6582\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.5997 - val_loss: 1.6472\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.6018 - val_loss: 1.6497\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.5736 - val_loss: 1.6370\n",
      "Epoch 29/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.5877 - val_loss: 1.6307\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.5807 - val_loss: 1.6424\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.5594 - val_loss: 1.6359\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.5503 - val_loss: 1.6443\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.5698 - val_loss: 1.6311\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.5359 - val_loss: 1.6204\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.5449 - val_loss: 1.6199\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.5323 - val_loss: 1.6213\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 1.5182 - val_loss: 1.6253\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.5424 - val_loss: 1.6198\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.5067 - val_loss: 1.6167\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.5169 - val_loss: 1.6246\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.5085 - val_loss: 1.6216\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.5069 - val_loss: 1.6275\n",
      "Epoch 43/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.4836 - val_loss: 1.6165\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.4746 - val_loss: 1.6253\n",
      "Epoch 45/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.4700 - val_loss: 1.6317\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4765 - val_loss: 1.6425\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.4700 - val_loss: 1.6230\n",
      "Epoch 48/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.4785 - val_loss: 1.6262\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.4798 - val_loss: 1.6458\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.4629 - val_loss: 1.6290\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.4611 - val_loss: 1.6391\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.4581 - val_loss: 1.6113\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.4527 - val_loss: 1.6306\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.4627 - val_loss: 1.6187\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.4395 - val_loss: 1.6248\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.4488 - val_loss: 1.6134\n",
      "Epoch 57/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.4497 - val_loss: 1.6291\n",
      "Epoch 58/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4577 - val_loss: 1.6397\n",
      "Epoch 59/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4414 - val_loss: 1.6380\n",
      "Epoch 60/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4326 - val_loss: 1.6358\n",
      "Epoch 61/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.4363 - val_loss: 1.6480\n",
      "Epoch 62/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.4584 - val_loss: 1.6297\n",
      "Epoch 63/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.4295 - val_loss: 1.6496\n",
      "Epoch 64/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.4363 - val_loss: 1.6228\n",
      "Epoch 65/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.4106 - val_loss: 1.6461\n",
      "Epoch 66/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.4217 - val_loss: 1.6315\n",
      "Epoch 67/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.4164 - val_loss: 1.6451\n",
      "Epoch 68/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.4000 - val_loss: 1.6260\n",
      "Epoch 69/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3983 - val_loss: 1.6444\n",
      "Epoch 70/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.4036 - val_loss: 1.6358\n",
      "Epoch 71/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.4210 - val_loss: 1.6391\n",
      "Epoch 72/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.4009 - val_loss: 1.6340\n",
      "Epoch 73/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3908 - val_loss: 1.6439\n",
      "Epoch 74/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.4130 - val_loss: 1.6411\n",
      "Epoch 75/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.4008 - val_loss: 1.6602\n",
      "Epoch 76/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3985 - val_loss: 1.6500\n",
      "Epoch 77/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.4141 - val_loss: 1.6506\n",
      "Epoch 78/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.3942 - val_loss: 1.6587\n",
      "Epoch 79/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3800 - val_loss: 1.6700\n",
      "Epoch 80/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3869 - val_loss: 1.6652\n",
      "Epoch 81/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.4096 - val_loss: 1.6752\n",
      "Epoch 82/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.4068 - val_loss: 1.6562\n",
      "Epoch 83/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 1.3891 - val_loss: 1.6596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.3951 - val_loss: 1.6911\n",
      "Epoch 85/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.4121 - val_loss: 1.6623\n",
      "Epoch 86/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4007 - val_loss: 1.6780\n",
      "Epoch 87/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3978 - val_loss: 1.6546\n",
      "Epoch 88/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4031 - val_loss: 1.6619\n",
      "Epoch 89/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.3913 - val_loss: 1.6586\n",
      "Epoch 90/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3858 - val_loss: 1.6715\n",
      "Epoch 91/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3907 - val_loss: 1.6818\n",
      "Epoch 92/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3794 - val_loss: 1.6786\n",
      "Epoch 93/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3841 - val_loss: 1.6632\n",
      "Epoch 94/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3926 - val_loss: 1.6715\n",
      "Epoch 95/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.3925 - val_loss: 1.6818\n",
      "Epoch 96/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.4130 - val_loss: 1.6738\n",
      "Epoch 97/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3673 - val_loss: 1.6880\n",
      "Epoch 98/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3813 - val_loss: 1.6680\n",
      "Epoch 99/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.4016 - val_loss: 1.6851\n",
      "Epoch 100/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3689 - val_loss: 1.6946\n",
      "Epoch 101/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3771 - val_loss: 1.6840\n",
      "Epoch 102/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3641 - val_loss: 1.6972\n",
      "Epoch 103/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3733 - val_loss: 1.6752\n",
      "Epoch 104/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3644 - val_loss: 1.6899\n",
      "Epoch 105/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.4116 - val_loss: 1.6856\n",
      "Epoch 106/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3437 - val_loss: 1.6719\n",
      "Epoch 107/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3443 - val_loss: 1.6967\n",
      "Epoch 108/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3698 - val_loss: 1.7169\n",
      "Epoch 109/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3747 - val_loss: 1.7071\n",
      "Epoch 110/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.3943 - val_loss: 1.7168\n",
      "Epoch 111/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3821 - val_loss: 1.7113\n",
      "Epoch 112/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3987 - val_loss: 1.7671\n",
      "Epoch 113/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3732 - val_loss: 1.7160\n",
      "Epoch 114/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3960 - val_loss: 1.7098\n",
      "Epoch 115/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3650 - val_loss: 1.7332\n",
      "Epoch 116/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3830 - val_loss: 1.7614\n",
      "Epoch 117/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3668 - val_loss: 1.7415\n",
      "Epoch 118/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3661 - val_loss: 1.7135\n",
      "Epoch 119/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3745 - val_loss: 1.7048\n",
      "Epoch 120/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3867 - val_loss: 1.7181\n",
      "Epoch 121/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3658 - val_loss: 1.7125\n",
      "Epoch 122/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3544 - val_loss: 1.7329\n",
      "Epoch 123/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3820 - val_loss: 1.7182\n",
      "Epoch 124/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.4131 - val_loss: 1.7086\n",
      "Epoch 125/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.4121 - val_loss: 1.7221\n",
      "Epoch 126/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3837 - val_loss: 1.7149\n",
      "Epoch 127/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3728 - val_loss: 1.7384\n",
      "Epoch 128/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3859 - val_loss: 1.7300\n",
      "Epoch 129/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3905 - val_loss: 1.7110\n",
      "Epoch 130/500\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 1.3429 - val_loss: 1.7229\n",
      "Epoch 131/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3435 - val_loss: 1.7071\n",
      "Epoch 132/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3551 - val_loss: 1.7340\n",
      "Epoch 133/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.3547 - val_loss: 1.7106\n",
      "Epoch 134/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3587 - val_loss: 1.7330\n",
      "Epoch 135/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3443 - val_loss: 1.7162\n",
      "Epoch 136/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3298 - val_loss: 1.7284\n",
      "Epoch 137/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.3245 - val_loss: 1.7256\n",
      "Epoch 138/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.3184 - val_loss: 1.7112\n",
      "Epoch 139/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3256 - val_loss: 1.7366\n",
      "Epoch 140/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.3495 - val_loss: 1.7292\n",
      "Epoch 141/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.3461 - val_loss: 1.7364\n",
      "Epoch 142/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.3456 - val_loss: 1.7328\n",
      "Epoch 143/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.3356 - val_loss: 1.7466\n",
      "Epoch 144/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3333 - val_loss: 1.7496\n",
      "Epoch 145/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3204 - val_loss: 1.7536\n",
      "Epoch 146/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3220 - val_loss: 1.7563\n",
      "Epoch 147/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3486 - val_loss: 1.7431\n",
      "Epoch 148/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.3495 - val_loss: 1.7488\n",
      "Epoch 149/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3205 - val_loss: 1.7426\n",
      "Epoch 150/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3462 - val_loss: 1.7372\n",
      "Epoch 151/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3619 - val_loss: 1.7556\n",
      "Epoch 152/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3680 - val_loss: 1.7588\n",
      "Epoch 153/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3502 - val_loss: 1.7504\n",
      "Epoch 154/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3349 - val_loss: 1.7755\n",
      "Epoch 155/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3346 - val_loss: 1.7664\n",
      "Epoch 156/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3332 - val_loss: 1.7812\n",
      "Epoch 157/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3459 - val_loss: 1.7625\n",
      "Epoch 158/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3402 - val_loss: 1.7775\n",
      "Epoch 159/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3456 - val_loss: 1.7885\n",
      "Epoch 160/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3288 - val_loss: 1.7574\n",
      "Epoch 161/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3329 - val_loss: 1.7308\n",
      "Epoch 162/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.3565 - val_loss: 1.7645\n",
      "Epoch 163/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3395 - val_loss: 1.7463\n",
      "Epoch 164/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.3341 - val_loss: 1.7483\n",
      "Epoch 165/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3283 - val_loss: 1.7727\n",
      "Epoch 166/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3013 - val_loss: 1.7595\n",
      "Epoch 167/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3252 - val_loss: 1.7419\n",
      "Epoch 168/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2983 - val_loss: 1.7539\n",
      "Epoch 169/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3279 - val_loss: 1.7716\n",
      "Epoch 170/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3171 - val_loss: 1.7691\n",
      "Epoch 171/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3152 - val_loss: 1.7778\n",
      "Epoch 172/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3221 - val_loss: 1.7914\n",
      "Epoch 173/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3121 - val_loss: 1.8019\n",
      "Epoch 174/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3088 - val_loss: 1.8066\n",
      "Epoch 175/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3081 - val_loss: 1.7648\n",
      "Epoch 176/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 1.3017 - val_loss: 1.7807\n",
      "Epoch 177/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3224 - val_loss: 1.7613\n",
      "Epoch 178/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3188 - val_loss: 1.7758\n",
      "Epoch 179/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2933 - val_loss: 1.7770\n",
      "Epoch 180/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2994 - val_loss: 1.7992\n",
      "Epoch 181/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.3108 - val_loss: 1.7832\n",
      "Epoch 182/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2930 - val_loss: 1.7956\n",
      "Epoch 183/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2893 - val_loss: 1.7979\n",
      "Epoch 184/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2901 - val_loss: 1.7965\n",
      "Epoch 185/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2828 - val_loss: 1.8209\n",
      "Epoch 186/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.2576 - val_loss: 1.8375\n",
      "Epoch 187/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2907 - val_loss: 1.8159\n",
      "Epoch 188/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2863 - val_loss: 1.7763\n",
      "Epoch 189/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2728 - val_loss: 1.7888\n",
      "Epoch 190/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2684 - val_loss: 1.7963\n",
      "Epoch 191/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2909 - val_loss: 1.8016\n",
      "Epoch 192/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2581 - val_loss: 1.7934\n",
      "Epoch 193/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2615 - val_loss: 1.8145\n",
      "Epoch 194/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2749 - val_loss: 1.8182\n",
      "Epoch 195/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2723 - val_loss: 1.7841\n",
      "Epoch 196/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2421 - val_loss: 1.8013\n",
      "Epoch 197/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2709 - val_loss: 1.7939\n",
      "Epoch 198/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2504 - val_loss: 1.8019\n",
      "Epoch 199/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2494 - val_loss: 1.8118\n",
      "Epoch 200/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.2418 - val_loss: 1.7992\n",
      "Epoch 201/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.2348 - val_loss: 1.8244\n",
      "Epoch 202/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.2436 - val_loss: 1.8178\n",
      "Epoch 203/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2297 - val_loss: 1.8648\n",
      "Epoch 204/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.2320 - val_loss: 1.8561\n",
      "Epoch 205/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2407 - val_loss: 1.8633\n",
      "Epoch 206/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2356 - val_loss: 1.8622\n",
      "Epoch 207/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2088 - val_loss: 1.8735\n",
      "Epoch 208/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2280 - val_loss: 1.9040\n",
      "Epoch 209/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.2172 - val_loss: 1.8656\n",
      "Epoch 210/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2515 - val_loss: 1.8515\n",
      "Epoch 211/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2698 - val_loss: 1.8528\n",
      "Epoch 212/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2791 - val_loss: 1.8562\n",
      "Epoch 213/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2615 - val_loss: 1.8483\n",
      "Epoch 214/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2581 - val_loss: 1.8169\n",
      "Epoch 215/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.2570 - val_loss: 1.8503\n",
      "Epoch 216/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.2196 - val_loss: 1.8664\n",
      "Epoch 217/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.2196 - val_loss: 1.8642\n",
      "Epoch 218/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2109 - val_loss: 1.8719\n",
      "Epoch 219/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.1891 - val_loss: 1.8773\n",
      "Epoch 220/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1974 - val_loss: 1.8648\n",
      "Epoch 221/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1986 - val_loss: 1.8978\n",
      "Epoch 222/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.1965 - val_loss: 1.8916\n",
      "Epoch 223/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 1.2084 - val_loss: 1.9193\n",
      "Epoch 224/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2076 - val_loss: 1.9518\n",
      "Epoch 225/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1965 - val_loss: 1.9058\n",
      "Epoch 226/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2024 - val_loss: 1.9493\n",
      "Epoch 227/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2361 - val_loss: 1.9117\n",
      "Epoch 228/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2408 - val_loss: 1.9424\n",
      "Epoch 229/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2785 - val_loss: 1.9089\n",
      "Epoch 230/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2870 - val_loss: 1.9112\n",
      "Epoch 231/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3147 - val_loss: 1.9285\n",
      "Epoch 232/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2881 - val_loss: 1.9177\n",
      "Epoch 233/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2790 - val_loss: 1.8907\n",
      "Epoch 234/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2978 - val_loss: 1.8891\n",
      "Epoch 235/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2982 - val_loss: 1.8613\n",
      "Epoch 236/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2926 - val_loss: 1.9008\n",
      "Epoch 237/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2701 - val_loss: 1.8927\n",
      "Epoch 238/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2922 - val_loss: 1.8883\n",
      "Epoch 239/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2793 - val_loss: 1.8993\n",
      "Epoch 240/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.2640 - val_loss: 1.8563\n",
      "Epoch 241/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2390 - val_loss: 1.9114\n",
      "Epoch 242/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2631 - val_loss: 1.8634\n",
      "Epoch 243/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.2249 - val_loss: 1.8861\n",
      "Epoch 244/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2398 - val_loss: 1.8426\n",
      "Epoch 245/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2274 - val_loss: 1.8844\n",
      "Epoch 246/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2327 - val_loss: 1.8576\n",
      "Epoch 247/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2409 - val_loss: 1.9150\n",
      "Epoch 248/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 41ms/step - loss: 1.3088 - val_loss: 1.8590\n",
      "Epoch 249/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2839 - val_loss: 1.8559\n",
      "Epoch 250/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.2373 - val_loss: 1.8637\n",
      "Epoch 251/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2473 - val_loss: 1.8473\n",
      "Epoch 252/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2336 - val_loss: 1.8493\n",
      "Epoch 253/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2120 - val_loss: 1.8198\n",
      "Epoch 254/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.2409 - val_loss: 1.8285\n",
      "Epoch 255/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.2249 - val_loss: 1.8635\n",
      "Epoch 256/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3581 - val_loss: 1.8708\n",
      "Epoch 257/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.4418 - val_loss: 1.8139\n",
      "Epoch 258/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.4146 - val_loss: 1.8325\n",
      "Epoch 259/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3949 - val_loss: 1.8372\n",
      "Epoch 260/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.3851 - val_loss: 1.7959\n",
      "Epoch 261/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.3916 - val_loss: 1.8056\n",
      "Epoch 262/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.4316 - val_loss: 1.8005\n",
      "Epoch 263/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4355 - val_loss: 1.7638\n",
      "Epoch 264/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.4375 - val_loss: 1.8131\n",
      "Epoch 265/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.4288 - val_loss: 1.8083\n",
      "Epoch 266/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.4393 - val_loss: 1.7750\n",
      "Epoch 267/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3858 - val_loss: 1.7479\n",
      "Epoch 268/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3638 - val_loss: 1.7784\n",
      "Epoch 269/500\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 1.3763 - val_loss: 1.7868\n",
      "Epoch 270/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.3516 - val_loss: 1.7876\n",
      "Epoch 271/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.3361 - val_loss: 1.7870\n",
      "Epoch 272/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3716 - val_loss: 1.7942\n",
      "Epoch 273/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3259 - val_loss: 1.7890\n",
      "Epoch 274/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3267 - val_loss: 1.8125\n",
      "Epoch 275/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3218 - val_loss: 1.8074\n",
      "Epoch 276/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3195 - val_loss: 1.8201\n",
      "Epoch 277/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3344 - val_loss: 1.8152\n",
      "Epoch 278/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3220 - val_loss: 1.8275\n",
      "Epoch 279/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.3036 - val_loss: 1.8108\n",
      "Epoch 280/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2788 - val_loss: 1.7824\n",
      "Epoch 281/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3210 - val_loss: 1.8072\n",
      "Epoch 282/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2814 - val_loss: 1.7927\n",
      "Epoch 283/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2513 - val_loss: 1.7891\n",
      "Epoch 284/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2641 - val_loss: 1.7924\n",
      "Epoch 285/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.2712 - val_loss: 1.8173\n",
      "Epoch 286/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2664 - val_loss: 1.8092\n",
      "Epoch 287/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2871 - val_loss: 1.8301\n",
      "Epoch 288/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2818 - val_loss: 1.8253\n",
      "Epoch 289/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2860 - val_loss: 1.8365\n",
      "Epoch 290/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2344 - val_loss: 1.8457\n",
      "Epoch 291/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2540 - val_loss: 1.8423\n",
      "Epoch 292/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2573 - val_loss: 1.8769\n",
      "Epoch 293/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2641 - val_loss: 1.8665\n",
      "Epoch 294/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2419 - val_loss: 1.8461\n",
      "Epoch 295/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2496 - val_loss: 1.9121\n",
      "Epoch 296/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.2750 - val_loss: 1.8820\n",
      "Epoch 297/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2637 - val_loss: 1.8909\n",
      "Epoch 298/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2499 - val_loss: 1.8637\n",
      "Epoch 299/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2923 - val_loss: 1.8919\n",
      "Epoch 300/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.2415 - val_loss: 1.8639\n",
      "Epoch 301/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2554 - val_loss: 1.8679\n",
      "Epoch 302/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2653 - val_loss: 1.8996\n",
      "Epoch 303/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2662 - val_loss: 1.8812\n",
      "Epoch 304/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2179 - val_loss: 1.9039\n",
      "Epoch 305/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2438 - val_loss: 1.8672\n",
      "Epoch 306/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2415 - val_loss: 1.8885\n",
      "Epoch 307/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2456 - val_loss: 1.8714\n",
      "Epoch 308/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.2448 - val_loss: 1.8677\n",
      "Epoch 309/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2419 - val_loss: 1.8822\n",
      "Epoch 310/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2708 - val_loss: 1.8688\n",
      "Epoch 311/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2325 - val_loss: 1.8859\n",
      "Epoch 312/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.1993 - val_loss: 1.8752\n",
      "Epoch 313/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2303 - val_loss: 1.8984\n",
      "Epoch 314/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2183 - val_loss: 1.8858\n",
      "Epoch 315/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.1729 - val_loss: 1.8972\n",
      "Epoch 316/500\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 1.2115 - val_loss: 1.8851\n",
      "Epoch 317/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2070 - val_loss: 1.8847\n",
      "Epoch 318/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2430 - val_loss: 1.9171\n",
      "Epoch 319/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.2668 - val_loss: 1.8941\n",
      "Epoch 320/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2453 - val_loss: 1.9056\n",
      "Epoch 321/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2580 - val_loss: 1.9004\n",
      "Epoch 322/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2652 - val_loss: 1.8608\n",
      "Epoch 323/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3316 - val_loss: 1.8728\n",
      "Epoch 324/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2968 - val_loss: 1.8659\n",
      "Epoch 325/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.2848 - val_loss: 1.8540\n",
      "Epoch 326/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3097 - val_loss: 1.8650\n",
      "Epoch 327/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2877 - val_loss: 1.8988\n",
      "Epoch 328/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3303 - val_loss: 1.8795\n",
      "Epoch 329/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.3163 - val_loss: 1.8440\n",
      "Epoch 330/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3094 - val_loss: 1.8945\n",
      "Epoch 331/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2997 - val_loss: 1.9003\n",
      "Epoch 332/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3055 - val_loss: 1.9449\n",
      "Epoch 333/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2880 - val_loss: 1.9420\n",
      "Epoch 334/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2664 - val_loss: 1.8997\n",
      "Epoch 335/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2334 - val_loss: 1.9233\n",
      "Epoch 336/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2182 - val_loss: 1.9172\n",
      "Epoch 337/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2429 - val_loss: 1.9316\n",
      "Epoch 338/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2387 - val_loss: 1.9023\n",
      "Epoch 339/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2067 - val_loss: 1.9586\n",
      "Epoch 340/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2376 - val_loss: 1.9223\n",
      "Epoch 341/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2181 - val_loss: 1.9103\n",
      "Epoch 342/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2250 - val_loss: 1.8996\n",
      "Epoch 343/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1989 - val_loss: 1.8986\n",
      "Epoch 344/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1989 - val_loss: 1.9173\n",
      "Epoch 345/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.2129 - val_loss: 1.9057\n",
      "Epoch 346/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2189 - val_loss: 1.9271\n",
      "Epoch 347/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1972 - val_loss: 1.9233\n",
      "Epoch 348/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2011 - val_loss: 1.9341\n",
      "Epoch 349/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1816 - val_loss: 1.9216\n",
      "Epoch 350/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.1765 - val_loss: 1.9236\n",
      "Epoch 351/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.1882 - val_loss: 1.9444\n",
      "Epoch 352/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1591 - val_loss: 1.9451\n",
      "Epoch 353/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1631 - val_loss: 1.9392\n",
      "Epoch 354/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1746 - val_loss: 1.9386\n",
      "Epoch 355/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1608 - val_loss: 1.9600\n",
      "Epoch 356/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.1631 - val_loss: 1.9624\n",
      "Epoch 357/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1648 - val_loss: 1.9464\n",
      "Epoch 358/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1499 - val_loss: 1.9522\n",
      "Epoch 359/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1696 - val_loss: 1.9546\n",
      "Epoch 360/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1502 - val_loss: 1.9711\n",
      "Epoch 361/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1800 - val_loss: 1.9751\n",
      "Epoch 362/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 1.1608 - val_loss: 1.9840\n",
      "Epoch 363/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1540 - val_loss: 1.9768\n",
      "Epoch 364/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1474 - val_loss: 1.9700\n",
      "Epoch 365/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1611 - val_loss: 2.0091\n",
      "Epoch 366/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1621 - val_loss: 1.9726\n",
      "Epoch 367/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1717 - val_loss: 1.9821\n",
      "Epoch 368/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1928 - val_loss: 1.9485\n",
      "Epoch 369/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2090 - val_loss: 1.9710\n",
      "Epoch 370/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2410 - val_loss: 1.9155\n",
      "Epoch 371/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2498 - val_loss: 1.9629\n",
      "Epoch 372/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.2602 - val_loss: 1.9451\n",
      "Epoch 373/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2774 - val_loss: 1.9371\n",
      "Epoch 374/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.2931 - val_loss: 1.9375\n",
      "Epoch 375/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2660 - val_loss: 1.9240\n",
      "Epoch 376/500\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 1.2803 - val_loss: 1.9524\n",
      "Epoch 377/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.3520 - val_loss: 1.9738\n",
      "Epoch 378/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3468 - val_loss: 1.9530\n",
      "Epoch 379/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.3048 - val_loss: 1.9733\n",
      "Epoch 380/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3403 - val_loss: 1.9738\n",
      "Epoch 381/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3078 - val_loss: 1.9526\n",
      "Epoch 382/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3254 - val_loss: 1.9449\n",
      "Epoch 383/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3234 - val_loss: 1.9460\n",
      "Epoch 384/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3193 - val_loss: 1.9209\n",
      "Epoch 385/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.2980 - val_loss: 1.9291\n",
      "Epoch 386/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3123 - val_loss: 1.9453\n",
      "Epoch 387/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3114 - val_loss: 1.9137\n",
      "Epoch 388/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2995 - val_loss: 1.8866\n",
      "Epoch 389/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2974 - val_loss: 1.9121\n",
      "Epoch 390/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2635 - val_loss: 1.9279\n",
      "Epoch 391/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2847 - val_loss: 1.9688\n",
      "Epoch 392/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3375 - val_loss: 1.9186\n",
      "Epoch 393/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3389 - val_loss: 1.9174\n",
      "Epoch 394/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.2928 - val_loss: 1.9290\n",
      "Epoch 395/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.3084 - val_loss: 1.9162\n",
      "Epoch 396/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2849 - val_loss: 1.9152\n",
      "Epoch 397/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2947 - val_loss: 1.8924\n",
      "Epoch 398/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2516 - val_loss: 1.8847\n",
      "Epoch 399/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2632 - val_loss: 1.8850\n",
      "Epoch 400/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2494 - val_loss: 1.8927\n",
      "Epoch 401/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2197 - val_loss: 1.9271\n",
      "Epoch 402/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.2242 - val_loss: 1.9124\n",
      "Epoch 403/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2214 - val_loss: 1.9724\n",
      "Epoch 404/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.2176 - val_loss: 1.9262\n",
      "Epoch 405/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2520 - val_loss: 1.9375\n",
      "Epoch 406/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2163 - val_loss: 1.9289\n",
      "Epoch 407/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2102 - val_loss: 1.9227\n",
      "Epoch 408/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1964 - val_loss: 1.9654\n",
      "Epoch 409/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 1.2144 - val_loss: 1.9150\n",
      "Epoch 410/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.1862 - val_loss: 1.9768\n",
      "Epoch 411/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1712 - val_loss: 1.9319\n",
      "Epoch 412/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1867 - val_loss: 1.9585\n",
      "Epoch 413/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1812 - val_loss: 1.9412\n",
      "Epoch 414/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1699 - val_loss: 1.9684\n",
      "Epoch 415/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1376 - val_loss: 1.9612\n",
      "Epoch 416/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1733 - val_loss: 1.9800\n",
      "Epoch 417/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1502 - val_loss: 1.9648\n",
      "Epoch 418/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1635 - val_loss: 1.9662\n",
      "Epoch 419/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1996 - val_loss: 1.9418\n",
      "Epoch 420/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.1661 - val_loss: 1.9599\n",
      "Epoch 421/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1817 - val_loss: 1.9718\n",
      "Epoch 422/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1763 - val_loss: 1.9861\n",
      "Epoch 423/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1875 - val_loss: 1.9862\n",
      "Epoch 424/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1875 - val_loss: 1.9631\n",
      "Epoch 425/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.1715 - val_loss: 1.9907\n",
      "Epoch 426/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1634 - val_loss: 1.9833\n",
      "Epoch 427/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1678 - val_loss: 2.0011\n",
      "Epoch 428/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1499 - val_loss: 2.0034\n",
      "Epoch 429/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1428 - val_loss: 2.0065\n",
      "Epoch 430/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1536 - val_loss: 2.0183\n",
      "Epoch 431/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.1821 - val_loss: 1.9843\n",
      "Epoch 432/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.1625 - val_loss: 2.0092\n",
      "Epoch 433/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 1.1549 - val_loss: 2.0111\n",
      "Epoch 434/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.1374 - val_loss: 2.0261\n",
      "Epoch 435/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.1141 - val_loss: 1.9973\n",
      "Epoch 436/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 1.1568 - val_loss: 2.0198\n",
      "Epoch 437/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1458 - val_loss: 2.0336\n",
      "Epoch 438/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1393 - val_loss: 2.0233\n",
      "Epoch 439/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.1255 - val_loss: 2.0189\n",
      "Epoch 440/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.1419 - val_loss: 2.0253\n",
      "Epoch 441/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.1503 - val_loss: 2.0046\n",
      "Epoch 442/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.1608 - val_loss: 2.0284\n",
      "Epoch 443/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1422 - val_loss: 2.0229\n",
      "Epoch 444/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.1323 - val_loss: 2.0157\n",
      "Epoch 445/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1381 - val_loss: 2.0570\n",
      "Epoch 446/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1319 - val_loss: 2.0249\n",
      "Epoch 447/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.1649 - val_loss: 2.0268\n",
      "Epoch 448/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1606 - val_loss: 2.0518\n",
      "Epoch 449/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1569 - val_loss: 2.0517\n",
      "Epoch 450/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 1.1482 - val_loss: 2.1414\n",
      "Epoch 451/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1746 - val_loss: 2.0451\n",
      "Epoch 452/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1922 - val_loss: 2.0845\n",
      "Epoch 453/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1603 - val_loss: 2.0611\n",
      "Epoch 454/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.2009 - val_loss: 2.1023\n",
      "Epoch 455/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 1.1997 - val_loss: 2.0467\n",
      "Epoch 456/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.1747 - val_loss: 2.0312\n",
      "Epoch 457/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.1765 - val_loss: 2.0397\n",
      "Epoch 458/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1678 - val_loss: 2.0128\n",
      "Epoch 459/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1691 - val_loss: 2.0007\n",
      "Epoch 460/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.1629 - val_loss: 2.0065\n",
      "Epoch 461/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1421 - val_loss: 2.0061\n",
      "Epoch 462/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.1572 - val_loss: 2.0262\n",
      "Epoch 463/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.1522 - val_loss: 2.0417\n",
      "Epoch 464/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1434 - val_loss: 2.0372\n",
      "Epoch 465/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1374 - val_loss: 2.0435\n",
      "Epoch 466/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1349 - val_loss: 2.0454\n",
      "Epoch 467/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.1311 - val_loss: 2.0028\n",
      "Epoch 468/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1707 - val_loss: 2.0349\n",
      "Epoch 469/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1607 - val_loss: 2.0157\n",
      "Epoch 470/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1294 - val_loss: 2.0343\n",
      "Epoch 471/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.1573 - val_loss: 2.0207\n",
      "Epoch 472/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1547 - val_loss: 2.0126\n",
      "Epoch 473/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.1263 - val_loss: 2.0116\n",
      "Epoch 474/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1217 - val_loss: 2.0254\n",
      "Epoch 475/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.1243 - val_loss: 1.9949\n",
      "Epoch 476/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.1379 - val_loss: 2.0412\n",
      "Epoch 477/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1369 - val_loss: 2.0503\n",
      "Epoch 478/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1587 - val_loss: 2.0498\n",
      "Epoch 479/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1263 - val_loss: 2.0672\n",
      "Epoch 480/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.1591 - val_loss: 2.0534\n",
      "Epoch 481/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1264 - val_loss: 2.0519\n",
      "Epoch 482/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1271 - val_loss: 2.0558\n",
      "Epoch 483/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.1393 - val_loss: 2.0721\n",
      "Epoch 484/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1650 - val_loss: 2.0799\n",
      "Epoch 485/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.1595 - val_loss: 2.0967\n",
      "Epoch 486/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.1271 - val_loss: 2.0646\n",
      "Epoch 487/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.1380 - val_loss: 2.1137\n",
      "Epoch 488/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1458 - val_loss: 2.1031\n",
      "Epoch 489/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1493 - val_loss: 2.0781\n",
      "Epoch 490/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1809 - val_loss: 2.1261\n",
      "Epoch 491/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.2359 - val_loss: 2.0859\n",
      "Epoch 492/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2203 - val_loss: 2.0342\n",
      "Epoch 493/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2604 - val_loss: 2.0515\n",
      "Epoch 494/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 32ms/step - loss: 1.2300 - val_loss: 2.0761\n",
      "Epoch 495/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2240 - val_loss: 2.0657\n",
      "Epoch 496/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2557 - val_loss: 2.0928\n",
      "Epoch 497/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.2344 - val_loss: 2.0729\n",
      "Epoch 498/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.2584 - val_loss: 2.1069\n",
      "Epoch 499/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3002 - val_loss: 2.1054\n",
      "Epoch 500/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.2797 - val_loss: 2.1199\n"
     ]
    }
   ],
   "source": [
    "hist = sequence_autoencoder.fit(X_train, y_train,\n",
    "                epochs=500,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder'), mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.plot(hist.history['loss'], label='training')\n",
    "plt.plot(hist.history['val_loss'], label='testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'figures/{name}', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_autoencoder = load_model(f'Models/{name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5378388554216867"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sequence_autoencoder.predict(X_train).argmax(axis=-1)\n",
    "accuracy_score(y_train.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4296875"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sequence_autoencoder.predict(X_test).argmax(axis=-1)\n",
    "accuracy_score(y_test.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Embedding-Conv1D-MaxPooling1D-Conv1D-MaxPooling1D-Conv1D Encoder Conv1D-Upsampling1D-Conv1D-Upsampling1D-Conv1D-Dense Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NBG_conv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 64  # Length of your sequences\n",
    "embed_size = 32\n",
    "latent_dim = 256\n",
    "dropout = 0\n",
    "\n",
    "inputs = Input(shape=(timesteps,))\n",
    "embedded = Embedding(vocab_size, embed_size)(inputs)\n",
    "encoded = Conv1D(latent_dim, 3, activation='relu', padding='same')(embedded)\n",
    "encoded = MaxPooling1D(2, padding='same')(encoded)\n",
    "encoded = Conv1D(2 * latent_dim, 3, activation='relu', padding='same')(encoded)\n",
    "encoded = MaxPooling1D(2, padding='same')(encoded)\n",
    "\n",
    "decoded = Conv1D(2 * latent_dim, 3, activation='relu', padding='same')(encoded)\n",
    "decoded = UpSampling1D(2)(decoded)\n",
    "decoded = Conv1D(latent_dim, 3, activation='relu', padding='same')(decoded)\n",
    "decoded = UpSampling1D(2)(decoded)\n",
    "decoded = Conv1D(1, 3, activation='relu', padding='same')(decoded)\n",
    "decoded = Dense(vocab_size, activation='softmax')(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 64, 32)            1152      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 64, 256)           24832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 32, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 32, 512)           393728    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 512)           786944    \n",
      "_________________________________________________________________\n",
      "up_sampling1d (UpSampling1D) (None, 32, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 32, 256)           393472    \n",
      "_________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1 (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 64, 1)             769       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64, 36)            72        \n",
      "=================================================================\n",
      "Total params: 1,600,969\n",
      "Trainable params: 1,600,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs, encoded)\n",
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(timesteps//4,2*latent_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layers = sequence_autoencoder.layers[-6:]\n",
    "decoded_input = decoder_layers[0](encoded_input)\n",
    "for decoder_layer in decoder_layers[1:]:\n",
    "    decoded_input = decoder_layer(decoded_input)\n",
    "# Create the decoder model\n",
    "decoder = Model(encoded_input, decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 64, 32)            1152      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 64, 256)           24832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 32, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 32, 512)           393728    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 16, 512)           0         \n",
      "=================================================================\n",
      "Total params: 419,712\n",
      "Trainable params: 419,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 16, 512)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 512)           786944    \n",
      "_________________________________________________________________\n",
      "up_sampling1d (UpSampling1D) (None, 32, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 32, 256)           393472    \n",
      "_________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1 (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 64, 1)             769       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64, 36)            72        \n",
      "=================================================================\n",
      "Total params: 1,181,257\n",
      "Trainable params: 1,181,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "\n",
    "mc = ModelCheckpoint(f'Models/{name}.hdf5', monitor='val_loss')\n",
    "\n",
    "optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "sequence_autoencoder.compile(optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "6/6 [==============================] - 3s 221ms/step - loss: 3.5750 - val_loss: 3.5440\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 3.5361 - val_loss: 3.5051\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 3.4979 - val_loss: 3.4665\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 3.4608 - val_loss: 3.4283\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 3.4240 - val_loss: 3.3906\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 3.3880 - val_loss: 3.3535\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 3.3512 - val_loss: 3.3168\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 3.3170 - val_loss: 3.2803\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 3.2787 - val_loss: 3.2442\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 3.2428 - val_loss: 3.2087\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 3.2102 - val_loss: 3.1737\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 3.1777 - val_loss: 3.1394\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 3.1454 - val_loss: 3.1053\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 3.1120 - val_loss: 3.0717\n",
      "Epoch 15/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 3.0808 - val_loss: 3.0385\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 3.0476 - val_loss: 3.0060\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 3.0127 - val_loss: 2.9741\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.9862 - val_loss: 2.9428\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 2.9558 - val_loss: 2.9121\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.9233 - val_loss: 2.8818\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 2.8918 - val_loss: 2.8518\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.8655 - val_loss: 2.8223\n",
      "Epoch 23/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.8357 - val_loss: 2.7935\n",
      "Epoch 24/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.8098 - val_loss: 2.7655\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.7835 - val_loss: 2.7380\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.7598 - val_loss: 2.7111\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.7335 - val_loss: 2.6848\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.7148 - val_loss: 2.6593\n",
      "Epoch 29/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.6764 - val_loss: 2.6342\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.6528 - val_loss: 2.6098\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.6306 - val_loss: 2.5861\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.6195 - val_loss: 2.5630\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.5983 - val_loss: 2.5405\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.5643 - val_loss: 2.5183\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.5538 - val_loss: 2.4970\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.5413 - val_loss: 2.4760\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.5054 - val_loss: 2.4556\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.4812 - val_loss: 2.4358\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.4736 - val_loss: 2.4167\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.4482 - val_loss: 2.3982\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.4266 - val_loss: 2.3803\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.4193 - val_loss: 2.3633\n",
      "Epoch 43/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.3938 - val_loss: 2.3468\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.3739 - val_loss: 2.3310\n",
      "Epoch 45/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.3815 - val_loss: 2.3157\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.3492 - val_loss: 2.3009\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.3315 - val_loss: 2.2866\n",
      "Epoch 48/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.3444 - val_loss: 2.2727\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.3293 - val_loss: 2.2594\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.3006 - val_loss: 2.2464\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.2692 - val_loss: 2.2338\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.2896 - val_loss: 2.2218\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.2813 - val_loss: 2.2102\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.2478 - val_loss: 2.1992\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.2587 - val_loss: 2.1887\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.2477 - val_loss: 2.1786\n",
      "Epoch 57/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.2322 - val_loss: 2.1687\n",
      "Epoch 58/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 2.2002 - val_loss: 2.1592\n",
      "Epoch 59/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.2327 - val_loss: 2.1502\n",
      "Epoch 60/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.1979 - val_loss: 2.1414\n",
      "Epoch 61/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.1918 - val_loss: 2.1327\n",
      "Epoch 62/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.1933 - val_loss: 2.1246\n",
      "Epoch 63/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.1792 - val_loss: 2.1168\n",
      "Epoch 64/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.1695 - val_loss: 2.1093\n",
      "Epoch 65/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.1735 - val_loss: 2.1023\n",
      "Epoch 66/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.1721 - val_loss: 2.0954\n",
      "Epoch 67/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.1680 - val_loss: 2.0888\n",
      "Epoch 68/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.1346 - val_loss: 2.0823\n",
      "Epoch 69/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.1213 - val_loss: 2.0762\n",
      "Epoch 70/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.1260 - val_loss: 2.0704\n",
      "Epoch 71/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.1436 - val_loss: 2.0649\n",
      "Epoch 72/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.1189 - val_loss: 2.0597\n",
      "Epoch 73/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.1288 - val_loss: 2.0547\n",
      "Epoch 74/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.1207 - val_loss: 2.0499\n",
      "Epoch 75/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.1245 - val_loss: 2.0452\n",
      "Epoch 76/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.1219 - val_loss: 2.0408\n",
      "Epoch 77/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.1125 - val_loss: 2.0365\n",
      "Epoch 78/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0997 - val_loss: 2.0322\n",
      "Epoch 79/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0757 - val_loss: 2.0281\n",
      "Epoch 80/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0974 - val_loss: 2.0243\n",
      "Epoch 81/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0851 - val_loss: 2.0207\n",
      "Epoch 82/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0581 - val_loss: 2.0172\n",
      "Epoch 83/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0967 - val_loss: 2.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0947 - val_loss: 2.0109\n",
      "Epoch 85/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0796 - val_loss: 2.0077\n",
      "Epoch 86/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0898 - val_loss: 2.0047\n",
      "Epoch 87/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0948 - val_loss: 2.0017\n",
      "Epoch 88/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0777 - val_loss: 1.9988\n",
      "Epoch 89/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0587 - val_loss: 1.9961\n",
      "Epoch 90/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0569 - val_loss: 1.9935\n",
      "Epoch 91/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0523 - val_loss: 1.9912\n",
      "Epoch 92/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0570 - val_loss: 1.9889\n",
      "Epoch 93/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0509 - val_loss: 1.9869\n",
      "Epoch 94/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0523 - val_loss: 1.9849\n",
      "Epoch 95/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0538 - val_loss: 1.9829\n",
      "Epoch 96/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0422 - val_loss: 1.9812\n",
      "Epoch 97/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 2.0317 - val_loss: 1.9793\n",
      "Epoch 98/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 2.0311 - val_loss: 1.9776\n",
      "Epoch 99/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 2.0274 - val_loss: 1.9758\n",
      "Epoch 100/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 2.0211 - val_loss: 1.9741\n",
      "Epoch 101/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0669 - val_loss: 1.9726\n",
      "Epoch 102/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0651 - val_loss: 1.9711\n",
      "Epoch 103/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0376 - val_loss: 1.9696\n",
      "Epoch 104/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0366 - val_loss: 1.9682\n",
      "Epoch 105/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0487 - val_loss: 1.9668\n",
      "Epoch 106/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0685 - val_loss: 1.9655\n",
      "Epoch 107/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0477 - val_loss: 1.9641\n",
      "Epoch 108/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0203 - val_loss: 1.9627\n",
      "Epoch 109/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0263 - val_loss: 1.9615\n",
      "Epoch 110/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0792 - val_loss: 1.9604\n",
      "Epoch 111/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0272 - val_loss: 1.9593\n",
      "Epoch 112/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0316 - val_loss: 1.9582\n",
      "Epoch 113/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0478 - val_loss: 1.9571\n",
      "Epoch 114/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 2.0621 - val_loss: 1.9561\n",
      "Epoch 115/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0492 - val_loss: 1.9551\n",
      "Epoch 116/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0226 - val_loss: 1.9541\n",
      "Epoch 117/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0258 - val_loss: 1.9532\n",
      "Epoch 118/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0557 - val_loss: 1.9524\n",
      "Epoch 119/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0721 - val_loss: 1.9515\n",
      "Epoch 120/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0460 - val_loss: 1.9507\n",
      "Epoch 121/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0643 - val_loss: 1.9499\n",
      "Epoch 122/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0056 - val_loss: 1.9490\n",
      "Epoch 123/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0236 - val_loss: 1.9482\n",
      "Epoch 124/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0369 - val_loss: 1.9475\n",
      "Epoch 125/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0379 - val_loss: 1.9468\n",
      "Epoch 126/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0373 - val_loss: 1.9460\n",
      "Epoch 127/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0198 - val_loss: 1.9454\n",
      "Epoch 128/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0226 - val_loss: 1.9448\n",
      "Epoch 129/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0129 - val_loss: 1.9443\n",
      "Epoch 130/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0364 - val_loss: 1.9438\n",
      "Epoch 131/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0158 - val_loss: 1.9432\n",
      "Epoch 132/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0108 - val_loss: 1.9425\n",
      "Epoch 133/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0131 - val_loss: 1.9419\n",
      "Epoch 134/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0450 - val_loss: 1.9414\n",
      "Epoch 135/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0194 - val_loss: 1.9409\n",
      "Epoch 136/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 2.0327 - val_loss: 1.9403\n",
      "Epoch 137/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9902 - val_loss: 1.9399\n",
      "Epoch 138/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 2.0563 - val_loss: 1.9395\n",
      "Epoch 139/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.0467 - val_loss: 1.9390\n",
      "Epoch 140/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0278 - val_loss: 1.9385\n",
      "Epoch 141/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0104 - val_loss: 1.9381\n",
      "Epoch 142/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9895 - val_loss: 1.9376\n",
      "Epoch 143/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0188 - val_loss: 1.9374\n",
      "Epoch 144/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0174 - val_loss: 1.9371\n",
      "Epoch 145/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0190 - val_loss: 1.9368\n",
      "Epoch 146/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9600 - val_loss: 1.9365\n",
      "Epoch 147/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0246 - val_loss: 1.9363\n",
      "Epoch 148/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9972 - val_loss: 1.9360\n",
      "Epoch 149/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0061 - val_loss: 1.9356\n",
      "Epoch 150/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0050 - val_loss: 1.9351\n",
      "Epoch 151/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0419 - val_loss: 1.9348\n",
      "Epoch 152/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0237 - val_loss: 1.9344\n",
      "Epoch 153/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0048 - val_loss: 1.9341\n",
      "Epoch 154/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.9998 - val_loss: 1.9338\n",
      "Epoch 155/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0093 - val_loss: 1.9334\n",
      "Epoch 156/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0257 - val_loss: 1.9330\n",
      "Epoch 157/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0177 - val_loss: 1.9328\n",
      "Epoch 158/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0211 - val_loss: 1.9325\n",
      "Epoch 159/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0184 - val_loss: 1.9323\n",
      "Epoch 160/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0522 - val_loss: 1.9321\n",
      "Epoch 161/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0349 - val_loss: 1.9319\n",
      "Epoch 162/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 2.0165 - val_loss: 1.9317\n",
      "Epoch 163/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.0067 - val_loss: 1.9314\n",
      "Epoch 164/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 1.9857 - val_loss: 1.9311\n",
      "Epoch 165/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0140 - val_loss: 1.9309\n",
      "Epoch 166/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 29ms/step - loss: 2.0121 - val_loss: 1.9305\n",
      "Epoch 167/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.9996 - val_loss: 1.9302\n",
      "Epoch 168/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0352 - val_loss: 1.9301\n",
      "Epoch 169/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0221 - val_loss: 1.9299\n",
      "Epoch 170/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.9958 - val_loss: 1.9297\n",
      "Epoch 171/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0426 - val_loss: 1.9295\n",
      "Epoch 172/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0061 - val_loss: 1.9294\n",
      "Epoch 173/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0262 - val_loss: 1.9292\n",
      "Epoch 174/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9989 - val_loss: 1.9290\n",
      "Epoch 175/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 2.0276 - val_loss: 1.9288\n",
      "Epoch 176/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0038 - val_loss: 1.9287\n",
      "Epoch 177/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9783 - val_loss: 1.9285\n",
      "Epoch 178/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0220 - val_loss: 1.9284\n",
      "Epoch 179/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9890 - val_loss: 1.9282\n",
      "Epoch 180/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9940 - val_loss: 1.9281\n",
      "Epoch 181/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9716 - val_loss: 1.9279\n",
      "Epoch 182/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0201 - val_loss: 1.9277\n",
      "Epoch 183/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0331 - val_loss: 1.9277\n",
      "Epoch 184/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.9937 - val_loss: 1.9275\n",
      "Epoch 185/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0330 - val_loss: 1.9272\n",
      "Epoch 186/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0256 - val_loss: 1.9270\n",
      "Epoch 187/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9857 - val_loss: 1.9269\n",
      "Epoch 188/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0151 - val_loss: 1.9268\n",
      "Epoch 189/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.9960 - val_loss: 1.9266\n",
      "Epoch 190/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9624 - val_loss: 1.9264\n",
      "Epoch 191/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.0331 - val_loss: 1.9263\n",
      "Epoch 192/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9927 - val_loss: 1.9263\n",
      "Epoch 193/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9736 - val_loss: 1.9262\n",
      "Epoch 194/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9874 - val_loss: 1.9261\n",
      "Epoch 195/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0103 - val_loss: 1.9261\n",
      "Epoch 196/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9768 - val_loss: 1.9259\n",
      "Epoch 197/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0036 - val_loss: 1.9258\n",
      "Epoch 198/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0143 - val_loss: 1.9256\n",
      "Epoch 199/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0031 - val_loss: 1.9254\n",
      "Epoch 200/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0008 - val_loss: 1.9252\n",
      "Epoch 201/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0234 - val_loss: 1.9252\n",
      "Epoch 202/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9976 - val_loss: 1.9251\n",
      "Epoch 203/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9772 - val_loss: 1.9251\n",
      "Epoch 204/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9785 - val_loss: 1.9251\n",
      "Epoch 205/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0125 - val_loss: 1.9251\n",
      "Epoch 206/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0184 - val_loss: 1.9251\n",
      "Epoch 207/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.0039 - val_loss: 1.9249\n",
      "Epoch 208/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0353 - val_loss: 1.9249\n",
      "Epoch 209/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0134 - val_loss: 1.9248\n",
      "Epoch 210/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0160 - val_loss: 1.9247\n",
      "Epoch 211/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 2.0209 - val_loss: 1.9246\n",
      "Epoch 212/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.0016 - val_loss: 1.9244\n",
      "Epoch 213/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0181 - val_loss: 1.9243\n",
      "Epoch 214/500\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 2.0006 - val_loss: 1.9241\n",
      "Epoch 215/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9983 - val_loss: 1.9239\n",
      "Epoch 216/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.0010 - val_loss: 1.9237\n",
      "Epoch 217/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0283 - val_loss: 1.9237\n",
      "Epoch 218/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0133 - val_loss: 1.9236\n",
      "Epoch 219/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0109 - val_loss: 1.9235\n",
      "Epoch 220/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9822 - val_loss: 1.9235\n",
      "Epoch 221/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0011 - val_loss: 1.9235\n",
      "Epoch 222/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9924 - val_loss: 1.9234\n",
      "Epoch 223/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0360 - val_loss: 1.9233\n",
      "Epoch 224/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9969 - val_loss: 1.9231\n",
      "Epoch 225/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9656 - val_loss: 1.9229\n",
      "Epoch 226/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0317 - val_loss: 1.9228\n",
      "Epoch 227/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9982 - val_loss: 1.9227\n",
      "Epoch 228/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9802 - val_loss: 1.9226\n",
      "Epoch 229/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0128 - val_loss: 1.9226\n",
      "Epoch 230/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9962 - val_loss: 1.9226\n",
      "Epoch 231/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0221 - val_loss: 1.9226\n",
      "Epoch 232/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0292 - val_loss: 1.9226\n",
      "Epoch 233/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0006 - val_loss: 1.9225\n",
      "Epoch 234/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.0196 - val_loss: 1.9226\n",
      "Epoch 235/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0156 - val_loss: 1.9226\n",
      "Epoch 236/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0102 - val_loss: 1.9226\n",
      "Epoch 237/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9988 - val_loss: 1.9226\n",
      "Epoch 238/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0083 - val_loss: 1.9225\n",
      "Epoch 239/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0336 - val_loss: 1.9225\n",
      "Epoch 240/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0144 - val_loss: 1.9224\n",
      "Epoch 241/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0060 - val_loss: 1.9221\n",
      "Epoch 242/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0352 - val_loss: 1.9220\n",
      "Epoch 243/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0185 - val_loss: 1.9219\n",
      "Epoch 244/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9744 - val_loss: 1.9216\n",
      "Epoch 245/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0064 - val_loss: 1.9215\n",
      "Epoch 246/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9912 - val_loss: 1.9215\n",
      "Epoch 247/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0236 - val_loss: 1.9215\n",
      "Epoch 248/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9889 - val_loss: 1.9214\n",
      "Epoch 249/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9895 - val_loss: 1.9213\n",
      "Epoch 250/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0330 - val_loss: 1.9213\n",
      "Epoch 251/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0010 - val_loss: 1.9214\n",
      "Epoch 252/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.9981 - val_loss: 1.9213\n",
      "Epoch 253/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 1.9937 - val_loss: 1.9212\n",
      "Epoch 254/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0349 - val_loss: 1.9212\n",
      "Epoch 255/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0074 - val_loss: 1.9211\n",
      "Epoch 256/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0050 - val_loss: 1.9211\n",
      "Epoch 257/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 2.0216 - val_loss: 1.9211\n",
      "Epoch 258/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0001 - val_loss: 1.9211\n",
      "Epoch 259/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9656 - val_loss: 1.9210\n",
      "Epoch 260/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9940 - val_loss: 1.9210\n",
      "Epoch 261/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0114 - val_loss: 1.9208\n",
      "Epoch 262/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9814 - val_loss: 1.9207\n",
      "Epoch 263/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.9870 - val_loss: 1.9207\n",
      "Epoch 264/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0186 - val_loss: 1.9207\n",
      "Epoch 265/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0140 - val_loss: 1.9207\n",
      "Epoch 266/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9873 - val_loss: 1.9206\n",
      "Epoch 267/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9789 - val_loss: 1.9204\n",
      "Epoch 268/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9770 - val_loss: 1.9203\n",
      "Epoch 269/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9839 - val_loss: 1.9203\n",
      "Epoch 270/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0280 - val_loss: 1.9204\n",
      "Epoch 271/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0052 - val_loss: 1.9204\n",
      "Epoch 272/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9923 - val_loss: 1.9205\n",
      "Epoch 273/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9941 - val_loss: 1.9206\n",
      "Epoch 274/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9809 - val_loss: 1.9205\n",
      "Epoch 275/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0182 - val_loss: 1.9204\n",
      "Epoch 276/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9867 - val_loss: 1.9203\n",
      "Epoch 277/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.9957 - val_loss: 1.9202\n",
      "Epoch 278/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0224 - val_loss: 1.9202\n",
      "Epoch 279/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0257 - val_loss: 1.9203\n",
      "Epoch 280/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0132 - val_loss: 1.9203\n",
      "Epoch 281/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9855 - val_loss: 1.9203\n",
      "Epoch 282/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9990 - val_loss: 1.9202\n",
      "Epoch 283/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0171 - val_loss: 1.9201\n",
      "Epoch 284/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0077 - val_loss: 1.9201\n",
      "Epoch 285/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0013 - val_loss: 1.9202\n",
      "Epoch 286/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0214 - val_loss: 1.9203\n",
      "Epoch 287/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0070 - val_loss: 1.9205\n",
      "Epoch 288/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9973 - val_loss: 1.9206\n",
      "Epoch 289/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0141 - val_loss: 1.9207\n",
      "Epoch 290/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0274 - val_loss: 1.9208\n",
      "Epoch 291/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.9988 - val_loss: 1.9209\n",
      "Epoch 292/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 1.9987 - val_loss: 1.9211\n",
      "Epoch 293/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9943 - val_loss: 1.9211\n",
      "Epoch 294/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0125 - val_loss: 1.9212\n",
      "Epoch 295/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0216 - val_loss: 1.9212\n",
      "Epoch 296/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9910 - val_loss: 1.9211\n",
      "Epoch 297/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0283 - val_loss: 1.9210\n",
      "Epoch 298/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9765 - val_loss: 1.9210\n",
      "Epoch 299/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0252 - val_loss: 1.9210\n",
      "Epoch 300/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9801 - val_loss: 1.9209\n",
      "Epoch 301/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0058 - val_loss: 1.9208\n",
      "Epoch 302/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0020 - val_loss: 1.9207\n",
      "Epoch 303/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0013 - val_loss: 1.9204\n",
      "Epoch 304/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0065 - val_loss: 1.9204\n",
      "Epoch 305/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0168 - val_loss: 1.9202\n",
      "Epoch 306/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.0054 - val_loss: 1.9202\n",
      "Epoch 307/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.9871 - val_loss: 1.9202\n",
      "Epoch 308/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9882 - val_loss: 1.9202\n",
      "Epoch 309/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0133 - val_loss: 1.9201\n",
      "Epoch 310/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0036 - val_loss: 1.9200\n",
      "Epoch 311/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9917 - val_loss: 1.9200\n",
      "Epoch 312/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9880 - val_loss: 1.9201\n",
      "Epoch 313/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.0172 - val_loss: 1.9201\n",
      "Epoch 314/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0120 - val_loss: 1.9201\n",
      "Epoch 315/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0009 - val_loss: 1.9202\n",
      "Epoch 316/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9900 - val_loss: 1.9202\n",
      "Epoch 317/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0085 - val_loss: 1.9203\n",
      "Epoch 318/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0124 - val_loss: 1.9204\n",
      "Epoch 319/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0031 - val_loss: 1.9205\n",
      "Epoch 320/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9826 - val_loss: 1.9205\n",
      "Epoch 321/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0236 - val_loss: 1.9205\n",
      "Epoch 322/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9972 - val_loss: 1.9204\n",
      "Epoch 323/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9930 - val_loss: 1.9205\n",
      "Epoch 324/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0108 - val_loss: 1.9204\n",
      "Epoch 325/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0204 - val_loss: 1.9203\n",
      "Epoch 326/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0223 - val_loss: 1.9202\n",
      "Epoch 327/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0158 - val_loss: 1.9202\n",
      "Epoch 328/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9948 - val_loss: 1.9201\n",
      "Epoch 329/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9889 - val_loss: 1.9200\n",
      "Epoch 330/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9849 - val_loss: 1.9201\n",
      "Epoch 331/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 1.9854 - val_loss: 1.9202\n",
      "Epoch 332/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0119 - val_loss: 1.9203\n",
      "Epoch 333/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 2.0071 - val_loss: 1.9203\n",
      "Epoch 334/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0180 - val_loss: 1.9201\n",
      "Epoch 335/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9973 - val_loss: 1.9198\n",
      "Epoch 336/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0202 - val_loss: 1.9197\n",
      "Epoch 337/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0134 - val_loss: 1.9195\n",
      "Epoch 338/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0100 - val_loss: 1.9196\n",
      "Epoch 339/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0179 - val_loss: 1.9197\n",
      "Epoch 340/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0055 - val_loss: 1.9198\n",
      "Epoch 341/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9672 - val_loss: 1.9200\n",
      "Epoch 342/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0117 - val_loss: 1.9201\n",
      "Epoch 343/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0123 - val_loss: 1.9202\n",
      "Epoch 344/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9870 - val_loss: 1.9201\n",
      "Epoch 345/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9700 - val_loss: 1.9200\n",
      "Epoch 346/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9914 - val_loss: 1.9201\n",
      "Epoch 347/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9841 - val_loss: 1.9200\n",
      "Epoch 348/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0071 - val_loss: 1.9200\n",
      "Epoch 349/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9842 - val_loss: 1.9200\n",
      "Epoch 350/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9914 - val_loss: 1.9200\n",
      "Epoch 351/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9874 - val_loss: 1.9201\n",
      "Epoch 352/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9589 - val_loss: 1.9201\n",
      "Epoch 353/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0192 - val_loss: 1.9203\n",
      "Epoch 354/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9887 - val_loss: 1.9203\n",
      "Epoch 355/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.0093 - val_loss: 1.9202\n",
      "Epoch 356/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0024 - val_loss: 1.9201\n",
      "Epoch 357/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9953 - val_loss: 1.9202\n",
      "Epoch 358/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0225 - val_loss: 1.9203\n",
      "Epoch 359/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0043 - val_loss: 1.9202\n",
      "Epoch 360/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0059 - val_loss: 1.9202\n",
      "Epoch 361/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.9771 - val_loss: 1.9202\n",
      "Epoch 362/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9747 - val_loss: 1.9202\n",
      "Epoch 363/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0041 - val_loss: 1.9201\n",
      "Epoch 364/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9928 - val_loss: 1.9199\n",
      "Epoch 365/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9906 - val_loss: 1.9199\n",
      "Epoch 366/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9944 - val_loss: 1.9199\n",
      "Epoch 367/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9987 - val_loss: 1.9200\n",
      "Epoch 368/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9893 - val_loss: 1.9200\n",
      "Epoch 369/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9982 - val_loss: 1.9202\n",
      "Epoch 370/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 2.0044 - val_loss: 1.9204\n",
      "Epoch 371/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9811 - val_loss: 1.9205\n",
      "Epoch 372/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9961 - val_loss: 1.9206\n",
      "Epoch 373/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0089 - val_loss: 1.9206\n",
      "Epoch 374/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.9879 - val_loss: 1.9205\n",
      "Epoch 375/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0048 - val_loss: 1.9203\n",
      "Epoch 376/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0220 - val_loss: 1.9201\n",
      "Epoch 377/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0072 - val_loss: 1.9200\n",
      "Epoch 378/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0041 - val_loss: 1.9200\n",
      "Epoch 379/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0455 - val_loss: 1.9201\n",
      "Epoch 380/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9845 - val_loss: 1.9202\n",
      "Epoch 381/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9992 - val_loss: 1.9201\n",
      "Epoch 382/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9945 - val_loss: 1.9200\n",
      "Epoch 383/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9876 - val_loss: 1.9201\n",
      "Epoch 384/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0131 - val_loss: 1.9201\n",
      "Epoch 385/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9977 - val_loss: 1.9199\n",
      "Epoch 386/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0028 - val_loss: 1.9198\n",
      "Epoch 387/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0004 - val_loss: 1.9198\n",
      "Epoch 388/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0209 - val_loss: 1.9199\n",
      "Epoch 389/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.9909 - val_loss: 1.9200\n",
      "Epoch 390/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0222 - val_loss: 1.9201\n",
      "Epoch 391/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9897 - val_loss: 1.9200\n",
      "Epoch 392/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0324 - val_loss: 1.9198\n",
      "Epoch 393/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9687 - val_loss: 1.9197\n",
      "Epoch 394/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9753 - val_loss: 1.9195\n",
      "Epoch 395/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9948 - val_loss: 1.9197\n",
      "Epoch 396/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0070 - val_loss: 1.9198\n",
      "Epoch 397/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0127 - val_loss: 1.9200\n",
      "Epoch 398/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0162 - val_loss: 1.9202\n",
      "Epoch 399/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0122 - val_loss: 1.9202\n",
      "Epoch 400/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.9792 - val_loss: 1.9203\n",
      "Epoch 401/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0156 - val_loss: 1.9203\n",
      "Epoch 402/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0343 - val_loss: 1.9204\n",
      "Epoch 403/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0191 - val_loss: 1.9204\n",
      "Epoch 404/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.9822 - val_loss: 1.9203\n",
      "Epoch 405/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9940 - val_loss: 1.9202\n",
      "Epoch 406/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9987 - val_loss: 1.9200\n",
      "Epoch 407/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.9969 - val_loss: 1.9198\n",
      "Epoch 408/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9852 - val_loss: 1.9195\n",
      "Epoch 409/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 2.0126 - val_loss: 1.9194\n",
      "Epoch 410/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9907 - val_loss: 1.9194\n",
      "Epoch 411/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0253 - val_loss: 1.9194\n",
      "Epoch 412/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 32ms/step - loss: 1.9865 - val_loss: 1.9193\n",
      "Epoch 413/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0005 - val_loss: 1.9193\n",
      "Epoch 414/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0128 - val_loss: 1.9192\n",
      "Epoch 415/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0319 - val_loss: 1.9191\n",
      "Epoch 416/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0103 - val_loss: 1.9189\n",
      "Epoch 417/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9987 - val_loss: 1.9188\n",
      "Epoch 418/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9884 - val_loss: 1.9187\n",
      "Epoch 419/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.9805 - val_loss: 1.9187\n",
      "Epoch 420/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0017 - val_loss: 1.9189\n",
      "Epoch 421/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9779 - val_loss: 1.9191\n",
      "Epoch 422/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0007 - val_loss: 1.9194\n",
      "Epoch 423/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0140 - val_loss: 1.9198\n",
      "Epoch 424/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.9913 - val_loss: 1.9198\n",
      "Epoch 425/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 2.0111 - val_loss: 1.9199\n",
      "Epoch 426/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0177 - val_loss: 1.9199\n",
      "Epoch 427/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0057 - val_loss: 1.9198\n",
      "Epoch 428/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0013 - val_loss: 1.9198\n",
      "Epoch 429/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0032 - val_loss: 1.9199\n",
      "Epoch 430/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0332 - val_loss: 1.9200\n",
      "Epoch 431/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9714 - val_loss: 1.9198\n",
      "Epoch 432/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0137 - val_loss: 1.9198\n",
      "Epoch 433/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0165 - val_loss: 1.9197\n",
      "Epoch 434/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9917 - val_loss: 1.9195\n",
      "Epoch 435/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0193 - val_loss: 1.9195\n",
      "Epoch 436/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0057 - val_loss: 1.9194\n",
      "Epoch 437/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0018 - val_loss: 1.9194\n",
      "Epoch 438/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9853 - val_loss: 1.9194\n",
      "Epoch 439/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9764 - val_loss: 1.9196\n",
      "Epoch 440/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.9837 - val_loss: 1.9197\n",
      "Epoch 441/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9982 - val_loss: 1.9196\n",
      "Epoch 442/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9948 - val_loss: 1.9194\n",
      "Epoch 443/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9945 - val_loss: 1.9194\n",
      "Epoch 444/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0106 - val_loss: 1.9194\n",
      "Epoch 445/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9887 - val_loss: 1.9194\n",
      "Epoch 446/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0128 - val_loss: 1.9195\n",
      "Epoch 447/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 1.9917 - val_loss: 1.9192\n",
      "Epoch 448/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 1.9770 - val_loss: 1.9190\n",
      "Epoch 449/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.9943 - val_loss: 1.9190\n",
      "Epoch 450/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.0106 - val_loss: 1.9190\n",
      "Epoch 451/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 2.0136 - val_loss: 1.9190\n",
      "Epoch 452/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0109 - val_loss: 1.9190\n",
      "Epoch 453/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.9842 - val_loss: 1.9190\n",
      "Epoch 454/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0023 - val_loss: 1.9191\n",
      "Epoch 455/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9839 - val_loss: 1.9192\n",
      "Epoch 456/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9843 - val_loss: 1.9193\n",
      "Epoch 457/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9997 - val_loss: 1.9192\n",
      "Epoch 458/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.9949 - val_loss: 1.9193\n",
      "Epoch 459/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0005 - val_loss: 1.9192\n",
      "Epoch 460/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.9985 - val_loss: 1.9191\n",
      "Epoch 461/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9984 - val_loss: 1.9192\n",
      "Epoch 462/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0328 - val_loss: 1.9194\n",
      "Epoch 463/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0112 - val_loss: 1.9197\n",
      "Epoch 464/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0088 - val_loss: 1.9197\n",
      "Epoch 465/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9964 - val_loss: 1.9197\n",
      "Epoch 466/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9925 - val_loss: 1.9196\n",
      "Epoch 467/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0216 - val_loss: 1.9197\n",
      "Epoch 468/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.9849 - val_loss: 1.9199\n",
      "Epoch 469/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9851 - val_loss: 1.9202\n",
      "Epoch 470/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 2.0064 - val_loss: 1.9204\n",
      "Epoch 471/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.0174 - val_loss: 1.9204\n",
      "Epoch 472/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0126 - val_loss: 1.9204\n",
      "Epoch 473/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0113 - val_loss: 1.9204\n",
      "Epoch 474/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.0319 - val_loss: 1.9207\n",
      "Epoch 475/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9738 - val_loss: 1.9205\n",
      "Epoch 476/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.0029 - val_loss: 1.9203\n",
      "Epoch 477/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9997 - val_loss: 1.9201\n",
      "Epoch 478/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9892 - val_loss: 1.9201\n",
      "Epoch 479/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.9963 - val_loss: 1.9200\n",
      "Epoch 480/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9958 - val_loss: 1.9201\n",
      "Epoch 481/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 2.0078 - val_loss: 1.9201\n",
      "Epoch 482/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.9949 - val_loss: 1.9203\n",
      "Epoch 483/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0256 - val_loss: 1.9202\n",
      "Epoch 484/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.9936 - val_loss: 1.9202\n",
      "Epoch 485/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9663 - val_loss: 1.9205\n",
      "Epoch 486/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.9661 - val_loss: 1.9207\n",
      "Epoch 487/500\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 2.0124 - val_loss: 1.9209\n",
      "Epoch 488/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.9902 - val_loss: 1.9206\n",
      "Epoch 489/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 1.9735 - val_loss: 1.9204\n",
      "Epoch 490/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.0238 - val_loss: 1.9204\n",
      "Epoch 491/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.0185 - val_loss: 1.9206\n",
      "Epoch 492/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 2.0013 - val_loss: 1.9205\n",
      "Epoch 493/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 2.0138 - val_loss: 1.9205\n",
      "Epoch 494/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 29ms/step - loss: 1.9831 - val_loss: 1.9206\n",
      "Epoch 495/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.9831 - val_loss: 1.9207\n",
      "Epoch 496/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.9747 - val_loss: 1.9210\n",
      "Epoch 497/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.9954 - val_loss: 1.9210\n",
      "Epoch 498/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0154 - val_loss: 1.9209\n",
      "Epoch 499/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.9871 - val_loss: 1.9206\n",
      "Epoch 500/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.9877 - val_loss: 1.9203\n"
     ]
    }
   ],
   "source": [
    "hist = sequence_autoencoder.fit(X_train, y_train,\n",
    "                epochs=500,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder'), mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.plot(hist.history['loss'], label='training')\n",
    "plt.plot(hist.history['val_loss'], label='testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'figures/{name}', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_autoencoder = load_model(f'Models/{name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4605609939759036"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sequence_autoencoder.predict(X_train).argmax(axis=-1)\n",
    "accuracy_score(y_train.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4962797619047619"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = sequence_autoencoder.predict(X_test).argmax(axis=-1)\n",
    "accuracy_score(y_test.argmax(-1).reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "train_encoded = encoder(X_train).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-b279aa5c1370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianMixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/kuacc/apps/anaconda/3.6/envs/root_env/lib/python3.7/site-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kuacc/apps/anaconda/3.6/envs/root_env/lib/python3.7/site-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdo_init\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mlower_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfty\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdo_init\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_bound_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kuacc/apps/anaconda/3.6/envs/root_env/lib/python3.7/site-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36m_initialize_parameters\u001b[0;34m(self, X, random_state)\u001b[0m\n\u001b[1;32m    154\u001b[0m                              % self.init_params)\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kuacc/apps/anaconda/3.6/envs/root_env/lib/python3.7/site-packages/sklearn/mixture/_gaussian_mixture.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, X, resp)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariances_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovariances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             self.precisions_cholesky_ = _compute_precision_cholesky(\n\u001b[0;32m--> 649\u001b[0;31m                 covariances, self.covariance_type)\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             self.precisions_cholesky_ = np.array(\n",
      "\u001b[0;32m/kuacc/apps/anaconda/3.6/envs/root_env/lib/python3.7/site-packages/sklearn/mixture/_gaussian_mixture.py\u001b[0m in \u001b[0;36m_compute_precision_cholesky\u001b[0;34m(covariances, covariance_type)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovariances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0mcov_chol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimate_precision_error_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kuacc/apps/anaconda/3.6/envs/root_env/lib/python3.7/site-packages/scipy/linalg/decomp_cholesky.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \"\"\"\n\u001b[1;32m     90\u001b[0m     c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n\u001b[0;32m---> 91\u001b[0;31m                          check_finite=check_finite)\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kuacc/apps/anaconda/3.6/envs/root_env/lib/python3.7/site-packages/scipy/linalg/decomp_cholesky.py\u001b[0m in \u001b[0;36m_cholesky\u001b[0;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"\"\"Common code for cholesky() and cho_factor().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray_chkfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \"\"\"\n\u001b[1;32m    483\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllFloat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         raise ValueError(\n\u001b[1;32m    486\u001b[0m             \"array must not contain infs or NaNs\")\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gm1 = GaussianMixture(n_components=5)\n",
    "gm1.fit(train_encoded.reshape(num_train, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples1 = gm1.sample(10)\n",
    "\n",
    "samples1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = samples1[0].reshape(10, 16, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = decoder(trials).numpy().argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = replace_with_dict(preds, v2n_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preds).to_csv(\"FirstOut.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"FirstOut.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Embedding-LSTM Encoder Embedding-LSTM-Dense Decoder Bassline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(minor_data, minor_data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = X_train.shape[0]\n",
    "\n",
    "num_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 64\n",
    "\n",
    "n2v_mapping['<bon/>'] = 36\n",
    "v2n_mapping = {value:key for key, value in n2v_mapping.items()}\n",
    "vocab_size = len(n2v_mapping)\n",
    "\n",
    "encoder_inputs_training = X_train\n",
    "decoder_inputs_training = np.ones((num_train, timesteps))\n",
    "decoder_inputs_training[:,0] *= n2v_mapping['<bon/>']\n",
    "decoder_inputs_training[:,1:] = X_train[:,:-1]\n",
    "#decoder_outputs_training = np.ones((num_train, timesteps))\n",
    "#decoder_outputs_training[:,-1] *= n2v_mapping['<bon/>']\n",
    "#decoder_outputs_training[:,:-1] = X_train\n",
    "#decoder_outputs_training = one_hot_encode(decoder_outputs_training, vocab_size)\n",
    "decoder_outputs_training = one_hot_encode(X_train, vocab_size)\n",
    "\n",
    "encoder_inputs_testing = X_test\n",
    "decoder_inputs_testing = np.ones((num_test, timesteps))\n",
    "decoder_inputs_testing[:,0] *= n2v_mapping['<bon/>']\n",
    "decoder_inputs_testing[:,1:] = X_test[:,:-1]\n",
    "#decoder_outputs_testing = np.ones((num_test, timesteps))\n",
    "#decoder_outputs_testing[:,-1] *= n2v_mapping['<bon/>']\n",
    "#decoder_outputs_testing[:,:-1] = X_test[:,1:]\n",
    "#decoder_outputs_testing = one_hot_encode(decoder_outputs_testing, vocab_size)\n",
    "decoder_outputs_testing = one_hot_encode(X_test, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187, 64, 37)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NBG_lm_lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 32\n",
    "latent_dim = 256\n",
    "dropout = 0.\n",
    "\n",
    "encoder_inputs = Input(shape=(timesteps,))\n",
    "embedding = Embedding(vocab_size, embed_size)\n",
    "encoder_embedding_outputs = embedding(encoder_inputs)\n",
    "encoder = LSTM(latent_dim, return_state=True, dropout=dropout)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embedding_outputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(timesteps,))\n",
    "decoder_embedding_outputs = embedding(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=dropout)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding_outputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 64, 32)       1184        input_11[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, 256), (None, 295936      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   [(None, 64, 256), (N 295936      embedding_5[1][0]                \n",
      "                                                                 lstm_7[0][1]                     \n",
      "                                                                 lstm_7[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64, 37)       9509        lstm_8[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 602,565\n",
      "Trainable params: 602,565\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "6/6 [==============================] - 3s 201ms/step - loss: 3.0739 - val_loss: 2.1161\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 2.0140 - val_loss: 2.0283\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.9644 - val_loss: 2.0317\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.9497 - val_loss: 2.0160\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 1.9399 - val_loss: 2.0019\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.9665 - val_loss: 1.9890\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.9329 - val_loss: 1.9802\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.8718 - val_loss: 1.9814\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.8368 - val_loss: 1.9580\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.8554 - val_loss: 1.9239\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.8040 - val_loss: 1.9106\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.8361 - val_loss: 1.9091\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.8029 - val_loss: 1.9066\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.8354 - val_loss: 1.9059\n",
      "Epoch 15/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.7973 - val_loss: 1.8903\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.7591 - val_loss: 1.8848\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.7490 - val_loss: 1.8931\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.7847 - val_loss: 1.8759\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.7593 - val_loss: 1.8678\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.7446 - val_loss: 1.8586\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.7255 - val_loss: 1.8394\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.6857 - val_loss: 1.8229\n",
      "Epoch 23/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.6838 - val_loss: 1.8027\n",
      "Epoch 24/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.7043 - val_loss: 1.7828\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.6953 - val_loss: 1.7737\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.6278 - val_loss: 1.7748\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 1.6586 - val_loss: 1.7386\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.5800 - val_loss: 1.7157\n",
      "Epoch 29/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.5917 - val_loss: 1.7101\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.5849 - val_loss: 1.6872\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.5309 - val_loss: 1.6873\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.5044 - val_loss: 1.6840\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.4877 - val_loss: 1.6757\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.5078 - val_loss: 1.6642\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 1.4808 - val_loss: 1.6747\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.4465 - val_loss: 1.6635\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.4004 - val_loss: 1.6618\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.4349 - val_loss: 1.6411\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.4185 - val_loss: 1.6529\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.3832 - val_loss: 1.6337\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.3566 - val_loss: 1.6206\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 1.3219 - val_loss: 1.6391\n",
      "Epoch 43/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2835 - val_loss: 1.6296\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.2635 - val_loss: 1.6264\n",
      "Epoch 45/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.2755 - val_loss: 1.6384\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.2512 - val_loss: 1.6370\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2025 - val_loss: 1.6700\n",
      "Epoch 48/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.2073 - val_loss: 1.6472\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.1957 - val_loss: 1.6546\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.1388 - val_loss: 1.6712\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 1.1366 - val_loss: 1.6755\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 1.0965 - val_loss: 1.7031\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.0535 - val_loss: 1.7007\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.0388 - val_loss: 1.7236\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 1.0637 - val_loss: 1.7055\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.9926 - val_loss: 1.7621\n",
      "Epoch 57/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 1.0072 - val_loss: 1.7566\n",
      "Epoch 58/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.9554 - val_loss: 1.7831\n",
      "Epoch 59/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.9099 - val_loss: 1.7746\n",
      "Epoch 60/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8725 - val_loss: 1.8177\n",
      "Epoch 61/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.8657 - val_loss: 1.8513\n",
      "Epoch 62/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.7992 - val_loss: 1.8414\n",
      "Epoch 63/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.7987 - val_loss: 1.8994\n",
      "Epoch 64/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.7626 - val_loss: 1.8970\n",
      "Epoch 65/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.8060 - val_loss: 1.9126\n",
      "Epoch 66/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.7637 - val_loss: 1.9454\n",
      "Epoch 67/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.7174 - val_loss: 1.9612\n",
      "Epoch 68/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.6693 - val_loss: 1.9804\n",
      "Epoch 69/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.6282 - val_loss: 1.9935\n",
      "Epoch 70/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.5877 - val_loss: 2.0651\n",
      "Epoch 71/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.5663 - val_loss: 2.0957\n",
      "Epoch 72/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.5243 - val_loss: 2.1077\n",
      "Epoch 73/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.4982 - val_loss: 2.1214\n",
      "Epoch 74/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.4724 - val_loss: 2.2075\n",
      "Epoch 75/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.4667 - val_loss: 2.1915\n",
      "Epoch 76/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.4210 - val_loss: 2.2636\n",
      "Epoch 77/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.4013 - val_loss: 2.2614\n",
      "Epoch 78/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.3601 - val_loss: 2.3215\n",
      "Epoch 79/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.3379 - val_loss: 2.3138\n",
      "Epoch 80/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.3259 - val_loss: 2.4104\n",
      "Epoch 81/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.2886 - val_loss: 2.4521\n",
      "Epoch 82/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.2880 - val_loss: 2.4813\n",
      "Epoch 83/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.2919 - val_loss: 2.4942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.3508 - val_loss: 2.4808\n",
      "Epoch 85/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.3488 - val_loss: 2.4789\n",
      "Epoch 86/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.3067 - val_loss: 2.5401\n",
      "Epoch 87/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.2897 - val_loss: 2.5186\n",
      "Epoch 88/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.2486 - val_loss: 2.5816\n",
      "Epoch 89/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.2253 - val_loss: 2.6984\n",
      "Epoch 90/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1963 - val_loss: 2.6665\n",
      "Epoch 91/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.2139 - val_loss: 2.6901\n",
      "Epoch 92/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.1873 - val_loss: 2.7058\n",
      "Epoch 93/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1619 - val_loss: 2.7502\n",
      "Epoch 94/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1597 - val_loss: 2.7785\n",
      "Epoch 95/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.1436 - val_loss: 2.7806\n",
      "Epoch 96/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.1580 - val_loss: 2.8869\n",
      "Epoch 97/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.1671 - val_loss: 2.8759\n",
      "Epoch 98/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.1642 - val_loss: 2.8611\n",
      "Epoch 99/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1569 - val_loss: 2.8992\n",
      "Epoch 100/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1594 - val_loss: 2.9279\n",
      "Epoch 101/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.1331 - val_loss: 2.9628\n",
      "Epoch 102/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1204 - val_loss: 3.0025\n",
      "Epoch 103/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.1029 - val_loss: 2.9826\n",
      "Epoch 104/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0819 - val_loss: 3.0464\n",
      "Epoch 105/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0755 - val_loss: 3.0424\n",
      "Epoch 106/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0607 - val_loss: 3.0821\n",
      "Epoch 107/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0545 - val_loss: 3.1407\n",
      "Epoch 108/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0452 - val_loss: 3.1827\n",
      "Epoch 109/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0439 - val_loss: 3.1983\n",
      "Epoch 110/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0386 - val_loss: 3.2295\n",
      "Epoch 111/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0363 - val_loss: 3.2488\n",
      "Epoch 112/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0358 - val_loss: 3.2825\n",
      "Epoch 113/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0357 - val_loss: 3.3170\n",
      "Epoch 114/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0329 - val_loss: 3.3225\n",
      "Epoch 115/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0294 - val_loss: 3.3534\n",
      "Epoch 116/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0284 - val_loss: 3.3464\n",
      "Epoch 117/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.0260 - val_loss: 3.3770\n",
      "Epoch 118/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0249 - val_loss: 3.3986\n",
      "Epoch 119/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0235 - val_loss: 3.4216\n",
      "Epoch 120/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0216 - val_loss: 3.4350\n",
      "Epoch 121/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0206 - val_loss: 3.4644\n",
      "Epoch 122/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0206 - val_loss: 3.4757\n",
      "Epoch 123/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0208 - val_loss: 3.5082\n",
      "Epoch 124/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0204 - val_loss: 3.4922\n",
      "Epoch 125/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0210 - val_loss: 3.5121\n",
      "Epoch 126/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0199 - val_loss: 3.5495\n",
      "Epoch 127/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0248 - val_loss: 3.5077\n",
      "Epoch 128/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0239 - val_loss: 3.5554\n",
      "Epoch 129/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0255 - val_loss: 3.4980\n",
      "Epoch 130/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0239 - val_loss: 3.5442\n",
      "Epoch 131/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0219 - val_loss: 3.5822\n",
      "Epoch 132/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0495 - val_loss: 3.5722\n",
      "Epoch 133/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0671 - val_loss: 3.4027\n",
      "Epoch 134/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0615 - val_loss: 3.4192\n",
      "Epoch 135/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0676 - val_loss: 3.5650\n",
      "Epoch 136/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0558 - val_loss: 3.4291\n",
      "Epoch 137/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1339 - val_loss: 3.4103\n",
      "Epoch 138/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.2783 - val_loss: 3.4635\n",
      "Epoch 139/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.4127 - val_loss: 3.0752\n",
      "Epoch 140/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.4589 - val_loss: 3.0339\n",
      "Epoch 141/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.4008 - val_loss: 3.0446\n",
      "Epoch 142/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.3456 - val_loss: 2.9605\n",
      "Epoch 143/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.2624 - val_loss: 3.0169\n",
      "Epoch 144/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.1827 - val_loss: 3.1344\n",
      "Epoch 145/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.1371 - val_loss: 3.1007\n",
      "Epoch 146/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0984 - val_loss: 3.1898\n",
      "Epoch 147/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0773 - val_loss: 3.2307\n",
      "Epoch 148/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0600 - val_loss: 3.2630\n",
      "Epoch 149/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0500 - val_loss: 3.3723\n",
      "Epoch 150/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0408 - val_loss: 3.4037\n",
      "Epoch 151/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0357 - val_loss: 3.4304\n",
      "Epoch 152/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0305 - val_loss: 3.4541\n",
      "Epoch 153/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0337 - val_loss: 3.4864\n",
      "Epoch 154/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0380 - val_loss: 3.5250\n",
      "Epoch 155/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0361 - val_loss: 3.5196\n",
      "Epoch 156/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0329 - val_loss: 3.5841\n",
      "Epoch 157/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0303 - val_loss: 3.6058\n",
      "Epoch 158/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0268 - val_loss: 3.5810\n",
      "Epoch 159/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0223 - val_loss: 3.5897\n",
      "Epoch 160/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0204 - val_loss: 3.6239\n",
      "Epoch 161/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0187 - val_loss: 3.6438\n",
      "Epoch 162/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0173 - val_loss: 3.6701\n",
      "Epoch 163/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0166 - val_loss: 3.6785\n",
      "Epoch 164/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0151 - val_loss: 3.6818\n",
      "Epoch 165/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0136 - val_loss: 3.7092\n",
      "Epoch 166/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0133 - val_loss: 3.7180\n",
      "Epoch 167/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0133 - val_loss: 3.7354\n",
      "Epoch 168/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0126 - val_loss: 3.7554\n",
      "Epoch 169/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0116 - val_loss: 3.7701\n",
      "Epoch 170/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0115 - val_loss: 3.7468\n",
      "Epoch 171/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0111 - val_loss: 3.7645\n",
      "Epoch 172/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0105 - val_loss: 3.7942\n",
      "Epoch 173/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0112 - val_loss: 3.8126\n",
      "Epoch 174/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0119 - val_loss: 3.8015\n",
      "Epoch 175/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0115 - val_loss: 3.8049\n",
      "Epoch 176/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0107 - val_loss: 3.8193\n",
      "Epoch 177/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0102 - val_loss: 3.8282\n",
      "Epoch 178/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0096 - val_loss: 3.8400\n",
      "Epoch 179/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0083 - val_loss: 3.8351\n",
      "Epoch 180/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0080 - val_loss: 3.8462\n",
      "Epoch 181/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0078 - val_loss: 3.8543\n",
      "Epoch 182/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0074 - val_loss: 3.8648\n",
      "Epoch 183/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0075 - val_loss: 3.8723\n",
      "Epoch 184/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0077 - val_loss: 3.8764\n",
      "Epoch 185/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0081 - val_loss: 3.8941\n",
      "Epoch 186/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0072 - val_loss: 3.8871\n",
      "Epoch 187/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0066 - val_loss: 3.8875\n",
      "Epoch 188/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0064 - val_loss: 3.8927\n",
      "Epoch 189/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0061 - val_loss: 3.8991\n",
      "Epoch 190/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0065 - val_loss: 3.9072\n",
      "Epoch 191/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0059 - val_loss: 3.9238\n",
      "Epoch 192/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0059 - val_loss: 3.9320\n",
      "Epoch 193/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0061 - val_loss: 3.9429\n",
      "Epoch 194/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0062 - val_loss: 3.9326\n",
      "Epoch 195/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0051 - val_loss: 3.9312\n",
      "Epoch 196/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0055 - val_loss: 3.9358\n",
      "Epoch 197/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0050 - val_loss: 3.9424\n",
      "Epoch 198/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.0056 - val_loss: 3.9542\n",
      "Epoch 199/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0049 - val_loss: 3.9697\n",
      "Epoch 200/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0052 - val_loss: 3.9801\n",
      "Epoch 201/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0050 - val_loss: 3.9843\n",
      "Epoch 202/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0049 - val_loss: 3.9872\n",
      "Epoch 203/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0047 - val_loss: 3.9933\n",
      "Epoch 204/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0054 - val_loss: 4.0039\n",
      "Epoch 205/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0064 - val_loss: 4.0342\n",
      "Epoch 206/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0070 - val_loss: 4.0053\n",
      "Epoch 207/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0061 - val_loss: 4.0062\n",
      "Epoch 208/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0078 - val_loss: 4.0231\n",
      "Epoch 209/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0097 - val_loss: 4.0157\n",
      "Epoch 210/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0209 - val_loss: 4.0464\n",
      "Epoch 211/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0173 - val_loss: 4.0461\n",
      "Epoch 212/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0227 - val_loss: 4.0189\n",
      "Epoch 213/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0228 - val_loss: 4.0460\n",
      "Epoch 214/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0150 - val_loss: 4.0188\n",
      "Epoch 215/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0166 - val_loss: 3.9668\n",
      "Epoch 216/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0142 - val_loss: 4.0178\n",
      "Epoch 217/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0154 - val_loss: 4.0031\n",
      "Epoch 218/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0120 - val_loss: 3.9599\n",
      "Epoch 219/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0112 - val_loss: 3.9796\n",
      "Epoch 220/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0083 - val_loss: 4.0159\n",
      "Epoch 221/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0075 - val_loss: 4.0116\n",
      "Epoch 222/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0071 - val_loss: 3.9997\n",
      "Epoch 223/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0067 - val_loss: 4.0313\n",
      "Epoch 224/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0065 - val_loss: 4.0429\n",
      "Epoch 225/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.0060 - val_loss: 4.0446\n",
      "Epoch 226/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0059 - val_loss: 4.0513\n",
      "Epoch 227/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0055 - val_loss: 4.0515\n",
      "Epoch 228/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.0051 - val_loss: 4.0691\n",
      "Epoch 229/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.0047 - val_loss: 4.0922\n",
      "Epoch 230/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0042 - val_loss: 4.1101\n",
      "Epoch 231/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0045 - val_loss: 4.1139\n",
      "Epoch 232/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0039 - val_loss: 4.1177\n",
      "Epoch 233/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0043 - val_loss: 4.1158\n",
      "Epoch 234/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0040 - val_loss: 4.1176\n",
      "Epoch 235/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0034 - val_loss: 4.1279\n",
      "Epoch 236/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0035 - val_loss: 4.1322\n",
      "Epoch 237/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 4.1397\n",
      "Epoch 238/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0033 - val_loss: 4.1383\n",
      "Epoch 239/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0044 - val_loss: 4.1549\n",
      "Epoch 240/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0035 - val_loss: 4.1628\n",
      "Epoch 241/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0038 - val_loss: 4.1677\n",
      "Epoch 242/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0050 - val_loss: 4.1663\n",
      "Epoch 243/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0048 - val_loss: 4.1802\n",
      "Epoch 244/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0065 - val_loss: 4.1850\n",
      "Epoch 245/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0072 - val_loss: 4.1840\n",
      "Epoch 246/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0063 - val_loss: 4.1769\n",
      "Epoch 247/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0073 - val_loss: 4.1349\n",
      "Epoch 248/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0077 - val_loss: 4.1007\n",
      "Epoch 249/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0082 - val_loss: 4.1103\n",
      "Epoch 250/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0070 - val_loss: 4.1228\n",
      "Epoch 251/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0075 - val_loss: 4.1444\n",
      "Epoch 252/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.0072 - val_loss: 4.1220\n",
      "Epoch 253/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0075 - val_loss: 4.1399\n",
      "Epoch 254/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0062 - val_loss: 4.1555\n",
      "Epoch 255/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0048 - val_loss: 4.1631\n",
      "Epoch 256/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0048 - val_loss: 4.1883\n",
      "Epoch 257/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0054 - val_loss: 4.1784\n",
      "Epoch 258/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0048 - val_loss: 4.1586\n",
      "Epoch 259/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0037 - val_loss: 4.1515\n",
      "Epoch 260/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0032 - val_loss: 4.1590\n",
      "Epoch 261/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0034 - val_loss: 4.1734\n",
      "Epoch 262/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0030 - val_loss: 4.1785\n",
      "Epoch 263/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0034 - val_loss: 4.1799\n",
      "Epoch 264/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0030 - val_loss: 4.1835\n",
      "Epoch 265/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0026 - val_loss: 4.1850\n",
      "Epoch 266/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0025 - val_loss: 4.1852\n",
      "Epoch 267/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0026 - val_loss: 4.1867\n",
      "Epoch 268/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 4.1971\n",
      "Epoch 269/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0027 - val_loss: 4.2037\n",
      "Epoch 270/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0022 - val_loss: 4.2061\n",
      "Epoch 271/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0020 - val_loss: 4.2069\n",
      "Epoch 272/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0022 - val_loss: 4.2088\n",
      "Epoch 273/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0021 - val_loss: 4.2143\n",
      "Epoch 274/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0021 - val_loss: 4.2195\n",
      "Epoch 275/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0019 - val_loss: 4.2244\n",
      "Epoch 276/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.0019 - val_loss: 4.2295\n",
      "Epoch 277/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0019 - val_loss: 4.2340\n",
      "Epoch 278/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0018 - val_loss: 4.2362\n",
      "Epoch 279/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0019 - val_loss: 4.2412\n",
      "Epoch 280/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0018 - val_loss: 4.2454\n",
      "Epoch 281/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0018 - val_loss: 4.2509\n",
      "Epoch 282/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0017 - val_loss: 4.2594\n",
      "Epoch 283/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0019 - val_loss: 4.2622\n",
      "Epoch 284/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0021 - val_loss: 4.2833\n",
      "Epoch 285/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0019 - val_loss: 4.2780\n",
      "Epoch 286/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0016 - val_loss: 4.2810\n",
      "Epoch 287/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0017 - val_loss: 4.2820\n",
      "Epoch 288/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0016 - val_loss: 4.2844\n",
      "Epoch 289/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.0015 - val_loss: 4.2908\n",
      "Epoch 290/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.0018 - val_loss: 4.2941\n",
      "Epoch 291/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0016 - val_loss: 4.2952\n",
      "Epoch 292/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.0025 - val_loss: 4.3056\n",
      "Epoch 293/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0030 - val_loss: 4.3222\n",
      "Epoch 294/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0027 - val_loss: 4.3139\n",
      "Epoch 295/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0034 - val_loss: 4.3323\n",
      "Epoch 296/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0033 - val_loss: 4.3326\n",
      "Epoch 297/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0037 - val_loss: 4.3310\n",
      "Epoch 298/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0030 - val_loss: 4.3122\n",
      "Epoch 299/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0036 - val_loss: 4.3149\n",
      "Epoch 300/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0030 - val_loss: 4.3679\n",
      "Epoch 301/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0031 - val_loss: 4.3525\n",
      "Epoch 302/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0031 - val_loss: 4.3422\n",
      "Epoch 303/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0029 - val_loss: 4.3468\n",
      "Epoch 304/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0039 - val_loss: 4.3776\n",
      "Epoch 305/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.0039 - val_loss: 4.3787\n",
      "Epoch 306/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0041 - val_loss: 4.3588\n",
      "Epoch 307/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0047 - val_loss: 4.3705\n",
      "Epoch 308/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0038 - val_loss: 4.3608\n",
      "Epoch 309/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0025 - val_loss: 4.4004\n",
      "Epoch 310/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0026 - val_loss: 4.4083\n",
      "Epoch 311/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0027 - val_loss: 4.3892\n",
      "Epoch 312/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0024 - val_loss: 4.3705\n",
      "Epoch 313/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.1483 - val_loss: 4.2413\n",
      "Epoch 314/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.7145 - val_loss: 3.5265\n",
      "Epoch 315/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.9059 - val_loss: 3.2756\n",
      "Epoch 316/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.9572 - val_loss: 3.0264\n",
      "Epoch 317/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.0223 - val_loss: 2.8340\n",
      "Epoch 318/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.8730 - val_loss: 2.8103\n",
      "Epoch 319/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.7131 - val_loss: 2.6403\n",
      "Epoch 320/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.5621 - val_loss: 2.6689\n",
      "Epoch 321/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.4665 - val_loss: 2.6358\n",
      "Epoch 322/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.3689 - val_loss: 2.7085\n",
      "Epoch 323/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.3021 - val_loss: 2.7131\n",
      "Epoch 324/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.2427 - val_loss: 2.7362\n",
      "Epoch 325/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.1862 - val_loss: 2.8089\n",
      "Epoch 326/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.1590 - val_loss: 2.8546\n",
      "Epoch 327/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.1258 - val_loss: 2.8905\n",
      "Epoch 328/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.1029 - val_loss: 2.9398\n",
      "Epoch 329/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0844 - val_loss: 2.9931\n",
      "Epoch 330/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0757 - val_loss: 3.0498\n",
      "Epoch 331/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0660 - val_loss: 3.0712\n",
      "Epoch 332/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0572 - val_loss: 3.1072\n",
      "Epoch 333/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0509 - val_loss: 3.1423\n",
      "Epoch 334/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0448 - val_loss: 3.1702\n",
      "Epoch 335/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0396 - val_loss: 3.1957\n",
      "Epoch 336/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0369 - val_loss: 3.2234\n",
      "Epoch 337/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0344 - val_loss: 3.2476\n",
      "Epoch 338/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0323 - val_loss: 3.2820\n",
      "Epoch 339/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0308 - val_loss: 3.2849\n",
      "Epoch 340/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0289 - val_loss: 3.2929\n",
      "Epoch 341/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0275 - val_loss: 3.3191\n",
      "Epoch 342/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0255 - val_loss: 3.3562\n",
      "Epoch 343/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0230 - val_loss: 3.3621\n",
      "Epoch 344/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0222 - val_loss: 3.3696\n",
      "Epoch 345/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0203 - val_loss: 3.3986\n",
      "Epoch 346/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0185 - val_loss: 3.4165\n",
      "Epoch 347/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0177 - val_loss: 3.4280\n",
      "Epoch 348/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0175 - val_loss: 3.4322\n",
      "Epoch 349/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0157 - val_loss: 3.4524\n",
      "Epoch 350/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0147 - val_loss: 3.4757\n",
      "Epoch 351/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0147 - val_loss: 3.4837\n",
      "Epoch 352/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0141 - val_loss: 3.4851\n",
      "Epoch 353/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0139 - val_loss: 3.5019\n",
      "Epoch 354/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0131 - val_loss: 3.5184\n",
      "Epoch 355/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0125 - val_loss: 3.5248\n",
      "Epoch 356/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0129 - val_loss: 3.5294\n",
      "Epoch 357/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0123 - val_loss: 3.5448\n",
      "Epoch 358/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0128 - val_loss: 3.5529\n",
      "Epoch 359/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0145 - val_loss: 3.5694\n",
      "Epoch 360/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0125 - val_loss: 3.5730\n",
      "Epoch 361/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0118 - val_loss: 3.5816\n",
      "Epoch 362/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0120 - val_loss: 3.6114\n",
      "Epoch 363/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0116 - val_loss: 3.6252\n",
      "Epoch 364/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0111 - val_loss: 3.6505\n",
      "Epoch 365/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0109 - val_loss: 3.6678\n",
      "Epoch 366/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0111 - val_loss: 3.6525\n",
      "Epoch 367/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0144 - val_loss: 3.6409\n",
      "Epoch 368/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0128 - val_loss: 3.6417\n",
      "Epoch 369/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0119 - val_loss: 3.6341\n",
      "Epoch 370/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0110 - val_loss: 3.6385\n",
      "Epoch 371/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0110 - val_loss: 3.6453\n",
      "Epoch 372/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0112 - val_loss: 3.6527\n",
      "Epoch 373/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0105 - val_loss: 3.6686\n",
      "Epoch 374/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0103 - val_loss: 3.6779\n",
      "Epoch 375/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0121 - val_loss: 3.6767\n",
      "Epoch 376/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0146 - val_loss: 3.6628\n",
      "Epoch 377/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0143 - val_loss: 3.6990\n",
      "Epoch 378/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0125 - val_loss: 3.7113\n",
      "Epoch 379/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0113 - val_loss: 3.7061\n",
      "Epoch 380/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0094 - val_loss: 3.7346\n",
      "Epoch 381/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0090 - val_loss: 3.7429\n",
      "Epoch 382/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0081 - val_loss: 3.7390\n",
      "Epoch 383/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0073 - val_loss: 3.7577\n",
      "Epoch 384/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0067 - val_loss: 3.7724\n",
      "Epoch 385/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0065 - val_loss: 3.7731\n",
      "Epoch 386/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0061 - val_loss: 3.7809\n",
      "Epoch 387/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0058 - val_loss: 3.7961\n",
      "Epoch 388/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0055 - val_loss: 3.8108\n",
      "Epoch 389/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0054 - val_loss: 3.8147\n",
      "Epoch 390/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0053 - val_loss: 3.8145\n",
      "Epoch 391/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.0054 - val_loss: 3.8203\n",
      "Epoch 392/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0055 - val_loss: 3.8334\n",
      "Epoch 393/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0054 - val_loss: 3.8327\n",
      "Epoch 394/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0049 - val_loss: 3.8242\n",
      "Epoch 395/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0047 - val_loss: 3.8251\n",
      "Epoch 396/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0044 - val_loss: 3.8336\n",
      "Epoch 397/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0053 - val_loss: 3.8466\n",
      "Epoch 398/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0048 - val_loss: 3.8467\n",
      "Epoch 399/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0049 - val_loss: 3.8582\n",
      "Epoch 400/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0046 - val_loss: 3.8720\n",
      "Epoch 401/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0045 - val_loss: 3.8717\n",
      "Epoch 402/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0045 - val_loss: 3.8732\n",
      "Epoch 403/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0047 - val_loss: 3.8745\n",
      "Epoch 404/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0043 - val_loss: 3.8848\n",
      "Epoch 405/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0040 - val_loss: 3.8875\n",
      "Epoch 406/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0039 - val_loss: 3.8898\n",
      "Epoch 407/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0040 - val_loss: 3.8984\n",
      "Epoch 408/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0037 - val_loss: 3.9087\n",
      "Epoch 409/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0039 - val_loss: 3.9171\n",
      "Epoch 410/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0043 - val_loss: 3.9148\n",
      "Epoch 411/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0041 - val_loss: 3.9134\n",
      "Epoch 412/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0040 - val_loss: 3.9173\n",
      "Epoch 413/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0042 - val_loss: 3.9246\n",
      "Epoch 414/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0039 - val_loss: 3.9222\n",
      "Epoch 415/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0042 - val_loss: 3.9264\n",
      "Epoch 416/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0038 - val_loss: 3.9381\n",
      "Epoch 417/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0039 - val_loss: 3.9515\n",
      "Epoch 418/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0037 - val_loss: 3.9487\n",
      "Epoch 419/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0032 - val_loss: 3.9569\n",
      "Epoch 420/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0036 - val_loss: 3.9646\n",
      "Epoch 421/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0035 - val_loss: 3.9627\n",
      "Epoch 422/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0031 - val_loss: 3.9550\n",
      "Epoch 423/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 3.9611\n",
      "Epoch 424/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0029 - val_loss: 3.9697\n",
      "Epoch 425/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0029 - val_loss: 3.9746\n",
      "Epoch 426/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0027 - val_loss: 3.9756\n",
      "Epoch 427/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0026 - val_loss: 3.9790\n",
      "Epoch 428/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0026 - val_loss: 3.9838\n",
      "Epoch 429/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0025 - val_loss: 3.9892\n",
      "Epoch 430/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0028 - val_loss: 3.9960\n",
      "Epoch 431/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0035 - val_loss: 4.0022\n",
      "Epoch 432/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0026 - val_loss: 4.0123\n",
      "Epoch 433/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0025 - val_loss: 4.0150\n",
      "Epoch 434/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0026 - val_loss: 4.0180\n",
      "Epoch 435/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0029 - val_loss: 4.0174\n",
      "Epoch 436/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0026 - val_loss: 4.0228\n",
      "Epoch 437/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0025 - val_loss: 4.0312\n",
      "Epoch 438/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0026 - val_loss: 4.0349\n",
      "Epoch 439/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0024 - val_loss: 4.0348\n",
      "Epoch 440/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0023 - val_loss: 4.0402\n",
      "Epoch 441/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0034 - val_loss: 4.0438\n",
      "Epoch 442/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0032 - val_loss: 4.0488\n",
      "Epoch 443/500\n",
      "6/6 [==============================] - 0s 42ms/step - loss: 0.0030 - val_loss: 4.0577\n",
      "Epoch 444/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0022 - val_loss: 4.0501\n",
      "Epoch 445/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0022 - val_loss: 4.0636\n",
      "Epoch 446/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0025 - val_loss: 4.0679\n",
      "Epoch 447/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0026 - val_loss: 4.0686\n",
      "Epoch 448/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0022 - val_loss: 4.0766\n",
      "Epoch 449/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0022 - val_loss: 4.0804\n",
      "Epoch 450/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0022 - val_loss: 4.0868\n",
      "Epoch 451/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0023 - val_loss: 4.0970\n",
      "Epoch 452/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0037 - val_loss: 4.1002\n",
      "Epoch 453/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0028 - val_loss: 4.0948\n",
      "Epoch 454/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0025 - val_loss: 4.0960\n",
      "Epoch 455/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0026 - val_loss: 4.1264\n",
      "Epoch 456/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0085 - val_loss: 4.1404\n",
      "Epoch 457/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0115 - val_loss: 4.0945\n",
      "Epoch 458/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0508 - val_loss: 4.0768\n",
      "Epoch 459/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.1665 - val_loss: 3.9308\n",
      "Epoch 460/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.2616 - val_loss: 3.8656\n",
      "Epoch 461/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.2937 - val_loss: 3.9314\n",
      "Epoch 462/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.3059 - val_loss: 3.6120\n",
      "Epoch 463/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.2415 - val_loss: 3.6201\n",
      "Epoch 464/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.2053 - val_loss: 3.6603\n",
      "Epoch 465/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.1581 - val_loss: 3.6453\n",
      "Epoch 466/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.1050 - val_loss: 3.7208\n",
      "Epoch 467/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0788 - val_loss: 3.7499\n",
      "Epoch 468/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0572 - val_loss: 3.7615\n",
      "Epoch 469/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0385 - val_loss: 3.8551\n",
      "Epoch 470/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0287 - val_loss: 3.8597\n",
      "Epoch 471/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0229 - val_loss: 3.8699\n",
      "Epoch 472/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0191 - val_loss: 3.8901\n",
      "Epoch 473/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0158 - val_loss: 3.9212\n",
      "Epoch 474/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0139 - val_loss: 3.9413\n",
      "Epoch 475/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0118 - val_loss: 3.9544\n",
      "Epoch 476/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0107 - val_loss: 3.9772\n",
      "Epoch 477/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.0098 - val_loss: 4.0010\n",
      "Epoch 478/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0089 - val_loss: 4.0164\n",
      "Epoch 479/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0082 - val_loss: 4.0266\n",
      "Epoch 480/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0077 - val_loss: 4.0262\n",
      "Epoch 481/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0081 - val_loss: 4.0351\n",
      "Epoch 482/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0073 - val_loss: 4.0564\n",
      "Epoch 483/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0076 - val_loss: 4.0570\n",
      "Epoch 484/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0070 - val_loss: 4.0614\n",
      "Epoch 485/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0062 - val_loss: 4.0699\n",
      "Epoch 486/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0063 - val_loss: 4.0733\n",
      "Epoch 487/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0064 - val_loss: 4.0781\n",
      "Epoch 488/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0059 - val_loss: 4.0979\n",
      "Epoch 489/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0054 - val_loss: 4.1062\n",
      "Epoch 490/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0060 - val_loss: 4.1071\n",
      "Epoch 491/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.0055 - val_loss: 4.1176\n",
      "Epoch 492/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.0049 - val_loss: 4.1174\n",
      "Epoch 493/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0048 - val_loss: 4.1240\n",
      "Epoch 494/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0054 - val_loss: 4.1308\n",
      "Epoch 495/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0046 - val_loss: 4.1382\n",
      "Epoch 496/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0048 - val_loss: 4.1430\n",
      "Epoch 497/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0042 - val_loss: 4.1541\n",
      "Epoch 498/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.0043 - val_loss: 4.1579\n",
      "Epoch 499/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0043 - val_loss: 4.1661\n",
      "Epoch 500/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0050 - val_loss: 4.1816\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "\n",
    "mc = ModelCheckpoint(f'Models/{name}.hdf5', monitor='val_loss')\n",
    "\n",
    "optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "model.compile(optimizer, loss='categorical_crossentropy')\n",
    "hist = model.fit([encoder_inputs_training, decoder_inputs_training], decoder_outputs_training,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 validation_data=([encoder_inputs_testing, decoder_inputs_testing], decoder_outputs_testing),\n",
    "                 callbacks=[mc]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.plot(hist.history['loss'], label='training')\n",
    "plt.plot(hist.history['val_loss'], label='testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'figures/{name}', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding_outputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 64, 32)            1184      \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                [(None, 256), (None, 256) 295936    \n",
      "=================================================================\n",
      "Total params: 297,120\n",
      "Trainable params: 297,120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 64, 32)       1184        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   [(None, 64, 256), (N 295936      embedding_5[1][0]                \n",
      "                                                                 input_13[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64, 37)       9509        lstm_8[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 306,629\n",
      "Trainable params: 306,629\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, 64), dtype=tf.float32, name='input_12'), name='input_12', description=\"created by layer 'input_12'\"), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    }
   ],
   "source": [
    "results = np.zeros((num_train, timesteps))\n",
    "train_encoded = encoder_model.predict(encoder_inputs_training[:,:,np.newaxis])\n",
    "train_inputs = np.ones((num_train, timesteps + 1)) * n2v_mapping['<bon/>']\n",
    "for i in range(timesteps):\n",
    "    decoded_results = decoder_model.predict([train_inputs[:,i+1].reshape(-1, 1)] + train_encoded)\n",
    "    train_encoded = decoded_results[1:]\n",
    "    #preds = decoded_results[0].argmax(-1)\n",
    "    train_inputs[:,i+1] = decoded_results[0].argmax(-1).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3428308823529412"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(encoder_inputs_training.reshape(-1), train_inputs[:,1:].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((num_test, timesteps))\n",
    "test_encoded = encoder_model.predict(encoder_inputs_testing[:,:,np.newaxis])\n",
    "test_inputs = np.ones((num_test, timesteps + 1)) * n2v_mapping['<bon/>']\n",
    "for i in range(timesteps):\n",
    "    decoded_results = decoder_model.predict([test_inputs[:,i+1].reshape(-1, 1)] + test_encoded)\n",
    "    test_encoded = decoded_results[1:]\n",
    "    #preds = decoded_results[0].argmax(-1)\n",
    "    test_inputs[:,i+1] = decoded_results[0].argmax(-1).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33035714285714285"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(encoder_inputs_testing.reshape(-1), test_inputs[:,1:].reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6: Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NBG_vae_dense'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 64\n",
    "intermediate_dim = 64\n",
    "latent_dim = 2\n",
    "embed_size = 16\n",
    "\n",
    "ohe_inputs = Input(shape=(original_dim, vocab_size))\n",
    "inputs = K.argmax(ohe_inputs)\n",
    "embeddings = Embedding(vocab_size, embed_size)(inputs)\n",
    "h = Dense(intermediate_dim, activation='relu')(embeddings)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_sigma = Dense(latent_dim)(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(original_dim, latent_dim),\n",
    "                              mean=0., stddev=0.1)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "z = Lambda(sampling)([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "encoder = Model(ohe_inputs, [z_mean, z_log_sigma, z], name='encoder')\n",
    "\n",
    "# Create decoder\n",
    "latent_inputs = Input(shape=(original_dim, latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(vocab_size, activation='softmax')(x)\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(ohe_inputs)[2])\n",
    "vae = Model(ohe_inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = keras.losses.categorical_crossentropy(ohe_inputs, outputs)\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_mlp\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 36)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            [(None, 64, 2), (Non 1924        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, 64, 36)       2532        encoder[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.argmax (TFOpLambda)     (None, 64)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 64, 16)       576         tf.math.argmax[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64, 64)       1088        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64, 2)        130         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64, 2)        130         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 64, 2)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.square (TFOpLambda)     (None, 64, 2)        0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda)   (None, 64, 2)        0           tf.__operators__.add[0][0]       \n",
      "                                                                 tf.math.square[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.exp (TFOpLambda)        (None, 64, 2)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast (TFOpLambda)            (None, 64, 36)       0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.convert_to_tensor (TFOpLambd (None, 64, 36)       0           decoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract_1 (TFOpLambda) (None, 64, 2)        0           tf.math.subtract[0][0]           \n",
      "                                                                 tf.math.exp[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.keras.backend.categorical_cr (None, 64)           0           tf.cast[0][0]                    \n",
      "                                                                 tf.convert_to_tensor[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum (TFOpLambda) (None, 64)           0           tf.math.subtract_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 64)           0           tf.keras.backend.categorical_cros\n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (None, 64)           0           tf.math.reduce_sum[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 64)           0           tf.math.multiply[0][0]           \n",
      "                                                                 tf.math.multiply_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean (TFOpLambda ()                   0           tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_loss (AddLoss)              ()                   0           tf.math.reduce_mean[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 4,456\n",
      "Trainable params: 4,456\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      [(None, 64, 2)]           0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64, 64)            192       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64, 36)            2340      \n",
      "=================================================================\n",
      "Total params: 2,532\n",
      "Trainable params: 2,532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "6/6 [==============================] - 1s 109ms/step - loss: 226.1325 - val_loss: 224.2118\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 223.6439 - val_loss: 220.9565\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 219.9919 - val_loss: 216.9333\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 215.5485 - val_loss: 210.4333\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 208.6724 - val_loss: 202.3813\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 200.5450 - val_loss: 190.7230\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 188.6285 - val_loss: 175.9446\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 172.0448 - val_loss: 154.9147\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 151.5080 - val_loss: 131.2661\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 126.7293 - val_loss: 110.1905\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 114.5561 - val_loss: 97.0137\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 102.0840 - val_loss: 89.4730\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 96.2352 - val_loss: 83.9304\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 91.7065 - val_loss: 79.1042\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 15/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 84.0578 - val_loss: 74.4038\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 81.0746 - val_loss: 69.6762\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 75.9826 - val_loss: 64.9917\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 69.3002 - val_loss: 60.8844\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 65.7325 - val_loss: 57.3392\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 63.2025 - val_loss: 54.2650\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 60.0903 - val_loss: 51.8592\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 56.3306 - val_loss: 49.8390\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 57.2497 - val_loss: 48.0011\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 24/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 52.6065 - val_loss: 46.4794\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 52.7748 - val_loss: 45.1098\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 50.6446 - val_loss: 43.7357\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 48.5660 - val_loss: 42.4291\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 49.2290 - val_loss: 41.2785\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 29/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 47.2966 - val_loss: 40.1634\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 46.7814 - val_loss: 39.1472\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 44.4074 - val_loss: 38.2693\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 43.8336 - val_loss: 37.4414\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 42.9486 - val_loss: 36.7284\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 43.0265 - val_loss: 36.0086\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 40.3555 - val_loss: 35.3249\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 41.0004 - val_loss: 34.6221\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 40.3771 - val_loss: 33.9290\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 38.6327 - val_loss: 33.2877\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 37.2678 - val_loss: 32.6394\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 36.0649 - val_loss: 31.9843\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 36.9179 - val_loss: 31.2974\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 36.3797 - val_loss: 30.6150\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 43/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 35.8260 - val_loss: 29.9674\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 35.1251 - val_loss: 29.4139\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 33.7749 - val_loss: 28.8779\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 34.1009 - val_loss: 28.2621\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 31.6765 - val_loss: 27.7530\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 48/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 30.7982 - val_loss: 27.2911\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 31.7353 - val_loss: 26.7909\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 31.5241 - val_loss: 26.3999\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 30.8890 - val_loss: 26.0036\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 28.8113 - val_loss: 25.6908\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 28.7505 - val_loss: 25.2788\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 28.3686 - val_loss: 24.8472\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 27.7703 - val_loss: 24.4887\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 28.8985 - val_loss: 24.2102\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 57/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 27.1745 - val_loss: 23.9536\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 58/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 27.5166 - val_loss: 23.6931\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 59/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 26.1314 - val_loss: 23.4467\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 60/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 26.1647 - val_loss: 23.2451\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 61/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 26.1388 - val_loss: 22.9814\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 62/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 25.9927 - val_loss: 22.7837\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 63/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 26.8814 - val_loss: 22.3962\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 64/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 25.2297 - val_loss: 22.0470\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 65/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 24.7539 - val_loss: 21.7716\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 66/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 25.8162 - val_loss: 21.5826\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 23.5198 - val_loss: 21.3924\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 68/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 24.4250 - val_loss: 21.2002\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 69/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 24.0350 - val_loss: 20.9599\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 70/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 23.1220 - val_loss: 20.7734\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 71/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 23.7561 - val_loss: 20.6286\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 72/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 23.0918 - val_loss: 20.3854\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 73/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 22.0656 - val_loss: 20.1967\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 74/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 22.2004 - val_loss: 19.9173\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 75/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 23.1089 - val_loss: 19.7316\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 76/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 22.3295 - val_loss: 19.5742\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 77/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 21.2091 - val_loss: 19.3741\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 78/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 20.9842 - val_loss: 19.2122\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 79/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 20.6077 - val_loss: 19.1096\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 80/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 21.4704 - val_loss: 18.8860\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 81/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 20.4011 - val_loss: 18.6516\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 82/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 20.3061 - val_loss: 18.5233\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 83/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 20.5644 - val_loss: 18.4312\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 84/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 20.5834 - val_loss: 18.2597\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 85/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 20.7247 - val_loss: 18.0663\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 86/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 20.9186 - val_loss: 17.8423\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 87/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 20.2479 - val_loss: 17.7231\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 88/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 19.1088 - val_loss: 17.5642\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 20.6068 - val_loss: 17.3955\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 90/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 18.2288 - val_loss: 17.2819\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 91/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 18.7492 - val_loss: 17.1331\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 92/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 19.4398 - val_loss: 16.9926\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 93/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 18.7324 - val_loss: 16.8492\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 94/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 18.3008 - val_loss: 16.8484\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 95/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 18.3678 - val_loss: 16.6708\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 96/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 19.3978 - val_loss: 16.4961\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 97/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 17.8391 - val_loss: 16.2846\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 98/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 18.5987 - val_loss: 16.2149\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 99/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 18.1337 - val_loss: 16.0814\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 100/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 17.0882 - val_loss: 15.9552\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 101/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 17.6932 - val_loss: 15.7913\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 102/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 17.1263 - val_loss: 15.7375\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 103/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 17.4882 - val_loss: 15.5942\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 104/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 17.0668 - val_loss: 15.5074\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 105/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 16.6912 - val_loss: 15.3744\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 106/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 16.5838 - val_loss: 15.2723\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 107/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 16.3356 - val_loss: 15.1799\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 108/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 16.2186 - val_loss: 15.0558\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 109/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 16.6620 - val_loss: 14.9213\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 110/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 15.9200 - val_loss: 14.7963\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 16.6385 - val_loss: 14.6922\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 112/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 15.4263 - val_loss: 14.5841\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 113/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 16.2546 - val_loss: 14.4853\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 114/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 16.1189 - val_loss: 14.3945\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 115/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 15.2404 - val_loss: 14.2562\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 116/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 15.6437 - val_loss: 14.1925\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 117/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 15.4765 - val_loss: 14.0806\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 118/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 15.9072 - val_loss: 14.0191\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 119/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 15.1612 - val_loss: 13.9445\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 120/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 14.8784 - val_loss: 13.8240\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 121/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 15.4324 - val_loss: 13.6923\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 122/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 14.5987 - val_loss: 13.5739\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 123/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 14.9830 - val_loss: 13.4893\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 124/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 14.1623 - val_loss: 13.3903\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 125/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 15.2464 - val_loss: 13.2730\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 126/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 14.7324 - val_loss: 13.2340\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 127/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 14.0840 - val_loss: 13.1413\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 128/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 13.9351 - val_loss: 13.0578\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 129/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 14.1144 - val_loss: 12.9358\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 130/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 14.2274 - val_loss: 12.8582\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 131/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 13.8338 - val_loss: 12.7916\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 132/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 13.4317 - val_loss: 12.7102\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 13.7216 - val_loss: 12.6231\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 134/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 13.7265 - val_loss: 12.5398\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 135/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 13.3056 - val_loss: 12.4399\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 136/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 12.9113 - val_loss: 12.3773\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 137/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 13.5574 - val_loss: 12.3299\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 138/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 12.9918 - val_loss: 12.2415\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 139/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 13.2005 - val_loss: 12.1547\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 140/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 12.4364 - val_loss: 12.0583\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 141/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 12.6400 - val_loss: 11.9454\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 142/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 12.6109 - val_loss: 11.8483\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 143/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 12.2071 - val_loss: 11.7112\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 144/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 12.9827 - val_loss: 11.6625\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 145/500\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 12.4983 - val_loss: 11.6108\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 146/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 12.6235 - val_loss: 11.5411\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 147/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 12.1809 - val_loss: 11.4237\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 148/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 11.8449 - val_loss: 11.3831\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 149/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 12.9101 - val_loss: 11.2953\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 150/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 12.5981 - val_loss: 11.2584\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 151/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 11.9969 - val_loss: 11.2673\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 152/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 12.2441 - val_loss: 11.1518\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 153/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 11.6135 - val_loss: 11.0547\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 154/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 11.9519 - val_loss: 10.9521\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 11.7285 - val_loss: 10.8586\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 156/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 11.0849 - val_loss: 10.7872\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 157/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 11.7204 - val_loss: 10.6722\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 158/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 11.1865 - val_loss: 10.6376\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 159/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 11.2914 - val_loss: 10.5640\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 160/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 10.6313 - val_loss: 10.5046\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 161/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 10.9827 - val_loss: 10.3955\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 162/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 10.8558 - val_loss: 10.3766\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 163/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 10.9215 - val_loss: 10.3158\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 164/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 11.4602 - val_loss: 10.2396\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 165/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 10.6854 - val_loss: 10.1771\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 166/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 10.3919 - val_loss: 10.0918\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 167/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 10.5477 - val_loss: 10.0376\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 168/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 11.1059 - val_loss: 10.0009\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 169/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 10.7217 - val_loss: 9.9754\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 170/500\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 10.1580 - val_loss: 9.9035\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 171/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 10.8038 - val_loss: 9.8379\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 172/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 10.0721 - val_loss: 9.8077\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 173/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 10.4606 - val_loss: 9.6995\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 174/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 9.9340 - val_loss: 9.6653\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 175/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 10.3764 - val_loss: 9.6290\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 176/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 10.3279 - val_loss: 9.5694\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 10.7211 - val_loss: 9.5229\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 178/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 10.3010 - val_loss: 9.4451\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 179/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 9.9502 - val_loss: 9.3641\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 180/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 9.9114 - val_loss: 9.2972\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 181/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 9.9754 - val_loss: 9.2198\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 182/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 9.8633 - val_loss: 9.2354\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 183/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 9.5306 - val_loss: 9.1811\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 184/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 9.1064 - val_loss: 9.1498\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 185/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 10.0194 - val_loss: 9.0611\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 186/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 8.9377 - val_loss: 8.9925\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 187/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 10.2206 - val_loss: 8.9521\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 188/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 9.1945 - val_loss: 8.8576\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 189/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 9.3109 - val_loss: 8.8223\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 190/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 9.2926 - val_loss: 8.7699\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 191/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 9.2855 - val_loss: 8.7454\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 192/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 9.0087 - val_loss: 8.6405\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 193/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 9.0247 - val_loss: 8.5729\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 194/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 9.3458 - val_loss: 8.5201\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 195/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 9.0826 - val_loss: 8.5420\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 196/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 8.4469 - val_loss: 8.4868\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 197/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 8.7893 - val_loss: 8.4152\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 198/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 8.7043 - val_loss: 8.4061\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 8.5168 - val_loss: 8.3762\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 200/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 8.7712 - val_loss: 8.3086\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 201/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 8.3693 - val_loss: 8.2876\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 202/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 8.8538 - val_loss: 8.2304\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 203/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 8.7382 - val_loss: 8.1739\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 204/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 7.9927 - val_loss: 8.1012\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 205/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 8.6479 - val_loss: 8.0877\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 206/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 8.6930 - val_loss: 8.0476\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 207/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 8.6708 - val_loss: 7.9751\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 208/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 8.1271 - val_loss: 7.9674\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 209/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 8.2396 - val_loss: 7.9321\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 210/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 8.0224 - val_loss: 7.8699\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 211/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 7.8587 - val_loss: 7.7982\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 212/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 8.0616 - val_loss: 7.7166\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 213/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 8.0000 - val_loss: 7.6881\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 214/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 7.9826 - val_loss: 7.6769\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 215/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 7.5915 - val_loss: 7.6227\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 216/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 7.7559 - val_loss: 7.5341\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 217/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 8.1036 - val_loss: 7.4829\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 218/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.3929 - val_loss: 7.4608\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 219/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 7.8521 - val_loss: 7.4266\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 220/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 7.5332 - val_loss: 7.4170\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 7.8371 - val_loss: 7.3976\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 222/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 7.8704 - val_loss: 7.3610\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 223/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 8.0372 - val_loss: 7.3331\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 224/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 7.4987 - val_loss: 7.2926\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 225/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 7.5862 - val_loss: 7.2360\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 226/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 7.3231 - val_loss: 7.2181\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 227/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 7.2509 - val_loss: 7.1991\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 228/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 7.2720 - val_loss: 7.1229\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 229/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 7.1540 - val_loss: 7.1047\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 230/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 6.7938 - val_loss: 7.0479\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 231/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 7.0853 - val_loss: 7.0229\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 232/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 7.1029 - val_loss: 7.0038\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 233/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 7.0791 - val_loss: 6.9467\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 234/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 6.7208 - val_loss: 6.9351\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 235/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 7.0962 - val_loss: 6.8932\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 236/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 6.6418 - val_loss: 6.8280\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 237/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 6.8215 - val_loss: 6.8147\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 238/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 6.8273 - val_loss: 6.8026\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 239/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 6.8082 - val_loss: 6.7807\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 240/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 6.6265 - val_loss: 6.7020\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 241/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 6.4878 - val_loss: 6.6953\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 242/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 6.7864 - val_loss: 6.6883\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 6.4426 - val_loss: 6.6639\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 244/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 6.7017 - val_loss: 6.6330\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 245/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 6.6257 - val_loss: 6.5989\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 246/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 6.5274 - val_loss: 6.5718\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 247/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 6.6553 - val_loss: 6.5266\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 248/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 6.4593 - val_loss: 6.5294\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 249/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 6.6594 - val_loss: 6.4968\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 250/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 6.5267 - val_loss: 6.4906\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 251/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 6.4265 - val_loss: 6.4484\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 252/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 6.2636 - val_loss: 6.4295\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 253/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 6.5583 - val_loss: 6.3865\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 254/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 6.0864 - val_loss: 6.3599\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 255/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 6.2448 - val_loss: 6.3588\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 256/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 6.4487 - val_loss: 6.3018\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 257/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 6.2772 - val_loss: 6.3123\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 258/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 6.3768 - val_loss: 6.2577\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 259/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 6.0062 - val_loss: 6.1935\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 260/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 6.2398 - val_loss: 6.1799\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 261/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 6.2395 - val_loss: 6.1785\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 262/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 6.0974 - val_loss: 6.1161\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 263/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.9334 - val_loss: 6.0627\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 264/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 6.0444 - val_loss: 6.0734\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 265/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 5.9066 - val_loss: 6.0494\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 266/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.7144 - val_loss: 5.9925\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 267/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.6590 - val_loss: 5.9668\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 268/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 6.1961 - val_loss: 5.9336\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 269/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 5.8942 - val_loss: 5.9388\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 270/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.6665 - val_loss: 5.8839\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 271/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 5.9933 - val_loss: 5.9036\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 272/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.5175 - val_loss: 5.9036\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 273/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.8440 - val_loss: 5.9042\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 274/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 5.9243 - val_loss: 5.8648\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 275/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 5.7517 - val_loss: 5.8455\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 276/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 5.4563 - val_loss: 5.8170\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 277/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 5.7128 - val_loss: 5.8219\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 278/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 5.3688 - val_loss: 5.7830\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 279/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 5.4504 - val_loss: 5.7299\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 280/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 5.7577 - val_loss: 5.7035\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 281/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 5.6740 - val_loss: 5.6946\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 282/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.4593 - val_loss: 5.6749\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 283/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 5.8565 - val_loss: 5.6565\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 284/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 5.4800 - val_loss: 5.6455\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 285/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.7135 - val_loss: 5.6412\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 286/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.4846 - val_loss: 5.5809\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 287/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 5.1890 - val_loss: 5.5174\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 288/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.2297 - val_loss: 5.4853\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 289/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 5.2456 - val_loss: 5.4646\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 290/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 5.4842 - val_loss: 5.4688\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 291/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.2883 - val_loss: 5.4249\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 292/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.0423 - val_loss: 5.4488\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 293/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.2025 - val_loss: 5.4149\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 294/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 5.2928 - val_loss: 5.3985\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 295/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 5.1194 - val_loss: 5.3819\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 296/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 4.9481 - val_loss: 5.3753\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 297/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 4.9012 - val_loss: 5.3341\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 298/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 4.9726 - val_loss: 5.3122\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 299/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.0670 - val_loss: 5.2825\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 300/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.8887 - val_loss: 5.2907\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 301/500\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 4.8656 - val_loss: 5.2882\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 302/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.9995 - val_loss: 5.2443\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 303/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.9198 - val_loss: 5.1944\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 304/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 4.8840 - val_loss: 5.1838\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 305/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.0623 - val_loss: 5.1306\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 306/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.8339 - val_loss: 5.1223\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 307/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.1116 - val_loss: 5.1211\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 308/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.8775 - val_loss: 5.1177\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 309/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 4.8893 - val_loss: 5.1070\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 310/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 4.7832 - val_loss: 5.0671\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 311/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 4.9267 - val_loss: 5.0790\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 312/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 4.8392 - val_loss: 5.0540\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 313/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 5.1210 - val_loss: 5.0337\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 314/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 4.8025 - val_loss: 5.0427\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 315/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 4.9685 - val_loss: 5.0243\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 316/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.6589 - val_loss: 5.0070\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 317/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 4.6438 - val_loss: 4.9975\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 318/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.6190 - val_loss: 4.9480\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 319/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.5320 - val_loss: 4.9387\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 320/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.6809 - val_loss: 4.8937\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 321/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 4.8372 - val_loss: 4.8693\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 322/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.4537 - val_loss: 4.8645\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 323/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 4.4719 - val_loss: 4.8597\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 324/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.6694 - val_loss: 4.8273\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 325/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 4.7283 - val_loss: 4.8169\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 326/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 4.5679 - val_loss: 4.8208\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 327/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 4.5987 - val_loss: 4.7885\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 328/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.5370 - val_loss: 4.8083\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 329/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.4365 - val_loss: 4.7897\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 330/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 4.2278 - val_loss: 4.7518\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 4.3139 - val_loss: 4.7161\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 332/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.4034 - val_loss: 4.6870\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 333/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.2336 - val_loss: 4.6796\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 334/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 4.4540 - val_loss: 4.6647\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 335/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 4.3823 - val_loss: 4.6314\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 336/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.2562 - val_loss: 4.6056\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 337/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 4.3835 - val_loss: 4.6083\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 338/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.4073 - val_loss: 4.6232\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 339/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 4.3325 - val_loss: 4.6203\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 340/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 4.1884 - val_loss: 4.6291\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 341/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 4.1143 - val_loss: 4.5829\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 342/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 4.3411 - val_loss: 4.5741\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 343/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.2165 - val_loss: 4.5384\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 344/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 4.1603 - val_loss: 4.5412\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 345/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.0995 - val_loss: 4.5006\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 346/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 4.6545 - val_loss: 4.5033\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 347/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 4.1495 - val_loss: 4.5251\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 348/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 4.1884 - val_loss: 4.5029\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 349/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.0629 - val_loss: 4.4500\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 350/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 3.9946 - val_loss: 4.4395\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 351/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 4.0417 - val_loss: 4.4111\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 352/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 4.1052 - val_loss: 4.4149\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 3.9958 - val_loss: 4.3958\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 354/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 3.9550 - val_loss: 4.3534\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 355/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 4.0481 - val_loss: 4.3502\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 356/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 3.8588 - val_loss: 4.3331\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 357/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 4.0392 - val_loss: 4.2955\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 358/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 3.9644 - val_loss: 4.2734\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 359/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 3.9580 - val_loss: 4.2920\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 360/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 3.8926 - val_loss: 4.2596\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 361/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 3.8742 - val_loss: 4.2427\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 362/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 4.0614 - val_loss: 4.2551\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 363/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 3.8138 - val_loss: 4.2520\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 364/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 3.7710 - val_loss: 4.2988\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 365/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.9808 - val_loss: 4.2910\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 366/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.7936 - val_loss: 4.2747\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 367/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.7250 - val_loss: 4.2717\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 368/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.7330 - val_loss: 4.2665\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 369/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 3.8026 - val_loss: 4.2369\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 370/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.8332 - val_loss: 4.2395\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 371/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.8236 - val_loss: 4.1904\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 372/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.5572 - val_loss: 4.1707\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 373/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.7475 - val_loss: 4.1341\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 374/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.6516 - val_loss: 4.1217\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.7803 - val_loss: 4.1045\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 376/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 3.6011 - val_loss: 4.0987\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 377/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.5347 - val_loss: 4.0746\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 378/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.7564 - val_loss: 4.0917\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 379/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.6374 - val_loss: 4.0921\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 380/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.7142 - val_loss: 4.0817\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 381/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.6852 - val_loss: 4.0668\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 382/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 3.6880 - val_loss: 4.0607\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 383/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.7251 - val_loss: 4.0499\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 384/500\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 3.5783 - val_loss: 4.0288\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 385/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.6742 - val_loss: 4.0157\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 386/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 3.6462 - val_loss: 4.0169\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 387/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.5445 - val_loss: 3.9817\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 388/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.6570 - val_loss: 3.9865\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 389/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 3.4682 - val_loss: 3.9765\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 390/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.6839 - val_loss: 3.9474\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 391/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 3.5897 - val_loss: 3.9453\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 392/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.6975 - val_loss: 3.9427\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 393/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.3867 - val_loss: 3.9538\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 394/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.3891 - val_loss: 3.9534\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 395/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 3.5434 - val_loss: 3.9661\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 396/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.4496 - val_loss: 3.9535\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 397/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.6696 - val_loss: 3.9329\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 398/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.5929 - val_loss: 3.9251\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 399/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 3.4335 - val_loss: 3.9505\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 400/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.5075 - val_loss: 3.9381\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 401/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.4651 - val_loss: 3.9079\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 402/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.3193 - val_loss: 3.9059\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 403/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.3392 - val_loss: 3.8768\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 404/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 3.3309 - val_loss: 3.8545\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 405/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 3.4950 - val_loss: 3.8540\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 406/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.4429 - val_loss: 3.8252\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 407/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 3.2373 - val_loss: 3.8223\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 408/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.2951 - val_loss: 3.7891\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 409/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 3.4045 - val_loss: 3.7916\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 410/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 3.2343 - val_loss: 3.7801\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 411/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.3693 - val_loss: 3.8176\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 412/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 3.2633 - val_loss: 3.8039\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 413/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.1844 - val_loss: 3.7392\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 414/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 3.4371 - val_loss: 3.6703\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 415/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.2010 - val_loss: 3.6820\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 416/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 3.1937 - val_loss: 3.6830\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 417/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.2000 - val_loss: 3.6714\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 418/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.3800 - val_loss: 3.6751\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 419/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.3107 - val_loss: 3.6519\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 420/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 3.1633 - val_loss: 3.6042\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 421/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 3.0994 - val_loss: 3.6134\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 422/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.2758 - val_loss: 3.6077\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 423/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 3.1271 - val_loss: 3.5913\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 424/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 3.0744 - val_loss: 3.6173\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 425/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 2.9906 - val_loss: 3.5681\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 426/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 3.0380 - val_loss: 3.5899\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 427/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.0836 - val_loss: 3.5856\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 428/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 3.0687 - val_loss: 3.6002\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 429/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.0500 - val_loss: 3.5651\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 430/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.0768 - val_loss: 3.5894\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 431/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.0740 - val_loss: 3.5426\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 432/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 3.0119 - val_loss: 3.5724\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 433/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.0167 - val_loss: 3.5446\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 434/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.9349 - val_loss: 3.5204\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 435/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.0099 - val_loss: 3.5075\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 436/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.1140 - val_loss: 3.4851\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 437/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 2.9906 - val_loss: 3.4906\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 438/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 2.9425 - val_loss: 3.4729\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 439/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3.1556 - val_loss: 3.4872\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 440/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 3.0785 - val_loss: 3.5048\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 441/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.9371 - val_loss: 3.4976\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 442/500\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 2.9026 - val_loss: 3.4575\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 443/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.9932 - val_loss: 3.4445\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 444/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.0704 - val_loss: 3.4577\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 445/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 2.9621 - val_loss: 3.4596\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 446/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 3.0995 - val_loss: 3.4482\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 447/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.8880 - val_loss: 3.4604\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 448/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 2.9525 - val_loss: 3.4156\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 449/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.8447 - val_loss: 3.3854\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 450/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 2.8913 - val_loss: 3.4111\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 451/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.8611 - val_loss: 3.3870\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 452/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 2.8195 - val_loss: 3.3616\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 453/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 2.7740 - val_loss: 3.3648\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 454/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.8598 - val_loss: 3.3340\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 455/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.7797 - val_loss: 3.3429\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 456/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.7051 - val_loss: 3.3314\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 457/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 2.7995 - val_loss: 3.3069\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 458/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 2.7544 - val_loss: 3.2918\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 459/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.7953 - val_loss: 3.2698\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 460/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.8104 - val_loss: 3.2847\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 461/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.7145 - val_loss: 3.2445\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 462/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 2.7741 - val_loss: 3.2162\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 463/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 2.7659 - val_loss: 3.1870\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 464/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 2.9065 - val_loss: 3.1801\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 465/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 2.9739 - val_loss: 3.2204\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 466/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 2.7286 - val_loss: 3.2268\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 467/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 2.7452 - val_loss: 3.2008\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 468/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 2.7198 - val_loss: 3.2114\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 469/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.7730 - val_loss: 3.2184\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 470/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.7042 - val_loss: 3.2075\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 471/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 2.7387 - val_loss: 3.2381\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 472/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 2.6089 - val_loss: 3.1909\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 473/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.6956 - val_loss: 3.1477\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 474/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.7170 - val_loss: 3.1903\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 475/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 2.6187 - val_loss: 3.1781\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 476/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.5608 - val_loss: 3.1575\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 477/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.5978 - val_loss: 3.1732\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 478/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.6356 - val_loss: 3.1637\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 479/500\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 2.6524 - val_loss: 3.1733\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 480/500\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 2.6484 - val_loss: 3.1216\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 481/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.6691 - val_loss: 3.1057\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 482/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.5770 - val_loss: 3.0814\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 483/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.5464 - val_loss: 3.0657\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 484/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.5946 - val_loss: 3.0883\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 485/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.6602 - val_loss: 3.0521\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 486/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.6806 - val_loss: 3.0540\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 487/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.5341 - val_loss: 3.0536\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 488/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 2.7935 - val_loss: 3.0282\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 489/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.7819 - val_loss: 3.0433\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 490/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 2.5376 - val_loss: 3.0676\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 491/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 2.5653 - val_loss: 3.0038\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 492/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 2.4840 - val_loss: 3.0165\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 493/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 2.4790 - val_loss: 2.9633\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 494/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 2.5749 - val_loss: 2.9557\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 495/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 2.5048 - val_loss: 2.9571\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 496/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 2.5405 - val_loss: 2.9549\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 497/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 2.5042 - val_loss: 2.9419\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 498/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.4373 - val_loss: 2.9264\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 499/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 2.4915 - val_loss: 2.8820\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "Epoch 500/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.5893 - val_loss: 2.8801\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    }
   ],
   "source": [
    "mc = ModelCheckpoint(f'Models/{name}.hdf5', monitor='val_loss', save_format=\"tf\")\n",
    "\n",
    "hist = vae.fit(y_train, y_train,\n",
    "        epochs=500,\n",
    "        batch_size=32,\n",
    "        validation_data=(y_test, y_test),\n",
    "              callbacks=[mc]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.plot(hist.history['loss'], label='training')\n",
    "plt.plot(hist.history['val_loss'], label='testing')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'figures/{name}', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "vae = load_model(f'Models/{name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9987763554216867"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = vae(y_train).numpy().argmax(-1)\n",
    "\n",
    "accuracy_score(X_train.reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9951636904761905"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = vae(y_test).numpy().argmax(-1)\n",
    "\n",
    "accuracy_score(X_test.reshape(-1), preds.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=30\n",
    "\n",
    "grid_x = np.linspace(-15, 15, n)\n",
    "grid_y = np.linspace(-15, 15, n)\n",
    "lst=[]\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]] * 64)[np.newaxis,:,:]\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        lst.append(x_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = decoder.predict(np.random.normal(size=(10, 64, 2)))\n",
    "preds.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation = decoder(np.random.normal(size=(10, 64, 2))).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = encoder(y_train)[2].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[1,2]] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.predict(train_encoded).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = encoder.predict(y_test, batch_size=32)[2]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
